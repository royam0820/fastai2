{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "10_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/fastai2-v4/blob/master/10_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gt4v7ROCFKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3a55a6-c097-4efb-c943-5d94fdf5d6a1"
      },
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 727kB 17.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 34.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 35.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 776.8MB 21kB/s \n",
            "\u001b[K     |████████████████████████████████| 12.8MB 70.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 10.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haVPjD2wn1Rz"
      },
      "source": [
        "NOTE: `ERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.` This error can be ignored.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCvJ0X2PNlyC"
      },
      "source": [
        "# !pip install torchtext==0.8.1\n",
        "# !pip install -U fastbook\n",
        "# import fastbook\n",
        "# fastbook.setup_book()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ob366S6RrLX",
        "outputId": "555f6a58-7d4b-4b99-8f05-c1b11da79703"
      },
      "source": [
        "# !uptime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 15:12:29 up 15 min,  0 users,  load average: 0.92, 0.45, 0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sse_69T0Nxq1"
      },
      "source": [
        "NOTE: ERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucm4LvWDCFKz"
      },
      "source": [
        "from fastbook import *\n",
        "from IPython.display import display,HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z7MMTTQCFKz"
      },
      "source": [
        "# NLP Deep Dive: RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCQyfIpPCFK0"
      },
      "source": [
        "In <<chapter_intro>> we saw that deep learning can be used to get great results with natural language datasets. Our example relied on using a pretrained language model and fine-tuning it to classify reviews. That example highlighted a difference between transfer learning in NLP and computer vision: in general in NLP the pretrained model is trained on a different task.\n",
        "\n",
        "**What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before). This kind of task is called *self-supervised learning*: we do not need to give labels to our model, just feed it lots and lots of texts**. It has a process to automatically get labels from the data, and this task isn't trivial: to properly guess the next word in a sentence, the model will have to develop an understanding of the English (or other) language. Self-supervised learning can also be used in other domains; for instance, see [\"Self-Supervised Learning and Computer Vision\"](https://www.fast.ai/2020/01/13/self_supervised/) for an introduction to vision applications. Self-supervised learning is not usually used for the model that is trained directly, but instead is used for pretraining a model used for transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLGhzpkZCFK0"
      },
      "source": [
        "> jargon: Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels. For instance, training a model to predict the next word in a text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilaw3RNaCFK1"
      },
      "source": [
        "**The language model we used to classify IMDb reviews was pretrained on Wikipedia**. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better. The Wikipedia English is slightly different from the IMDb English, so instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus and then use *that* as the base for our classifier.\n",
        "\n",
        "Even if our language model knows the basics of the language we are using in the task (e.g., our pretrained model is in English), it helps to get used to the style of the corpus we are targeting. It may be more informal language, or more technical, with new words to learn or different ways of composing sentences. In the case of the IMDb dataset, there will be lots of names of movie directors and actors, and often a less formal style of language than that seen in Wikipedia.\n",
        "\n",
        "We already saw that with fastai, we can download a pretrained English language model and use it to get state-of-the-art results for NLP classification. (We expect pretrained models in many more languages to be available soon—they might well be available by the time you are reading this book, in fact.) So, why are we learning how to train a language model in detail?\n",
        "\n",
        "One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine-tune the (sequence-based) language model prior to fine-tuning the classification model. For instance, for **the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached. Since there are 25,000 labeled reviews in the training set and 25,000 in the validation set, that makes 100,000 movie reviews altogether**. **We can use all of these reviews to fine-tune the pretrained language model, which was trained only on Wikipedia articles; this will result in a language model that is particularly good at predicting the next word of a movie review**.\n",
        "\n",
        "**This is known as the Universal Language Model Fine-tuning (ULMFit) approach**. The [paper](https://arxiv.org/abs/1801.06146) showed that this extra stage of fine-tuning of the language model, prior to transfer learning to a classification task, resulted in significantly better predictions. Using this approach, we have three stages for transfer learning in NLP, as summarized in <<ulmfit_process>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bif4KpwyCFK_"
      },
      "source": [
        "<img alt=\"Diagram of the ULMFiT process\" width=\"700\" caption=\"The ULMFiT process\" id=\"ulmfit_process\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00027.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxgUSzjsCFK_"
      },
      "source": [
        "We'll now explore how to apply a neural network to this language modeling problem, using the concepts introduced in the last two chapters. But before reading further, pause and think about how *you* would approach this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_K2Xf-XCFLA"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01eU9y2pCFLA"
      },
      "source": [
        "It's not at all obvious how we're going to use what we've learned so far to build a language model. Sentences can be different lengths, and documents can be very long. So, how can we predict the next word of a sentence using a neural network? Let's find out!\n",
        "\n",
        "We've already seen how categorical variables can be used as independent variables for a neural network. The approach we took for a single categorical variable was to:\n",
        "\n",
        "1. Make a list of all possible levels of that categorical variable (we'll call this list the *vocab*).\n",
        "1. Replace each level with its index in the vocab.\n",
        "1. Create an embedding matrix for this containing a row for each level (i.e., for each item of the vocab).\n",
        "1. Use this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to but faster and more efficient than a matrix that takes as input one-hot-encoded vectors representing the indexes.)\n",
        "\n",
        "We can do nearly the same thing with text! What is new is the idea of a sequence. **First we concatenate all of the documents in our dataset into one big long string and split it into words, giving us a very long list of words (or \"tokens\"). Our independent variable will be the sequence of words starting with the first word in our very long list and ending with the second to last, and our dependent variable will be the sequence of words starting with the second word and ending with the last word**. \n",
        "\n",
        "Our vocab will consist of a mix of common words that are already in the vocabulary of our pretrained model and new words specific to our corpus (cinematographic terms or actors names, for instance). Our embedding matrix will be built accordingly: for words that are in the vocabulary of our pretrained model, we will take the corresponding row in the embedding matrix of the pretrained model; but for new words we won't have anything, so we will just initialize the corresponding row with a random vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfSdYLmnCFLA"
      },
      "source": [
        "Each of the steps necessary to create a language model has jargon associated with it from the world of natural language processing, and fastai and PyTorch classes available to help. The steps are:\n",
        "\n",
        "- **Tokenization**: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)\n",
        "- **Numericalization**: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab\n",
        "- **Language model data loader creation**: fastai provides an `LMDataLoader` class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required\n",
        "- **Language model creation**: We need a special kind of model that does something we haven't seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a *recurrent neural network* (RNN). We will get to the details of these RNNs in the <<chapter_nlp_dive>>, but for now, you can think of it as just another deep neural network.\n",
        "\n",
        "Let's take a look at how each step works in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdgU5kAUCFLB"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u77hK3OVCFLB"
      },
      "source": [
        "When we said \"convert the text into a list of words,\" we left out a lot of details. For instance, what do we do with punctuation? How do we deal with a word like \"don't\"? Is it one word, or two? What about long medical or chemical words? Should they be split into their separate pieces of meaning? How about hyphenated words? What about languages like German and Polish where we can create really long words from many, many pieces? What about languages like Japanese and Chinese that don't use bases at all, and don't really have a well-defined idea of *word*?\n",
        "\n",
        "Because there is no one correct answer to these questions, there is no one approach to tokenization. There are three main approaches:\n",
        "\n",
        "- **Word-based**: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning \"don't\" into \"do n't\"). Generally, punctuation marks are also split into separate tokens.\n",
        "- **Subword based**: Split words into smaller parts, based on the most commonly occurring substrings. For instance, \"occasion\" might be tokenized as \"o c ca sion.\"\n",
        "- **Character-based**: Split a sentence into its individual characters.\n",
        "\n",
        "We'll be looking at word and subword tokenization here, and we'll leave character-based tokenization for you to implement in the questionnaire at the end of this chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J85mWwBxCFLC"
      },
      "source": [
        "> jargon: token: One element of a list created by the tokenization process. It could be a word, part of a word (a _subword_), or a single character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqsPYjaKCFLD"
      },
      "source": [
        "### Word Tokenization with fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC1V7sc_CFLD"
      },
      "source": [
        "Rather than providing its own tokenizers, fastai instead provides a consistent interface to a range of tokenizers in external libraries. Tokenization is an active field of research, and new and improved tokenizers are coming out all the time, so the defaults that fastai uses change too. However, the API and options shouldn't change too much, since fastai tries to maintain a consistent API even as the underlying technology changes.\n",
        "\n",
        "Let's try it out with the IMDb dataset that we used in <<chapter_intro>>:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtidkGx1CFLE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "66bb2d72-16e1-44ed-c27c-0f1deb38a885"
      },
      "source": [
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.IMDB, dest='/content/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3H8DXz7iVte",
        "outputId": "43feb9e6-8788-455a-f17f-e397717a5d2c"
      },
      "source": [
        "path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/content/imdb')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naFinxNoCFLE"
      },
      "source": [
        "We'll need to grab the text files in order to try out a tokenizer. Just like `get_image_files`, which we've used many times already, gets all the image files in a path, `get_text_files` gets all the text files in a path. We can also optionally pass `folders` to restrict the search to a particular list of subfolders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUZhFsCGCFLE"
      },
      "source": [
        "# getting all the text files in a path.\n",
        "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzfbBRtQdyob"
      },
      "source": [
        "??get_text_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jn5QGx7SUEJ"
      },
      "source": [
        "NOTE: the directory folder `unsup` means unsupervised; it holds movie reviews that do no have a rating attached to it. There are 50000 records unlabeled in that folder. Whereas, the train and test folders have 25000 records of movie reviews labeled each. Bringing to a total of 100000 movie reviews.  And we will use all these records to fine-tuned the pretrained language model which will then be particularly good at predicting the next word of a movie review.\n",
        "\n",
        "Now it relates to **semi-supervised learning**, that is can we do something useful with unlabeled data?. In this case, we add this corpus to our language model because its functions is to predict the next word, it does not care whether the movie review is positivie or negative.\n",
        "\n",
        "So, now we have 25000 movie reviews in `train`, 25000 movie reviews in `test` and 50000 movie reviews in `unsup`. So, by adding the `unsup` folder, we have twice as much data to build a language model with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89clRr_PCFLE"
      },
      "source": [
        "Here's a review that we'll tokenize (we'll just print the start of it here to save space):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWDzprNZCFLF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad9b15f5-ce64-4e72-82b7-eeea4120aad9"
      },
      "source": [
        "# example of a movie review (first 75 characters) that will be tokenized.\n",
        "txt = files[0].open().read(); txt[:75]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tenshu is imprisoned and sentenced to death. When he survives electrocution'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGVgBLERCFLF"
      },
      "source": [
        "As we write this book, **the default English word tokenizer for fastai uses a library called *spaCy***. It has a sophisticated rules engine with special rules for URLs, individual special English words, and much more. Rather than directly using `SpacyTokenizer`, however, we'll use `WordTokenizer`, since that will always point to fastai's current default word tokenizer (which may not necessarily be spaCy, depending when you're reading this).\n",
        "\n",
        "Let's try it out. We'll use fastai's `coll_repr(collection, n)` function to display the results. This displays the first *`n`* items of *`collection`*, along with the full size—it's what `L` uses by default. Note that fastai's tokenizers take a collection of documents to tokenize, so we have to wrap `txt` in a list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foPORbzzCFLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c006af0b-2dcb-491c-e73c-eb23065cee6e"
      },
      "source": [
        "# calling the tokenizer\n",
        "spacy = WordTokenizer()\n",
        "# applying the tokenizer to the text\n",
        "toks = first(spacy([txt]))\n",
        "# displaying the results of the tokenization\n",
        "print(coll_repr(toks, 30))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#260) ['Tenshu','is','imprisoned','and','sentenced','to','death','.','When','he','survives','electrocution','the','government','officials','give','him','a','choice','to','either','be','electrocute','at','a','greater','degree','or','agree','to'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9jvYSxtJT4d"
      },
      "source": [
        "NOTE: `WordTokenizer` is a fastai tokenizer (which currently is `spaCy`).  The `WordTokenizer` will return a generator so we need to call first (returning the first element of the generator) to be able to display the result. \n",
        "\n",
        "`coll_repr` collection representation displaying the tokenization results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGBczjMBCFLG"
      },
      "source": [
        "NOTE: As you see, spaCy has mainly just separated out the words and punctuation. But it does something else here too: it has split \"it's\" into \"it\" and \"'s\". That makes intuitive sense; these are separate words, really. Tokenization is a surprisingly subtle task, when you think about all the little details that have to be handled. Fortunately, spaCy handles these pretty well for us—for instance, here we see that \".\" is separated when it terminates a sentence, but not in an acronym or number:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ-kvtSxCFLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4f20c9-25a3-4b28-bc8a-1c842617edd0"
      },
      "source": [
        "# spaCy handling correctly acronym, decimal point and the end of a sentence.\n",
        "first(spacy(['The U.S. dollar $1 is $1.00.']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYZBU5yACFLG"
      },
      "source": [
        "fastai then adds some additional functionality to the tokenization process with the `Tokenizer` class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl31koLlCFLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9283d0b5-d057-4fba-800d-462c632c9cb6"
      },
      "source": [
        "# fastai additions to the tokenizer process\n",
        "tkn = Tokenizer(spacy)\n",
        "print(coll_repr(tkn(txt), 31))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#302) ['xxbos','xxmaj','tenshu','is','imprisoned','and','sentenced','to','death','.','xxmaj','when','he','survives','electrocution','the','government','officials','give','him','a','choice','to','either','be','electrocute','at','a','greater','degree','or'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuNWwauhfUqu"
      },
      "source": [
        "NOTE: fastai provides a tokenizer wrapper which add additional tokens such as for instance `xxbos` and `xxeos` for beginning and end of stream. Also, capital letters are turned into lower case letters (`xxmaj`), ... etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a93Kl3y2CFLH"
      },
      "source": [
        "Notice that there are now some tokens that start with the characters \"xx\", which is not a common word prefix in English. These are *special tokens*.\n",
        "\n",
        "For example, the first item in the list, `xxbos`, is a special token that indicates the start of a new text (\"BOS\" is a standard NLP acronym that means \"beginning of stream\"). By recognizing this start token, the model will be able to learn it needs to \"forget\" what was said previously and focus on upcoming words.\n",
        "\n",
        "These special tokens don't come from spaCy directly. They are there because fastai adds them by default, by applying a number of rules when processing text. **These rules are designed to make it easier for a model to recognize the important parts of a sentence**. In a sense, we are translating the original English language sequence into a simplified tokenized language—a language that is designed to be easy for a model to learn.\n",
        "\n",
        "For instance, the rules will replace a sequence of four exclamation points with a special *repeated character* token, followed by the number four, and then a single exclamation point. In this way, **the model's embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark**. Similarly, a capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word. This way, the embedding matrix only needs the lowercase versions of the words, saving compute and memory resources, but can still learn the concept of capitalization.\n",
        "\n",
        "Here are some of the main special tokens you'll see:\n",
        "\n",
        "- `xxbos`:: Indicates the beginning of a text (here, a review) (beginning of stream)\n",
        "- `xxmaj`:: Indicates the next word begins with a capital (since we lowercased everything)\n",
        "- `xxunk`:: Indicates the word is unknown\n",
        "\n",
        "To see the rules that were used, you can check the default rules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOJ79rPMCFLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b3ca8d-27b1-4a04-b1bf-eb7a11525d3e"
      },
      "source": [
        "defaults.text_proc_rules"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<function fastai.text.core.fix_html>,\n",
              " <function fastai.text.core.replace_rep>,\n",
              " <function fastai.text.core.replace_wrep>,\n",
              " <function fastai.text.core.spec_add_spaces>,\n",
              " <function fastai.text.core.rm_useless_spaces>,\n",
              " <function fastai.text.core.replace_all_caps>,\n",
              " <function fastai.text.core.replace_maj>,\n",
              " <function fastai.text.core.lowercase>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyy7TulhCFLH"
      },
      "source": [
        "As always, you can look at the source code of each of them in a notebook by typing:\n",
        "\n",
        "```\n",
        "??replace_rep\n",
        "```\n",
        "\n",
        "Here is a brief summary of what each does:\n",
        "\n",
        "- `fix_html`:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\n",
        "- `replace_rep`:: Replaces any character repeated three times or more with a special token for repetition (`xxrep`), the number of times it's repeated, then the character\n",
        "- `replace_wrep`:: Replaces any word repeated three times or more with a special token for word repetition (`xxwrep`), the number of times it's repeated, then the word\n",
        "- `spec_add_spaces`:: Adds spaces around / and #\n",
        "- `rm_useless_spaces`:: Removes all repetitions of the space character\n",
        "- `replace_all_caps`:: Lowercases a word written in all caps and adds a special token for all caps (`xxup`) in front of it\n",
        "- `replace_maj`:: Lowercases a capitalized word and adds a special token for capitalized (`xxmaj`) in front of it\n",
        "- `lowercase`:: Lowercases all text and adds a special token at the beginning (`xxbos`) and/or the end (`xxeos`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7T15qaNCFLI"
      },
      "source": [
        "Let's take a look at a few of them in action:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFhoXJoXCFLI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1412c363-eb58-4241-da0b-eefb8961c89a"
      },
      "source": [
        "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4QHeG7sCFLI"
      },
      "source": [
        "Now let's take a look at how subword tokenization would work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Qa1v3OCFLI"
      },
      "source": [
        "### Subword Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mp46MHLCFLJ"
      },
      "source": [
        "In addition to the *word tokenization* approach seen in the last section, another popular tokenization method is *subword tokenization*. Word tokenization relies on an assumption that spaces provide a useful separation of components of meaning in a sentence. However, this assumption is not always appropriate. For instance, consider this sentence: 我的名字是郝杰瑞 (\"My name is Jeremy Howard\" in Chinese). That's not going to work very well with a word tokenizer, because there are no spaces in it! Languages like Chinese and Japanese don't use spaces, and in fact they don't even have a well-defined concept of a \"word.\" There are also languages, like Turkish and Hungarian, that can add many subwords together without spaces, creating very long words that include a lot of separate pieces of information.\n",
        "\n",
        "To handle these cases, it's generally best to use subword tokenization. This proceeds in two steps:\n",
        "\n",
        "1. Analyze a corpus of documents to find the **most commonly occurring groups of letters**. These become the vocab.\n",
        "2. Tokenize the corpus using this vocab of *subword units*.\n",
        "\n",
        "Let's look at an example. For our corpus, we'll use the first 2,000 movie reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdfyEWQRCFLJ"
      },
      "source": [
        "# getting the first 2000 movie reviews for subword example\n",
        "txts = L(o.open().read() for o in files[:2000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIzEvDWACFLJ"
      },
      "source": [
        "We instantiate our tokenizer, passing in the size of the vocab we want to create, and then we need to \"train\" it. That is, we need to have it read our documents and find the common sequences of characters to create the vocab. This is done with `setup`. As we'll see shortly, `setup` is a special fastai method that is called automatically in our usual data processing pipelines. Since we're doing everything manually at the moment, however, we have to call it ourselves. Here's a function that does these steps for a given vocab size, and shows an example output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm09Cs1-CFLJ"
      },
      "source": [
        "# function calling the SubWordTokenizer\n",
        "def subword(sz):\n",
        "    sp = SubwordTokenizer(vocab_sz=sz)\n",
        "    sp.setup(txts)\n",
        "    return ' '.join(first(sp([txt]))[:40])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HR7wmdshmwo"
      },
      "source": [
        "NOTE: the default SubwordTokenizer uses SentenPiece. `setup`, transforms in fastai will call `setup` to do some initialization as per the transform being used such as, in this case, `SubwordTokenizer`. So, `sp.setup(txts)` will train the `SubwordTokenizer` to get the most commonly occurring groups of letters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsSMJeBoCFLJ"
      },
      "source": [
        "Let's try it out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9D4furqCFLJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8370e069-5882-4356-87e0-dfaac168ab1c"
      },
      "source": [
        "# calling the subword function with a vocab size of 1000\n",
        "subword(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'▁T en sh u ▁is ▁imp ri son ed ▁and ▁ s ent ence d ▁to ▁death . ▁When ▁he ▁su r v ive s ▁e le c t ro cu tion ▁the ▁go ver n ment ▁off ic ial'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmC56VjimY"
      },
      "source": [
        "NOTE: When using fastai's subword tokenizer, the special character `▁` represents a space character in the original text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "nzhqkWcNh0w5",
        "outputId": "2dadd936-b018-4eb4-c847-a5766825176d"
      },
      "source": [
        "len(subword(1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "139"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg9p7Lf0CFLK"
      },
      "source": [
        "**If we use a smaller vocab, then each token will represent fewer characters, and it will take more tokens to represent a sentence**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rz_fCJ9CFLK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e6fc366e-1024-4e04-e2d8-f67aaa27a1c7"
      },
      "source": [
        "# calling the subword function with a vocab size of 200\n",
        "subword(200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'▁ T en s h u ▁is ▁ i m p ri s on ed ▁and ▁ s ent en c ed ▁to ▁ d e a th . ▁ W h en ▁h e ▁ s ur v i'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1gX0J6nkSDW"
      },
      "source": [
        "NOTE: with a smaller vocab, more tokens are generated to create a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NAk6vJAbhfxQ",
        "outputId": "62a9774c-3436-454c-8322-2095e65d58c3"
      },
      "source": [
        "len(subword(200))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOqaK8aJCFLK"
      },
      "source": [
        "On the other hand, **if we use a larger vocab, then most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkQKD2SfCFLK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "911b9e49-f730-46ff-cf82-305f55f35b47"
      },
      "source": [
        "# calling the subword function with a vocab size of 10000\n",
        "subword(10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'▁Ten s hu ▁is ▁imp ri son ed ▁and ▁sentence d ▁to ▁death . ▁When ▁he ▁survive s ▁elect ro cut ion ▁the ▁government ▁official s ▁give ▁him ▁a ▁choice ▁to ▁either ▁be ▁elect ro cut e ▁at ▁a ▁great'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kiv9Ylpyki3N"
      },
      "source": [
        "NOTE: with a larger vocab, less tokens are generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "tkRVAp41iKIc",
        "outputId": "66d1d483-00d2-43f4-bee1-5f82485ab400"
      },
      "source": [
        "len(subword(10000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4srArCKECFLL"
      },
      "source": [
        "**Picking a subword vocab size represents a compromise: a larger vocab means fewer tokens per sentence, which means faster training, less memory, and less state for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn**.\n",
        "\n",
        "Overall, subword tokenization provides a way to easily scale between **character tokenization (i.e., using a small subword vocab)** and **word tokenization (i.e., using a large subword vocab)**, and handles every human language without needing language-specific algorithms to be developed. It can even handle other \"languages\" such as genomic sequences or MIDI music notation! For this reason, in the last year its popularity has soared, and it seems likely to become the most common tokenization approach (it may well already be, by the time you read this!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKYDFjxWCFLL"
      },
      "source": [
        "Once our texts have been split into tokens, we need to convert them to numbers. We'll look at that next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbXaTgv-CFLL"
      },
      "source": [
        "### Numericalization with fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tniv8Nd2CFLL"
      },
      "source": [
        "***Numericalization* is the process of mapping tokens to integers**. The steps are basically identical to those necessary to create a `Category` variable, such as the dependent variable of digits in MNIST:\n",
        "\n",
        "1. Make a list of all possible levels of that categorical variable (the vocab).\n",
        "1. Replace each level with its index in the vocab.\n",
        "\n",
        "Let's take a look at this in action on the word-tokenized text we saw earlier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwKSNtbDCFLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3031fbaa-ea2f-4577-d120-857cfe282294"
      },
      "source": [
        "toks = tkn(txt)\n",
        "print(coll_repr(tkn(txt), 31))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#302) ['xxbos','xxmaj','tenshu','is','imprisoned','and','sentenced','to','death','.','xxmaj','when','he','survives','electrocution','the','government','officials','give','him','a','choice','to','either','be','electrocute','at','a','greater','degree','or'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZMBz3gtCFLL"
      },
      "source": [
        "Just like with `SubwordTokenizer`, we need to call `setup` on `Numericalize`; this is how we create the vocab. That means we'll need our tokenized corpus first. Since tokenization takes a while, it's done in parallel by fastai; but for this manual walkthrough, we'll use a small subset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRkaWAb_CFLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed496201-6e17-4565-e29f-9446d60149bb"
      },
      "source": [
        "# getting 200 movie reviews\n",
        "toks200 = txts[:200].map(tkn)\n",
        "# example of one\n",
        "toks200[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#302) ['xxbos','xxmaj','tenshu','is','imprisoned','and','sentenced','to','death','.'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWjM-vyKCFLM"
      },
      "source": [
        "We can pass this to `setup` to create our vocab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNoo5C4eCFLM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3d1bc8b4-7cfa-417c-a92a-72edc26e05fd"
      },
      "source": [
        "num = Numericalize()\n",
        "num.setup(toks200)\n",
        "# showing a representation of a collection (L class)\n",
        "coll_repr(num.vocab,20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#2136) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','it','in','i'...]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGeHCg-rn7UJ"
      },
      "source": [
        "NOTE: The vocabulary thus generated, starts with the fastai special tokens `xx..`, then, we have the english token in order of frequency. Maximum embedding size is 60000, \"out of the bag words\" are treated with an `xxunk` (unknown) word token. Fastai uses a flag `min_freq=3` meaning any work appearing less than 3 times is replaced with an `xxunk` token. This limit is useful to avoid having an overly large embedding matrix.  This limit ia also better for computational performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swfjrXgUCFLM"
      },
      "source": [
        "Our special rules tokens appear first, and then every word appears once, in frequency order. The defaults to `Numericalize` are `min_freq=3,max_vocab=60000`. `max_vocab=60000` results in fastai replacing all words other than the most common 60,000 with a special *unknown word* token, `xxunk`. This is useful to avoid having an overly large embedding matrix, since that can slow down training and use up too much memory, and can also mean that there isn't enough data to train useful representations for rare words. However, this last issue is better handled by setting `min_freq`; the default `min_freq=3` means that any word appearing less than three times is replaced with `xxunk`.\n",
        "\n",
        "fastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the `vocab` parameter.\n",
        "\n",
        "Once we've created our `Numericalize` object, we can use it as if it were a function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEmzssyxCFLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c3fc6c-5986-4597-f055-de8dd40d7eeb"
      },
      "source": [
        "# tensor of integers\n",
        "nums = num(toks)[:20]; nums"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorText([  2,   8,   0,  16,   0,  12,   0,  15, 336,  11,   8,  62,  38,   0,   0,   9,   0,   0, 203,  87])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t63q4GAlCFLM"
      },
      "source": [
        "NOTE: `2` represents the token `xxbos` and `8` represents `xxmaj` for example.\n",
        "\n",
        "This time, our tokens have been converted to a tensor of integers that our model can receive. We can check that they map back to the original text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q0JFIP_CFLN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7cc039ec-7192-4ba5-d8b5-eb8829ada6fa"
      },
      "source": [
        "# mapping integers to texts\n",
        "' '.join(num.vocab[o] for o in nums)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'xxbos xxmaj xxunk is xxunk and xxunk to death . xxmaj when he xxunk xxunk the xxunk xxunk give him'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-c0VOMECFLN"
      },
      "source": [
        "Now that we have numbers, we need to put them in batches for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC9Vz8YfCFLN"
      },
      "source": [
        "### Putting Our Texts into Batches for a Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4HVx5u4CFLN"
      },
      "source": [
        "When dealing with images, we needed to resize them all to the same height and width before grouping them together in a mini-batch so they could stack together efficiently in a single tensor. Here it's going to be a little different, because one cannot simply resize text to a desired length. Also, we want our language model to read text in order, so that it can efficiently predict what the next word is. **This means that each new batch should begin precisely where the previous one left off**.\n",
        "\n",
        "Suppose we have the following text:\n",
        "\n",
        "> : In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\n",
        "\n",
        "The tokenization process will add special tokens and deal with punctuation to return this text:\n",
        "\n",
        "> : xxbos xxmaj in this chapter , we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface . xxmaj first we will look at the processing steps necessary to convert text into numbers and how to customize it . xxmaj by doing this , we 'll have another example of the preprocessor used in the data block xxup api . \\n xxmaj then we will study how we build a language model and train it for a while .\n",
        "\n",
        "We now have 90 tokens, separated by spaces. Let's say we want a batch size of 6. We need to break this text into 6 contiguous parts of length 15:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": false,
        "id": "ucQzS6zjCFLN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "d39c27a8-9574-46cd-fc55-bc37a28706f4"
      },
      "source": [
        "# Creating batches\n",
        "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
        "tokens = tkn(stream)\n",
        "# batch size and sequence length\n",
        "bs,seq_len = 6,15\n",
        "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>back</td>\n",
              "      <td>over</td>\n",
              "      <td>the</td>\n",
              "      <td>example</td>\n",
              "      <td>of</td>\n",
              "      <td>classifying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>movie</td>\n",
              "      <td>reviews</td>\n",
              "      <td>we</td>\n",
              "      <td>studied</td>\n",
              "      <td>in</td>\n",
              "      <td>chapter</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>dig</td>\n",
              "      <td>deeper</td>\n",
              "      <td>under</td>\n",
              "      <td>the</td>\n",
              "      <td>surface</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>first</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "      <td>the</td>\n",
              "      <td>processing</td>\n",
              "      <td>steps</td>\n",
              "      <td>necessary</td>\n",
              "      <td>to</td>\n",
              "      <td>convert</td>\n",
              "      <td>text</td>\n",
              "      <td>into</td>\n",
              "      <td>numbers</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>how</td>\n",
              "      <td>to</td>\n",
              "      <td>customize</td>\n",
              "      <td>it</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>by</td>\n",
              "      <td>doing</td>\n",
              "      <td>this</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>'ll</td>\n",
              "      <td>have</td>\n",
              "      <td>another</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>preprocessor</td>\n",
              "      <td>used</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "      <td>data</td>\n",
              "      <td>block</td>\n",
              "      <td>xxup</td>\n",
              "      <td>api</td>\n",
              "      <td>.</td>\n",
              "      <td>\\n</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>then</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>will</td>\n",
              "      <td>study</td>\n",
              "      <td>how</td>\n",
              "      <td>we</td>\n",
              "      <td>build</td>\n",
              "      <td>a</td>\n",
              "      <td>language</td>\n",
              "      <td>model</td>\n",
              "      <td>and</td>\n",
              "      <td>train</td>\n",
              "      <td>it</td>\n",
              "      <td>for</td>\n",
              "      <td>a</td>\n",
              "      <td>while</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgn9RjRTqqpc"
      },
      "source": [
        "NOTE: result: 6 contiguous parts of length 15."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcZVZQi-CFLN"
      },
      "source": [
        "In a perfect world, we could then give this one batch to our model. But that approach doesn't scale, because outside of this toy example it's unlikely that a single batch containing all the texts would fit in our GPU memory (here we have 90 tokens, but all the IMDb reviews together give several million).\n",
        "\n",
        "So, we need to divide this array more finely into **subarrays of a fixed sequence length**. It is important to maintain order within and across these subarrays, because we will use a model that maintains a state so that it remembers what it read previously when predicting what comes next. \n",
        "\n",
        "Going back to our previous example with 6 batches of length 15, if we chose a sequence length of 5, that would mean we first feed the following array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "id": "DMTdKFkzCFLO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "e37b8cb0-dc8d-4a65-a7ad-fffaf48832e9"
      },
      "source": [
        "#hide_input\n",
        "# batch size 6, sequence length 5 - 6x5\n",
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>movie</td>\n",
              "      <td>reviews</td>\n",
              "      <td>we</td>\n",
              "      <td>studied</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>first</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>how</td>\n",
              "      <td>to</td>\n",
              "      <td>customize</td>\n",
              "      <td>it</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>preprocessor</td>\n",
              "      <td>used</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>will</td>\n",
              "      <td>study</td>\n",
              "      <td>how</td>\n",
              "      <td>we</td>\n",
              "      <td>build</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRtzzHZzsWLf"
      },
      "source": [
        "NOTE: **We are splitting up horizontally**. This is our first mini-batch. Each row are independent. The first row of this batch connects to the first row of the second mini-batch and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbDWIDaXCFLO"
      },
      "source": [
        "Then this one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "id": "fualqU1zCFLO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "ada3bf60-f8a2-461e-baef-94c0e43f7ed7"
      },
      "source": [
        "#hide_input\n",
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>chapter</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>dig</td>\n",
              "      <td>deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>processing</td>\n",
              "      <td>steps</td>\n",
              "      <td>necessary</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxmaj</td>\n",
              "      <td>by</td>\n",
              "      <td>doing</td>\n",
              "      <td>this</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>data</td>\n",
              "      <td>block</td>\n",
              "      <td>xxup</td>\n",
              "      <td>api</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>a</td>\n",
              "      <td>language</td>\n",
              "      <td>model</td>\n",
              "      <td>and</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaHe-1-rCFLO"
      },
      "source": [
        "And finally:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "id": "AJxgvP6QCFLO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "461f7713-a846-448d-8f2d-a47e61def3fb"
      },
      "source": [
        "#hide_input\n",
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>over</td>\n",
              "      <td>the</td>\n",
              "      <td>example</td>\n",
              "      <td>of</td>\n",
              "      <td>classifying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>under</td>\n",
              "      <td>the</td>\n",
              "      <td>surface</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>convert</td>\n",
              "      <td>text</td>\n",
              "      <td>into</td>\n",
              "      <td>numbers</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>we</td>\n",
              "      <td>'ll</td>\n",
              "      <td>have</td>\n",
              "      <td>another</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>.</td>\n",
              "      <td>\\n</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>then</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>it</td>\n",
              "      <td>for</td>\n",
              "      <td>a</td>\n",
              "      <td>while</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inJTlsGgCFLP"
      },
      "source": [
        "Going back to our movie reviews dataset, **the first step is to transform the individual texts into a stream by concatenating them together**. As with images, it's best to **randomize the order of the inputs**, so at the beginning of each epoch we will shuffle the entries to make a new stream (**we shuffle the order of the documents**, not the order of the words inside them, or the texts would not make sense anymore!).\n",
        "\n",
        "We then **cut this stream into a certain number of batches** (which is our *batch size*). For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens. What is important is that we preserve the order of the tokens (so from 1 to 5,000 for the first mini-stream, then from 5,001 to 10,000...), because we want the model to read continuous rows of text (as in the preceding example). **An `xxbos` token is added at the start of each during preprocessing, so that the model knows when it reads the stream when a new entry is beginning**.\n",
        "\n",
        "So to recap, at every epoch we shuffle our collection of documents and concatenate them into a stream of tokens. We then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the mini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence length we picked.\n",
        "\n",
        "This is all done behind the scenes by the fastai library when we create an `LMDataLoader`. We do this by first applying our `Numericalize` object to the tokenized texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJoPxQHECFLP"
      },
      "source": [
        "# numericalization\n",
        "nums200 = toks200.map(num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5cc5GdICFLP"
      },
      "source": [
        "and then passing that to `LMDataLoader`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUbAsUnlCFLP"
      },
      "source": [
        "# creating a data loader\n",
        "dl = LMDataLoader(nums200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4trwVLWjCFLP"
      },
      "source": [
        "Let's confirm that this gives the expected results, by grabbing the first batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-RRvyM1CFLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386aadb5-682a-4d69-a2ba-307e73ae3f7d"
      },
      "source": [
        "# grabbing a first batch\n",
        "x,y = first(dl)\n",
        "x.shape,y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 72]), torch.Size([64, 72]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3nBFijgwC7b"
      },
      "source": [
        "NOTE: 64 is the default batch size and 72 is the default sequence length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-55sKLylCFLQ"
      },
      "source": [
        "and then looking at the first row of the independent variable, which should be the start of the first text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mhBB63fCFLQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "903d8549-190a-48c4-c4d8-d8ffdc56cb10"
      },
      "source": [
        "# Independent variable - looking at the first row of the vocab\n",
        "' '.join(num.vocab[o] for o in x[0][:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'xxbos xxmaj xxunk is xxunk and xxunk to death . xxmaj when he xxunk xxunk the xxunk xxunk give him'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvsx6IGzCFLQ"
      },
      "source": [
        "The dependent variable is the same thing offset by one token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_mNzpzLCFLQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b88fbed8-ec7e-43fc-94c6-f1476d925bd9"
      },
      "source": [
        "# Dependent the variable offset by one token - getting the next word\n",
        "' '.join(num.vocab[o] for o in y[0][:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'xxmaj xxunk is xxunk and xxunk to death . xxmaj when he xxunk xxunk the xxunk xxunk give him a'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYB5XqqqwptC"
      },
      "source": [
        "NOTE: the dependent variable has an offset of one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th-t0YXTCFLQ"
      },
      "source": [
        "This concludes all the preprocessing steps we need to apply to our data. We are now ready to train our text classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG6Ar7DSCFLQ"
      },
      "source": [
        "## Training a Text Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWVOO6dYCFLR"
      },
      "source": [
        "As we saw at the beginning of this chapter, there are two steps to training a state-of-the-art text classifier using transfer learning: \n",
        "- first we need to fine-tune our language model pretrained on Wikipedia to the corpus of IMDb reviews, and then \n",
        "- we can use that model to train a classifier.\n",
        "\n",
        "As usual, let's start with assembling our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_y8XygICFLR"
      },
      "source": [
        "### Language Model Using DataBlock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j63c8aMCFLY"
      },
      "source": [
        "**fastai handles tokenization and numericalization automatically when `TextBlock` is passed to `DataBlock`**. All of the arguments that can be passed to `Tokenize` and `Numericalize` can also be passed to `TextBlock`. In the next chapter we'll discuss the easiest ways to run each of these steps separately, to ease debugging—but you can always just debug by running them manually on a subset of your data as shown in the previous sections. And don't forget about `DataBlock`'s handy `summary` method, which is very useful for debugging data issues.\n",
        "\n",
        "Here's how we use `TextBlock` to create a language model, using fastai's defaults:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnBHDjBOCFLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6f1ffbca-cd0c-4c6e-ef13-4312405a3edc"
      },
      "source": [
        "# getting the IMDB text files\n",
        "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
        "\n",
        "# creating a language model data loader\n",
        "# using a TextBlock to create a language model using fastai's defaults\n",
        "dls_lm = DataBlock(\n",
        "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
        "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
        ").dataloaders(path, path=path, bs=128, seq_len=80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfLx1caxrdLO"
      },
      "source": [
        "[datablock api doc](https://docs.fast.ai/tutorial.datablock.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQp4ejPveiYd"
      },
      "source": [
        "NOTE: language models can use a lot of GPU, so you may need to decrease batchsize here (`bs`). `bs` decreased from 128 to 64 for this colab session.  You might also want to decrease the sequence length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R6g_SqfCFLZ"
      },
      "source": [
        "One thing that's different to previous types we've used in `DataBlock` is that we're not just using the class directly (i.e., `TextBlock(...)`, but instead are calling a *class method*. A class method is a Python method that, as the name suggests, belongs to a *class* rather than an *object*. (Be sure to search online for more information about class methods if you're not familiar with them, since they're commonly used in many Python libraries and applications; we've used them a few times previously in the book, but haven't called attention to them.) The reason that `TextBlock` is special is that setting up the numericalizer's vocab can take a long time (we have to read and tokenize every document to get the vocab). To be as efficient as possible it performs a few optimizations: \n",
        "\n",
        "- **It saves the tokenized documents in a temporary folder, so it doesn't have to tokenize them more than once**\n",
        "- **It runs multiple tokenization processes in parallel, to take advantage of your computer's CPUs**\n",
        "\n",
        "We need to tell `TextBlock` how to access the texts, so that it can do this initial preprocessing—that's what `from_folder` does.\n",
        "\n",
        "`show_batch` then works in the usual way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCXQ4dOsCFLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "8b9c7b0c-866e-4ec0-bfb0-70a487a51d83"
      },
      "source": [
        "# showing two batch items\n",
        "dls_lm.show_batch(max_n=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos during eddie murphy 's stand up a women from the audience yells at eddie and a man from the audience responds . what is said is , , women - xxup do xxup mr xxup rob ( this is a character from xxmaj saturday night live ) , the man responds with xxup shut xxup up xxup bitch . unlike the previous post saying the women yelled do gumby , this is incorrect , although the post - er</td>\n",
              "      <td>during eddie murphy 's stand up a women from the audience yells at eddie and a man from the audience responds . what is said is , , women - xxup do xxup mr xxup rob ( this is a character from xxmaj saturday night live ) , the man responds with xxup shut xxup up xxup bitch . unlike the previous post saying the women yelled do gumby , this is incorrect , although the post - er said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>, instead of a saccharine watered down version that is a total waste of time and money , is not easy ! xxmaj good luck ! xxmaj enjoy this film , but remember … it is only fantasy ! xxbos xxmaj after their very wealthy father of the xxmaj whitman family dies three very wealthy brothers set off to xxmaj india to find their wealthy self - indulgent mother who is undergoing some mental catharsis guilt trip by acting as</td>\n",
              "      <td>instead of a saccharine watered down version that is a total waste of time and money , is not easy ! xxmaj good luck ! xxmaj enjoy this film , but remember … it is only fantasy ! xxbos xxmaj after their very wealthy father of the xxmaj whitman family dies three very wealthy brothers set off to xxmaj india to find their wealthy self - indulgent mother who is undergoing some mental catharsis guilt trip by acting as a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRVLj_DZbBla"
      },
      "source": [
        "NOTE: `show_batch` denumericalize the text.  Also, in the second column `text_` we see our dependent variable offset by one. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Scag7PCFLZ"
      },
      "source": [
        "Now that our data is ready, we can fine-tune the pretrained language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-n36_1gCFLZ"
      },
      "source": [
        "### Fine-Tuning the Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uN1wFH8CFLa"
      },
      "source": [
        "**To convert the integer word indices into activations that we can use for our neural network, we will use embeddings, just like we did for collaborative filtering and tabular modeling**. Then we'll feed those embeddings into a *recurrent neural network* (RNN), using an architecture called *AWD-LSTM* (we will show you how to write such a model from scratch in <<chapter_nlp_dive>>). As we discussed earlier, the embeddings in the pretrained model are merged with random embeddings added for words that weren't in the pretraining vocabulary. This is handled automatically inside `language_model_learner`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPIUcjJXCFLa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "726ccf7f-fd28-4d4a-e642-d50111405b1f"
      },
      "source": [
        "# creating a language model learner\n",
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
        "    metrics=[accuracy, Perplexity()]).to_fp16()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARF8bFvYp5kF"
      },
      "source": [
        "NOTE: [fastai learner doc](https://docs.fast.ai/learner.html) The learner will learn to  predict what is the next word in a movie review.\n",
        "- `Perplexity()`: perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.\n",
        "- mixed precision training `.to_fp16()` speeds up the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ocGSTvfd9dH"
      },
      "source": [
        "NOTE: A language model learns to predict what word comes next in a sentence. And to predict the next word in a sentence you need to know quite a lot about English (assuming it is in English) and quite a lot about word knowledge.\n",
        "\n",
        "For instance:\n",
        "- I like to eat a hot ??? (dog)\n",
        "- It is a hot ??? (day)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22AsBikzCFLa"
      },
      "source": [
        "**The loss function used by default is cross-entropy loss, since we essentially have a classification problem** (the different categories being the words in our vocab). **The *perplexity* metric used here is often used in NLP for language models: it is the exponential of the loss** (i.e., `torch.exp(cross_entropy)`). We  also include the **accuracy metric, to see how many times our model is right when trying to predict the next word**, since cross-entropy (as we've seen) is both hard to interpret, and tells us more about the model's confidence than its accuracy.\n",
        "\n",
        "Let's go back to the process diagram from the beginning of this chapter. The first arrow has been completed for us and made available as a pretrained model in fastai, and we've just built the `DataLoaders` and `Learner` for the second stage. Now we're ready to fine-tune our language model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh9BRUZaCFLa"
      },
      "source": [
        "<img alt=\"Diagram of the ULMFiT process\" width=\"450\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00027.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82CxlcduCFLa"
      },
      "source": [
        "It takes quite a while to train each epoch, so we'll be saving the intermediate model results during the training process. Since `fine_tune` doesn't do that for us, we'll use `fit_one_cycle`. **Just like `cnn_learner`, `language_model_learner` automatically calls `freeze` when using a pretrained model (which is the default), so this will only train the embeddings (the only part of the model that contains randomly initialized weights—i.e., embeddings for words that are in our IMDb vocab, but aren't in the pretrained model vocab)**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAgPqcuLmE7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f7c92b-b3cb-40e8-b806-79c80f614cc7"
      },
      "source": [
        "learn.model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequentialRNN(\n",
              "  (0): AWD_LSTM(\n",
              "    (encoder): Embedding(60008, 400, padding_idx=1)\n",
              "    (encoder_dp): EmbeddingDropout(\n",
              "      (emb): Embedding(60008, 400, padding_idx=1)\n",
              "    )\n",
              "    (rnns): ModuleList(\n",
              "      (0): WeightDropout(\n",
              "        (module): LSTM(400, 1152, batch_first=True)\n",
              "      )\n",
              "      (1): WeightDropout(\n",
              "        (module): LSTM(1152, 1152, batch_first=True)\n",
              "      )\n",
              "      (2): WeightDropout(\n",
              "        (module): LSTM(1152, 400, batch_first=True)\n",
              "      )\n",
              "    )\n",
              "    (input_dp): RNNDropout()\n",
              "    (hidden_dps): ModuleList(\n",
              "      (0): RNNDropout()\n",
              "      (1): RNNDropout()\n",
              "      (2): RNNDropout()\n",
              "    )\n",
              "  )\n",
              "  (1): LinearDecoder(\n",
              "    (decoder): Linear(in_features=400, out_features=60008, bias=True)\n",
              "    (output_dp): RNNDropout()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVB1JnXJCFLa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "504c06bb-3131-4c33-952c-2bcfd0771636"
      },
      "source": [
        "# fine tuning the language model for IMDB\n",
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.013922</td>\n",
              "      <td>3.906585</td>\n",
              "      <td>0.299860</td>\n",
              "      <td>49.728813</td>\n",
              "      <td>29:53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 21min 4s, sys: 9min 1s, total: 30min 5s\n",
            "Wall time: 29min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAhjcc3yClN4"
      },
      "source": [
        "NOTE: The language model learner is only going to train the new embeddings, that is the part of the model that contains the randomly initialized weights; embedding for words that are in our IMDB vocab and are not in the pretained model vocab.  After one epoch, we get an accuracy of about 30%, nice! it means under a 3rd of a time our model is predicting the next word of a string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xbX2kYwCFLb"
      },
      "source": [
        "This model takes a while to train, so it's a good opportunity to talk about saving intermediary results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVQldqT4CFLb"
      },
      "source": [
        "### Saving and Loading Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvkVIseVCFLb"
      },
      "source": [
        "You can easily save the state of your model like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zypn_zukCFLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bfa2c7d-d29f-45ec-ae32-6a6859a5f11c"
      },
      "source": [
        "# saving the state of your model - epoch 1\n",
        "learn.save('1epoch')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/content/imdb/models/1epoch.pth')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej-O6hi_sxJw"
      },
      "source": [
        "# saving the model to gdrive\n",
        "model_save_name = '1epoch.pth'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(learn.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KroI1d04CFLb"
      },
      "source": [
        "This will create a file in `learn.path/models/` named *1epoch.pth*. If you want to load your model in another machine after creating your `Learner` the same way, or resume training later, you can load the content of this file with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QdGsICqCFLb"
      },
      "source": [
        "# loading the state of your model for epoch 1\n",
        "learn = learn.load('1epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO1N6bVwEAws"
      },
      "source": [
        "NOTE: before loading the model, make sure that the learner has been created."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0ycfHdMtMCs",
        "outputId": "882ad737-e804-4049-92fa-c9c14240d122"
      },
      "source": [
        "# loading the model from gdrive\n",
        "model_save_name = '1epoch.pth'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "learn.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWmgTQ_-CFLc"
      },
      "source": [
        "Once the initial training has completed, we can continue fine-tuning the model after unfreezing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJp9IA8jCFLc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "04c34eae-9b82-4445-ac3b-f1b5384716be"
      },
      "source": [
        "# fine tuning the model after unfreezing\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(10, 2e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/10 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='1078' class='' max='2630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      40.99% [1078/2630 04:54<07:03 4.3590]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.179487</td>\n",
              "      <td>4.127365</td>\n",
              "      <td>0.298322</td>\n",
              "      <td>62.014278</td>\n",
              "      <td>12:55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.904268</td>\n",
              "      <td>3.865157</td>\n",
              "      <td>0.315461</td>\n",
              "      <td>47.710781</td>\n",
              "      <td>12:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.747975</td>\n",
              "      <td>3.741694</td>\n",
              "      <td>0.324483</td>\n",
              "      <td>42.169373</td>\n",
              "      <td>12:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.664353</td>\n",
              "      <td>3.678273</td>\n",
              "      <td>0.329661</td>\n",
              "      <td>39.577972</td>\n",
              "      <td>13:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.568419</td>\n",
              "      <td>3.634495</td>\n",
              "      <td>0.333824</td>\n",
              "      <td>37.882729</td>\n",
              "      <td>12:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.496644</td>\n",
              "      <td>3.607989</td>\n",
              "      <td>0.336497</td>\n",
              "      <td>36.891800</td>\n",
              "      <td>12:42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.417458</td>\n",
              "      <td>3.591917</td>\n",
              "      <td>0.338646</td>\n",
              "      <td>36.303596</td>\n",
              "      <td>12:54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.358779</td>\n",
              "      <td>3.583853</td>\n",
              "      <td>0.340059</td>\n",
              "      <td>36.012047</td>\n",
              "      <td>12:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.313330</td>\n",
              "      <td>3.585232</td>\n",
              "      <td>0.340234</td>\n",
              "      <td>36.061733</td>\n",
              "      <td>12:45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.286820</td>\n",
              "      <td>3.588133</td>\n",
              "      <td>0.340094</td>\n",
              "      <td>36.166492</td>\n",
              "      <td>12:42</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuMtZB4HESQT"
      },
      "source": [
        "NOTE: after 10 epochs, we get an accuracy of almost 34%!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeGVMObCFLc"
      },
      "source": [
        "Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the *encoder*. We can save it with `save_encoder`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z91SCbXCFLc"
      },
      "source": [
        "# saving the fine tuned model - the encoder\n",
        "# saving all of our model except the final layer converting activations to probabilities\n",
        "learn.save_encoder('finetuned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9l21b3sEioH"
      },
      "source": [
        "NOTE: after the fine tuning, all we need to do is to save the encoder.  We will not need the final layer that leads to prediction. We need to save only the \"body\" of the model.  In NLP, we use the term encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-g8c2FoXKEp"
      },
      "source": [
        "# saving the model to gdrive\n",
        "model_save_name = 'finetuned.pth'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(learn.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqlVQg8tm_Re"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9BEDq8hJmAW",
        "outputId": "8c571f8b-e3c6-4086-f4d8-186456b272ce"
      },
      "source": [
        "# loading the model from gdrive\n",
        "model_save_name = 'finetuned.pth'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "learn.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4nh4AbmZt5g"
      },
      "source": [
        "NOTE: for our next step which is text classification, we do not care to get the final layer which gives us the next word (the decoding part), we just need the encoding part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWxHEfBKCFLc"
      },
      "source": [
        "> jargon: Encoder: The model not including the task-specific final layer(s). This term means much the same thing as _body_ when applied to vision CNNs, but \"encoder\" tends to be more used for NLP and generative models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0dD9wo-CFLc"
      },
      "source": [
        "This completes the second stage of the text classification process: fine-tuning the language model. We can now use it to fine-tune a classifier using the IMDb sentiment labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdEezxv_CFLc"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkMY4L29CFLd"
      },
      "source": [
        "Before we move on to fine-tuning the classifier, let's quickly try something different: using our model to generate random reviews. Since it's trained to guess what the next word of the sentence is, we can use the model to write new reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjDRbfwmCFLd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b4bab07a-cf08-4cd7-da89-98d3b053291e"
      },
      "source": [
        "# predicting the next sentence \n",
        "TEXT = \"I liked this movie because\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
        "         for _ in range(N_SENTENCES)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vlFVeZIAiiB"
      },
      "source": [
        "??learn.predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJHVfac5untR"
      },
      "source": [
        "NOTE: in sampling, `temperature` is a parameter allowing for more or less randomness in predictions.  Lower temperature setting equates to more confident, conservative network. Higher temperature setting equates to more excited, random network (and more mistakes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h6D2OxKCFLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dae5b3-9411-4aaa-9b5d-485f0ee811d5"
      },
      "source": [
        "print(\"\\n\".join(preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i liked this movie because it begins with David Thriller and Chris Blink ( james Born ) in a London expectations on the night of the opening . It 's the idea of the plot coming to mind\n",
            "i liked this movie because it looks like a documentary to show how a young woman who has high euro variety life in a family is not really so . That 's why i was sucked into the game . All the parts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGh6AZfRHCHE"
      },
      "source": [
        "NOTE: Good job! our fined tuned langage model is quite good at inventing language, even with a very simple approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij1DM5oECFLd"
      },
      "source": [
        "As you can see, we add some randomness (we pick a random word based on the probabilities returned by the model) so we don't get exactly the same review twice. Our model doesn't have any programmed knowledge of the structure of a sentence or grammar rules, yet it has clearly learned a lot about English sentences: we can see it capitalizes properly (***I* is just transformed to *i*** **because our rules require two characters or more to consider a word as capitalized**, **so it's normal to see it lowercased) and is using consistent tense**. The general review makes sense at first glance, and it's only if you read carefully that you can notice something is a bit off. Not bad for a model trained in a couple of hours! \n",
        "\n",
        "But our end goal wasn't to train a model to generate reviews, but to classify them... so let's use this model to do just that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdo7DC5cCFLd"
      },
      "source": [
        "### Creating the Classifier DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIopoKcICFLd"
      },
      "source": [
        "We're now moving from language model fine-tuning to classifier fine-tuning. **To recap, a language model predicts the next word of a document**, so it doesn't need any external labels. **A classifier, however, predicts some external label—in the case of IMDb, it's the sentiment of a document**.\n",
        "\n",
        "This means that the structure of our `DataBlock` for NLP classification will look very familiar. It's actually nearly the same as we've seen for the many image classification datasets we've worked with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah1ubJwHqBiq"
      },
      "source": [
        "path='/content/imdb'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2syFrR2CFLe"
      },
      "source": [
        "# NLP text classification\n",
        "dls_clas = DataBlock(\n",
        "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
        "    get_y = parent_label,\n",
        "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
        "    splitter=GrandparentSplitter(valid_name='test')\n",
        ").dataloaders(path, path=path, bs=128, seq_len=72)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qnBzGIHCFLe"
      },
      "source": [
        "Just like with image classification, `show_batch` shows the dependent variable (sentiment, in this case) with each independent variable (movie review text):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2VLFJJOCFLe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "2d3346c0-020e-4795-e8d2-9af8cb42cfe7"
      },
      "source": [
        "# show batch with the dependent variable (sentiment)\n",
        "dls_clas.show_batch(max_n=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love story is an interesting one , however . xxmaj kate xxmaj winslett is wonderful as xxmaj rose , an aristocratic young lady betrothed by xxmaj cal ( billy xxmaj zane ) . xxmaj early on the voyage xxmaj rose meets xxmaj jack ( leonardo dicaprio ) , a lower class artist on his way to xxmaj america after winning his ticket aboard xxmaj titanic in a poker game . xxmaj if he wants something , he goes and gets it</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xxbos xxrep 3 * xxmaj warning - this review contains \" plot spoilers , \" though nothing could \" spoil \" this movie any more than it already is . xxmaj it really xxup is that bad . xxrep 3 * \\n\\n xxmaj before i begin , xxmaj i 'd like to let everyone know that this definitely is one of those so - incredibly - bad - that - you - fall - over - laughing movies . xxmaj if you 're in a lighthearted mood and need a very hearty laugh , this is the movie for you . xxmaj now without further ado , my review : \\n\\n xxmaj this movie was found in a bargain bin at wal - mart . xxmaj that should be the first clue as to how good of a movie it is . xxmaj secondly , it stars the lame action</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scxhZbX2CFLe"
      },
      "source": [
        "Looking at the `DataBlock` definition, every piece is familiar from previous data blocks we've built, with two important exceptions:\n",
        "\n",
        "- **`TextBlock.from_folder` no longer has the `is_lm=True` parameter.**\n",
        "- **We pass the `vocab` we created for the language model fine-tuning.**\n",
        "\n",
        "The reason that we pass the `vocab` of the language model is to make sure we use the same correspondence of token to index. Otherwise the embeddings we learned in our fine-tuned language model won't make any sense to this model, and the fine-tuning step won't be of any use.\n",
        "\n",
        "**By passing `is_lm=False` (or not passing `is_lm` at all, since it defaults to `False`) we tell `TextBlock` that we have regular labeled data, rather than using the next tokens as labels.** There is one challenge we have to deal with, however, which is to do with collating multiple documents into a mini-batch. Let's see with an example, by trying to create a mini-batch containing the first 10 documents. First we'll numericalize them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TteAR-ruCFLe"
      },
      "source": [
        "# getting the first 10 movie reviews and numericalize them.\n",
        "nums_samp = toks200[:10].map(num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNg-8CzcCFLe"
      },
      "source": [
        "Let's now look at how many tokens each of these 10 movie reviews have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcWMzQ5lCFLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860f2bbe-8ceb-4662-d9e6-0634dbb69814"
      },
      "source": [
        "# looking at the token length for the first 10 movie reviews\n",
        "nums_samp.map(len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#10) [302,225,184,279,157,145,179,130,423,492]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5dTmJdyZ5fe"
      },
      "source": [
        "NOTE1:  Python’s `map()` is a built-in function that allows you to process and transform all the items in an iterable without using an explicit for loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No9vznDuK6C9"
      },
      "source": [
        "NOTE2: in this example, 492 is the longuest sequence.  When we split by sequence (`seq_len=72`), when doing so, we do not have the same sub-sequences for each of these tokens, they will be all of different lengths. To deal with that, we will add a padding token `xxpad` to every sequence in a mini-batch to make them all the same size.  fastai will also shuffle the documents so the documents collated into a single batch will tend of be of similar lengths.\n",
        "\n",
        "Sorting and padding are done by the Data Block API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKt7y1MYCFLf"
      },
      "source": [
        "Remember, **PyTorch `DataLoader`s need to collate all the items in a batch into a single tensor, and a single tensor has a fixed shape** **(i.e., it has some particular length on every axis, and all items must be consistent)**. This should sound familiar: we had the same issue with images. In that case, we used cropping, padding, and/or squishing to make all the inputs the same size. Cropping might not be a good idea for documents, because it seems likely we'd remove some key information (having said that, the same issue is true for images, and we use cropping there; data augmentation hasn't been well explored for NLP yet, so perhaps there are actually opportunities to use cropping in NLP too!). You can't really \"squish\" a document. So that leaves **padding**!\n",
        "\n",
        "**We will expand the shortest texts to make them all the same size. To do this, we use a special padding token (`xxpad`) that will be ignored by our model**. **Additionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set)**. **We do this by (approximately, for the training set) sorting the documents by length prior to each epoch**. **The result of this is that the documents collated into a single batch will tend of be of similar lengths. We won't pad every batch to the same size, but will instead use the size of the largest document in each batch as the target size.** (It is possible to do something similar with images, which is especially useful for irregularly sized rectangular images, but at the time of writing no library provides good support for this yet, and there aren't any papers covering it. It's something we're planning to add to fastai soon, however, so keep an eye on the book's website; we'll add information about this as soon as we have it working well.)\n",
        "\n",
        "**The sorting and padding are automatically done by the data block API** for us when using a `TextBlock`, with `is_lm=False`. (We don't have this same issue for language model data, since we concatenate all the documents together first, and then split them into equally sized sections.)\n",
        "\n",
        "We can now create a model to classify our texts:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWZEhPLvxJCp"
      },
      "source": [
        "> Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad some sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5nA6m7NCFLf"
      },
      "source": [
        "# creating a text classification model\n",
        "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
        "                                metrics=accuracy).to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pycU6lyLOV7H"
      },
      "source": [
        "learn.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwSoVuUMWCq3"
      },
      "source": [
        "NOTE: \n",
        "- `drop_mult=0.5` (drop out multiplier) We are using an AWD_LSTM, and it is a Recurrent Network of kind (RNN) with many dropouts activations. `drop_mult` is applied to all the dropouts weights of the config.\n",
        "```\n",
        "hidden_p:float=0.2, input_p:float=0.6, embed_p:float=0.1, weight_p:float=0.5\n",
        "using drop_mult=1.5 will basically set those to\n",
        "hidden_p:float=0.3, input_p:float=0.9, embed_p:float=0.15, weight_p:float=0.75\n",
        "```\n",
        "If you are overfitting, you will increase the dropout, and if underfitting, you will decrease the dropout. That says, having a small number of dropout gives a more accurate language model. Also, by experimentation from Jeremy, having more regularisations ends up not so good regarding the prediction of the next word for the language model but quite accurate for the classifier!.\n",
        "\n",
        "- `to_fp16()` indicates to fastai to run the model with mixed precision training.  Thus, intead of running the model on 32 bit float training (also called single precision float), it will run on 16 bit float training (also called half precision float), a nice feature to speed-up the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMIHmzc0CFLf"
      },
      "source": [
        "The final step prior to training the classifier is to load the encoder from our fine-tuned language model. We use `load_encoder` instead of `load` because we only have pretrained weights available for the encoder; `load` by default raises an exception if an incomplete model is loaded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExIlSmDgO93e"
      },
      "source": [
        "# loading the encoder from our fine-tuned language model.\n",
        "learn = learn.load_encoder('finetuned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMtbgb-kPBK_"
      },
      "source": [
        "NOTE: loading our fine-tuned language model to proceed to our text classification task.  Make sure the learner is created before loading the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkdE9vy7XbMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b241e38-0e26-4b5e-af35-932373a34625"
      },
      "source": [
        "# loading the model from gdrive\n",
        "model_save_name = 'finetuned.pth'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "learn.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCmpY2mwYSzg"
      },
      "source": [
        "> jargon: Encoder: The model not including the task-specific final layer(s). This term means much the same thing as _body_ when applied to vision CNNs, but \"encoder\" tends to be more used for NLP and generative models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-87k3E1CFLf"
      },
      "source": [
        "### Fine-Tuning the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64676-PwCFLf"
      },
      "source": [
        "The last step is to train with **discriminative learning rates and *gradual unfreezing***. In computer vision we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5M4UMWWCFLf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "b37c862b-5b00-4e89-e4a9-b2613661cc7f"
      },
      "source": [
        "# gradual unfreezing - unfreezing the final layer\n",
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.004397</td>\n",
              "      <td>0.503590</td>\n",
              "      <td>0.904680</td>\n",
              "      <td>01:42</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq89B5S9P8Fi"
      },
      "source": [
        "NOTE: by default the model's layers will be frozen, except for the final layer (the classification layer) that will be trained. We have 90% accuracy after just above ! mn processing for 1 epoch!."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfArscGNCFLg"
      },
      "source": [
        "In just one epoch we get the same result as our training in <<chapter_intro>>: not too bad! We can pass `-2` to `freeze_to` to freeze all except the last two parameter groups:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LBvzmq8CFLg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "1d493749-6587-496a-ed0a-2b8d77960550"
      },
      "source": [
        "# gradual unfreezing - unfreezing the last couple of layer groups\n",
        "learn.freeze_to(-2)\n",
        "# using discriminative learning rates\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.004086</td>\n",
              "      <td>0.546555</td>\n",
              "      <td>0.910840</td>\n",
              "      <td>01:56</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-FzWH_I2tbr"
      },
      "source": [
        "NB: `1e-2/(2.6**4)`= 0.000218; `1e-2`= 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoPKktGaCFLg"
      },
      "source": [
        "Then we can unfreeze a bit more, and continue training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBdqly8WCFLg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "8505c43f-9313-40c8-816c-a57182fdec2b"
      },
      "source": [
        "# gradual unfreezing - unfreezing the last three of layer groups\n",
        "learn.freeze_to(-3)\n",
        "# using discriminative learning rates\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.004330</td>\n",
              "      <td>0.576072</td>\n",
              "      <td>0.905760</td>\n",
              "      <td>02:41</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKtes1-UCFLg"
      },
      "source": [
        "And finally, the whole model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHG7R9QRCFLh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "e0dcefa6-9c31-4165-f974-e5ec0d15d498"
      },
      "source": [
        "# unfreezing\n",
        "learn.unfreeze()\n",
        "# using discriminative learning rates\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.003644</td>\n",
              "      <td>0.555775</td>\n",
              "      <td>0.909400</td>\n",
              "      <td>03:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.002081</td>\n",
              "      <td>0.566608</td>\n",
              "      <td>0.909360</td>\n",
              "      <td>03:15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaT2PJxTCFLh"
      },
      "source": [
        "We reached 94.3% accuracy, which was state-of-the-art performance just three years ago. By training another model on all the texts read backwards and averaging the predictions of those two models, we can even get to 95.1% accuracy, which was the state of the art introduced by the ULMFiT paper. It was only beaten a few months ago, by fine-tuning a much bigger model and using expensive data augmentation techniques (translating sentences in another language and back, using another model for translation).\n",
        "\n",
        "**Using a pretrained model let us build a fine-tuned language model that was pretty powerful, to either generate fake reviews or help classify them.** This is exciting stuff, but it's good to remember that this technology can also be used for malign purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxSzT7Y5LELs"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu1kPalEyqgv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9fdb54ed-6a00-4ce0-ce41-16fa6854f970"
      },
      "source": [
        "# amr predicting sentiment analysis\n",
        "learn.predict(\"I really loved that movie, it was awesome!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pos', tensor(1), tensor([7.3074e-09, 1.0000e+00]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "iRMQbNkGLJp8",
        "outputId": "c1c832a0-9b4b-45be-eeed-6a192137772d"
      },
      "source": [
        "from fastai.interpret import *\n",
        "#interp = Interpretation.from_learner(learn)\n",
        "interp = ClassificationInterpretation.from_learner(learn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "IASISgQH4kWI",
        "outputId": "52f3864a-e79f-4bb0-cf20-3167da5c010c"
      },
      "source": [
        "interp.plot_confusion_matrix()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WM4vKRNb510t",
        "outputId": "203bea36-3ef4-4f74-ce0d-1f47b78ce60d"
      },
      "source": [
        "interp.plot_top_losses(9, figsize=(15,11))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "      <th>predicted</th>\n",
              "      <th>probability</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos i really liked this quirky movie . xxmaj the characters are not the bland beautiful people that show up in so many movies and on xxup tv . xxmaj it has a realistic edge , with a captivating story line . xxmaj the main title sequence alone makes this movie fun to watch .</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.7421875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos xxmaj this was a very nice concert by the one and only xxup mj . xxmaj the choreography was excellent and the costumes were decent . xxmaj the vocals were okay . i have to admit that his vocals were crap on xxmaj human xxmaj nature and xxmaj billie xxmaj jean . xxmaj you could n't hear him half the time . xxmaj the other songs make up for the singing . xxmaj the xxmaj highlights of the show are : xxmaj jam xxmaj smooth xxmaj criminal i xxmaj just xxmaj ca n't xxmaj stop xxmaj loving xxmaj you xxmaj she 's xxmaj out xxmaj of xxmaj my xxmaj life xxmaj thriller xxmaj billie xxmaj jean ( the xxmaj dancing xxmaj not xxmaj the xxmaj singing ) xxmaj black or xxmaj white xxmaj man in the xxmaj mirror xxmaj the concert was almost perfect . xxmaj if it was</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.5234375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xxbos xxmaj this movie was pure genius . xxmaj john xxmaj waters is brilliant . xxmaj it is hilarious and i am not sick of it even after seeing it about 20 times since i bought it a few months ago . xxmaj the acting is great , although xxmaj ricki xxmaj lake could have been better . xxmaj and xxmaj johnny xxmaj depp is magnificent . xxmaj he is such a beautiful man and a very talented actor . xxmaj and seeing most of xxmaj johnny 's movies , this is probably my favorite . i give it 9.5 / 10 . xxmaj rent it today !</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.34375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xxbos xxmaj to surmise , this film involves two actors ( caine and xxmaj moran ) trying to con a gangster . xxmaj the plot is flimsy at best as several plot holes occur throughout . xxmaj however this normally should n't matter as the comedy should carry a film like this . xxmaj there are some genuinely funny bits ( mostly provided by xxmaj dylan xxmaj moran ) . xxmaj however , other times , there are long melodramatic scenes that fail to add anything to the movie . xxmaj caine 's character seemed overdone to me . xxmaj especially at the start , he continually quotes xxmaj shakespeare and acts like a pompous actor . xxmaj one could say he was playing the part properly but the character seemed to me flat and unfunny . xxmaj overall i would say see only if a fan of the actors</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.4921875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>xxbos xxmaj this film has the language , the style and the attitude down … plus greats rides from xxmaj xxunk ( a world champ ) and the great xxmaj jerry xxmaj lopez . xxmaj john xxmaj philbin as xxmaj turtle has the surf pidgin down , and the surfing scenes are still the best ever . a true classic that can be seen many times . xxmaj nia xxmaj peeples is a babe , and xxmaj laird xxmaj hamilton shows the early stuff that has made him the world 's number one extreme surfer .</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.2890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>xxbos xxmaj everything this film tried to do is done better - and superbly in \" run xxmaj lola xxmaj run \" . xxmaj the xxmaj red xxmaj haired xxmaj hip xxmaj cutie , the critical xxunk ) , xxmaj the xxmaj lover in jeopardy , and the \" crime pays - sometimes \" message . xxup but , unlike \" lola \" , it just is n't believable or well put together . xxmaj it is a labored knock off that might have worked for me if i had seen it before \" lola \" - but it pales in comparison . xxmaj yes ! xxmaj the xxmaj falling xxmaj beetle was nice ! xxmaj but that was about the only surprise in the film . xxmaj do yourself a favor and see the xxmaj real mccoy - ( and the xxup real hip xxmaj red xxmaj head !</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.15625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>xxbos xxmaj well , i must say , i initially found this short to be quite average , but having watched it nearly 5 times since ( its constantly shown on xxup ifc ) , xxmaj i 've developed an enjoyment of the simple plot elements and reality of the situations presented . xxmaj sofia xxmaj coppola contributes a solid addition to the category .</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.9765625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>xxbos xxmaj masterpiece . xxmaj carrot xxmaj top blows the screen away . xxmaj never has one movie captured the essence of the human spirit quite like \" chairman of the xxmaj board . \" 10 / 10 … do n't miss this instant classic .</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.7265625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>xxbos xxmaj just watched on xxunk this early experimental short film directed by xxmaj william xxmaj vance and xxmaj orson xxmaj welles . xxmaj yes , you read that right , xxmaj orson xxmaj welles ! xxmaj years before he gained fame for radio 's \" the xxmaj war of the xxmaj worlds \" and his feature debut xxmaj citizen xxmaj kane , xxmaj welles was a 19-year - old just finding his muse . xxmaj besides xxmaj vance and xxmaj welles , another player here was one xxmaj virginia xxmaj nicholson , who would become xxmaj orson 's first wife . xxmaj she plays a woman who keeps sitting on something that rocks back and forth courtesy of an african - american servant ( paul xxmaj edgerton in blackface ) . xxmaj during this time a man ( welles ) keeps passing her by ( courtesy of the scene</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.4765625</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlWlyqv2CFLh"
      },
      "source": [
        "# Disinformation and Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR2VjmK8CFLh"
      },
      "source": [
        "Even simple algorithms based on rules, before the days of widely available deep learning language models, could be used to create fraudulent accounts and try to influence policymakers. Jeff Kao, now a computational journalist at ProPublica, analyzed the comments that were sent to the US Federal Communications Commission (FCC) regarding a 2017 proposal to repeal net neutrality. In his article [\"More than a Million Pro-Repeal Net Neutrality Comments Were Likely Faked\"](https://hackernoon.com/more-than-a-million-pro-repeal-net-neutrality-comments-were-likely-faked-e9f0e3ed36a6), he reports how he discovered a large cluster of comments opposing net neutrality that seemed to have been generated by some sort of Mad Libs-style mail merge. In <<disinformation>>, the fake comments have been helpfully color-coded by Kao to highlight their formulaic nature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTrb7QZSCFLh"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/ethics/image16.png?raw=1\" width=\"700\" id=\"disinformation\" caption=\"Comments received by the FCC during the net neutrality debate\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNJrjvHeCFLh"
      },
      "source": [
        "Kao estimated that \"less than 800,000 of the 22M+ comments… could be considered truly unique\" and that \"more than 99% of the truly unique comments were in favor of keeping net neutrality.\"\n",
        "\n",
        "Given advances in language modeling that have occurred since 2017, such fraudulent campaigns could be nearly impossible to catch now.  **You now have all the necessary tools at your disposal to create a compelling language model—that is, something that can generate context-appropriate, believable text. It won't necessarily be perfectly accurate or correct, but it will be plausible.** Think about what this technology would mean when put together with the kinds of disinformation campaigns we have learned about in recent years. Take a look at the Reddit dialogue shown in <<ethics_reddit>>, where a **language model based on OpenAI's GPT-2 algorithm is having a conversation with itself about whether the US government should cut defense spending.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boV6XngNCFLh"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/ethics/image14.png?raw=1\" id=\"ethics_reddit\" caption=\"An algorithm talking to itself on Reddit\" alt=\"An algorithm talking to itself on Reddit\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZG8yftbCFLi"
      },
      "source": [
        "In this case, it was explicitly said that an algorithm was used, but imagine what would happen if a bad actor decided to release such an algorithm across social networks. They could do it slowly and carefully, allowing the algorithm to gradually develop followers and trust over time. It would not take many resources to have literally millions of accounts doing this. **In such a situation we could easily imagine getting to a point where the vast majority of discourse online was from bots, and nobody would have any idea that it was happening.**\n",
        "\n",
        "We are already starting to see examples of machine learning being used to generate identities. For example, <<katie_jones>> shows a **LinkedIn profile for Katie Jones.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qW_eJceCFLi"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/ethics/image15.jpeg?raw=1\" width=\"400\" id=\"katie_jones\" caption=\"Katie Jones's LinkedIn profile\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9QB7R4cCFLi"
      },
      "source": [
        "Katie Jones was connected on LinkedIn to several members of mainstream Washington think tanks. But she didn't exist. That image you see was auto-generated by a generative adversarial network, and somebody named Katie Jones has not, in fact, graduated from the Center for Strategic and International Studies.\n",
        "\n",
        "Many people assume or hope that algorithms will come to our defense here—that we will develop classification algorithms that can automatically recognise autogenerated content. The problem, however, is that this will always be an arms race, in which better classification (or discriminator) algorithms can be used to create better generation algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XghtrtWBCFLi"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1k_0tVhCFLi"
      },
      "source": [
        "In this chapter we explored the last application covered out of the box by the fastai library: text. We saw two types of models: language models that can generate texts, and a classifier that determines if a review is positive or negative. To build a state-of-the art classifier, we used a pretrained language model, fine-tuned it to the corpus of our task, then used its body (the encoder) with a new head to do the classification.\n",
        "\n",
        "Before we end this section, we'll take a look at how the fastai library can help you assemble your data for your specific problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66VpcI-gCFLi"
      },
      "source": [
        "## Questionnaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tRMfXj8CFLi"
      },
      "source": [
        "1. What is \"self-supervised learning\"?\n",
        "> Training a model without the use of labels. An example is a language model.\n",
        "1. What is a \"language model\"?\n",
        "> A language model is a self-supervised model that tries to predict the next word of a given passage of text.\n",
        "1. Why is a language model considered self-supervised?\n",
        "> There are no labels (ex: sentiment) provided during training. Instead, the model learns to predict the next word by reading lots of provided text with no labels.\n",
        "1. What are self-supervised models usually used for?\n",
        "> Sometimes, they are used by themselves. For example, a language model can be used for autocomplete algorithms! But often, they are used as a pre-trained model for transfer learning.\n",
        "1. Why do we fine-tune language models?\n",
        "> We can fine-tune the language model on the corpus of the desired downstream task, since the original pre-trained language model was trained on a corpus that is slightly different than the one for the current task.\n",
        "1. What are the three steps to create a state-of-the-art text classifier?\n",
        "> - Train a language model on a large corpus of text (already done for ULM-FiT by Sebastian Ruder and Jeremy!)\n",
        "> - Fine-tune the language model on text classification dataset\n",
        "> - Fine-tune the language model as a text classifier instead.\n",
        "1. How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n",
        "> By learning how to predict the next word of a movie review, the model better understands the language style and structure of the text classification dataset and can, therefore, perform better when fine-tuned as a classifier.\n",
        "1. What are the three steps to prepare your data for a language model?\n",
        "> - Tokenization\n",
        "> - Numericalization\n",
        "> - Language model DataLoader\n",
        "1. What is \"tokenization\"? Why do we need it?\n",
        "> Tokenization is the process of converting text into a list of words. It is not as simple as splitting on the spaces. Therefore, we need a tokenizer that deals with complicated cases like punctuation, hyphenated words, etc.\n",
        "> Also, tokenization is needed because we’re examining texts for patterns. It won’t be possible to find such patterns if we only consider a text at the sentence level, since patterns would only emerge if certain sentences frequently appeared together in sets, and this is a very rare phenomenon. Needless to say, examining texts in their entirety would be even less productive. We only find interesting patterns when we examine texts at the level of individual words, substrings, or individual characters.\n",
        "1. Name three different approaches to tokenization.\n",
        "> - Word-based tokenization\n",
        "> - Subword-based tokenization\n",
        "> - Character-based tokenization\n",
        "1. What is `xxbos`?\n",
        "> This is a special token added by fastai that indicated the beginning of the text (beginning of sequence).\n",
        "1. List four rules that fastai applies to text during tokenization.\n",
        "> Here are all the rules:\n",
        "> - fix_html :: replace special HTML characters by a readable version (IMDb reviews have quite a few of them for instance) ;\n",
        "> - replace_rep :: replace any character repeated three times or more by a special token for repetition (xxrep), the number of times it’s repeated, then the character ;\n",
        "> - replace_wrep :: replace any word repeated three times or more by a special token for word repetition (xxwrep), the number of times it’s repeated, then the word ;\n",
        "> - spec_add_spaces :: add spaces around / and # ;\n",
        "> - rm_useless_spaces :: remove all repetitions of the space character ;\n",
        "replace_all_caps :: lowercase a word written in all caps and adds a special token for all caps (xxcap) in front of it ;\n",
        "> - replace_maj :: lowercase a capitalized word and adds a special token for capitalized (xxmaj) in front of it ;\n",
        "> - lowercase :: lowercase all text and adds a special token at the beginning (xxbos) and/or the end (xxeos).\n",
        "1. Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\n",
        "> We can expect that repeated characters could have special or different meaning than just a single character. By replacing them with a special token showing the number of repetitions, the model’s embedding matrix can encode information about general concepts such as repeated characters rather than requiring a separate token for every number of repetitions of every character.\n",
        "1. What is \"numericalization\"?\n",
        "> This refers to the mapping of the tokens to integers to be passed into the model.\n",
        "1. Why might there be words that are replaced with the \"unknown word\" token?\n",
        "> If all the words in the dataset have a token associated with them, then the embedding matrix will be very large, increase memory usage, and slow down training. Therefore, only words with more than min_freq occurrence are assigned a token and finally a number, while others are replaced with the “unknown word” token.\n",
        "1. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book's website.)\n",
        "> - a. The dataset is split into 64 mini-streams (batch size)\n",
        "> - b. Each batch has 64 rows (batch size) and 64 columns (sequence length)\n",
        "> - c. The first row of the first batch contains the beginning of the first mini-stream (tokens 1-64)\n",
        "> - d. The second row of the first batch contains the beginning of the second mini-stream\n",
        "> - e. The first row of the second batch contains the second chunk of the first mini-stream (tokens 65-128)\n",
        "1. Why do we need padding for text classification? Why don't we need it for language modeling?\n",
        "> Since the documents have variable sizes, padding is needed to collate the batch. Other approaches. like cropping or squishing, either to negatively affect training or do not make sense in this context. Therefore, padding is used. It is not required for language modeling since the documents are all concatenated.\n",
        "1. What does an embedding matrix for NLP contain? What is its shape?\n",
        "> It contains vector representations of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens.\n",
        "1. What is \"perplexity\"?\n",
        "> Perplexity is a commonly used metric in NLP for language models. It is the exponential of the loss.\n",
        "1. Why do we have to pass the vocabulary of the language model to the classifier data block?\n",
        "> This is to ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LM fine-tuning.\n",
        "1. What is \"gradual unfreezing\"?\n",
        "> This refers to unfreezing one layer at a time and fine-tuning the pretrained model.\n",
        "1. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\n",
        "> The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7Xev-QOCFLj"
      },
      "source": [
        "### Further Research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpzyLh3VCFLj"
      },
      "source": [
        "1. See what you can learn about language models and disinformation. What are the best language models today? Take a look at some of their outputs. Do you find them convincing? How could a bad actor best use such a model to create conflict and uncertainty?\n",
        "1. Given the limitation that models are unlikely to be able to consistently recognize machine-generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leverage deep learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6WyPzieA3ta"
      },
      "source": [
        "# Readings\n",
        "- [fastai NLP](https://nlp.fast.ai/)\n",
        "- [ULMFiT paper](https://arxiv.org/pdf/1801.06146.pdf)- [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
        "- [fastai-transformers tutorial](https://docs.fast.ai/tutorial.transformers.html)\n",
        "- [fastai+transformers](https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2)\n",
        "\n",
        "- [Understanding fastai `fit_one_cycle`](https://iconof.com/1cycle-learning-rate-policy/)\n",
        "\n",
        "- [The state of transfer learning in NLP](https://ruder.io/state-of-transfer-learning-in-nlp/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsIaH-gXjKXK"
      },
      "source": [
        "# NLP Blogs\n",
        "Good NLP-related blogs:\n",
        "\n",
        "- [Sebastian Ruder](https://ruder.io/); [NLP newsletter](https://newsletter.ruder.io/)\n",
        "- [Joyce Xu](https://www.joycexu.io/)\n",
        "- [Jay Alammar](https://jalammar.github.io/)\n",
        "- [Stephen Merity](https://state.smerity.com/)\n",
        "- [Rachael Tatman](http://www.rctatman.com/)"
      ]
    }
  ]
}