{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_FR_TextClassification_allocine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1yJswghiMf5LHUKaiQWylHqkhS-tplN30",
      "authorship_tag": "ABX9TyMqDn9g9k7gwwlfJAUBfpBk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/fastai2-v4/blob/master/GPT2_FR_TextClassification_allocine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rt1mPbap1Xu"
      },
      "source": [
        "# OpenAI GPT2 model Infos\n",
        "[Model Info](https://huggingface.co/transformers/model_doc/gpt2.html)\n",
        "\n",
        "[The Illustrated GPT-2 (Visualizing Transformer Language Models)](http://jalammar.github.io/illustrated-gpt2/#model-output)\n",
        "\n",
        "GPT-2 is a large transformer-based language model with **1.5 billion parameters**, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. \n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFR2r-YCNR0c"
      },
      "source": [
        "# Infos Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZwyf9TCnzRC"
      },
      "source": [
        "This notebook is used to fine-tune GPT2 model for text classification using Huggingface transformers library on a custom dataset.\n",
        "\n",
        "For the dataset, we are using the French AlloCine reviews, which are French reviews on films. \n",
        "\n",
        "Main idea: is to use a GPT2 model that has been pretrained with French texts, and then fine-tune this language model on a sub-domain which are the French reviews from AlloCine website.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_gVwEfqd0Ng"
      },
      "source": [
        "# !nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKWtHnZ_yZ_L"
      },
      "source": [
        "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6UAMPRiyuJ1"
      },
      "source": [
        "!pip install -Uq transformers\n",
        "from fastai.text.all import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haXlok9TMSJr"
      },
      "source": [
        "# BelGPT-2 - a GPT-2 model pre-trained on French corpora\n",
        "This language model has been trained on French texts from different sources: Wikipedia, news, EuroParl texts. Its size is 60Gb.\n",
        "\n",
        "https://github.com/antoiloui/belgpt2/blob/master/docs/index.md\n",
        "\n",
        "https://huggingface.co/antoiloui/belgpt2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX0goP1nMlIv"
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, GPT2Config"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYsLRHbh5mRq"
      },
      "source": [
        "model_name = \"antoiloui/belgpt2\"\n",
        "ds_name = ''\n",
        "\n",
        "max_len = 256\n",
        "bs = 4\n",
        "val_bs = bs*2\n",
        "\n",
        "lr = 2e-5"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUHanvZOC0ZG"
      },
      "source": [
        "# # Initializing a GPT2 configuration\n",
        "# configuration = GPT2Config()\n",
        "\n",
        "# # Initializing a model from the configuration\n",
        "# model = GPT2ForSequenceClassification(configuration)\n",
        "\n",
        "# # Accessing the model configuration\n",
        "# configuration = model.config; configuration"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WX6Y04Ec_uL",
        "outputId": "bec48e4d-28e0-4cf6-ef4c-ba644ae6bbee"
      },
      "source": [
        "#tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token='<pad>')\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.model_max_len = 512\n",
        "model = GPT2ForSequenceClassification.from_pretrained(model_name)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at antoiloui/belgpt2 were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at antoiloui/belgpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcMbvtuzDOgU",
        "outputId": "16a6622b-2bca-4264-c89e-63bb7a988e0a"
      },
      "source": [
        "model.config"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"_name_or_path\": \"antoiloui/belgpt2\",\n",
              "  \"_num_labels\": 2,\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"architectures\": [\n",
              "    \"GPT2LMHeadModel\"\n",
              "  ],\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 50256,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 50256,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_ctx\": 1024,\n",
              "  \"n_embd\": 768,\n",
              "  \"n_head\": 12,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 12,\n",
              "  \"n_positions\": 1024,\n",
              "  \"output_past\": true,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"transformers_version\": \"4.6.1\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50257\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLwAp4hMpxff"
      },
      "source": [
        "### Allocine Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFzznGPY5bH4"
      },
      "source": [
        "#creating a directory allocine\n",
        "path = Path('/content/allocine/')\n",
        "path.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b55p-OxvNz8F"
      },
      "source": [
        "#creating a directory allocine\n",
        "path = Path('/content/allocine/models')\n",
        "path.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SncnpDS181gg",
        "outputId": "d97924a7-4e4c-492e-954a-62fb23682e13"
      },
      "source": [
        "path = Path('/content/allocine/'); path"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/content/allocine')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhEsxocL2UVA"
      },
      "source": [
        "# # downloading the AlloCine dataset\n",
        "# !wget -q https://github.com/TheophileBlard/french-sentiment-analysis-with-bert/raw/master/allocine_dataset/data.tar.bz2\n",
        "# !tar -xf /content/data.tar.bz2 -C '/content/allocine'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "rbTDvZKfAQtd",
        "outputId": "1d06df82-1e65-4268-a639-00d9cc4cc926"
      },
      "source": [
        "train_df = pd.read_json(path/'data/train.jsonl', lines=True, nrows=10000)\n",
        "train_df.head(1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>film-url</th>\n",
              "      <th>review</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-135259/critiques/spectateurs</td>\n",
              "      <td>Si vous cherchez du cinéma abrutissant à tous les étages,n'ayant aucune peur du cliché en castagnettes et moralement douteux,\"From Paris with love\" est fait pour vous.Toutes les productions Besson,via sa filière EuropaCorp ont de quoi faire naître la moquerie.Paris y est encore une fois montrée comme une capitale exotique,mais attention si l'on se dirige vers la banlieue,on y trouve tout plein d'intégristes musulmans prêts à faire sauter le caisson d'une ambassadrice américaine.Nauséeux.Alors on se dit qu'on va au moins pouvoir apprécier la déconnade d'un classique buddy-movie avec le jeun...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             film-url  ... polarity\n",
              "0  http://www.allocine.fr/film/fichefilm-135259/critiques/spectateurs  ...        0\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4SjmKIYWDRm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "2c454628-cfbd-4776-d191-6ed4ae4b3dc6"
      },
      "source": [
        "train_df.to_csv(path/'data/train.csv', encoding = 'utf-8', header = True, index = False)\n",
        "train_df.head(2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>film-url</th>\n",
              "      <th>review</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-135259/critiques/spectateurs</td>\n",
              "      <td>Si vous cherchez du cinéma abrutissant à tous les étages,n'ayant aucune peur du cliché en castagnettes et moralement douteux,\"From Paris with love\" est fait pour vous.Toutes les productions Besson,via sa filière EuropaCorp ont de quoi faire naître la moquerie.Paris y est encore une fois montrée comme une capitale exotique,mais attention si l'on se dirige vers la banlieue,on y trouve tout plein d'intégristes musulmans prêts à faire sauter le caisson d'une ambassadrice américaine.Nauséeux.Alors on se dit qu'on va au moins pouvoir apprécier la déconnade d'un classique buddy-movie avec le jeun...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-172430/critiques/spectateurs</td>\n",
              "      <td>Trash, re-trash et re-re-trash...! Une horreur sans nom. Imaginez-vous les 20 premières minutes de Orange Mécanique dilatées sur plus de 70 minutes de bande VHS pourrave et revisitées par Korine à la sauce années 2000 : les dandys-punk de Kubrick ont laissé place à des papys lubriques déguisés en sacs-poubelles forniquant les troncs d'arbres, le dispositif esthétique se résume à du filmage-réalité enfilant des scènes de destruction, de soumission, de pornographie ou encore de maltraitance ( youtube, youtube et re-youtube...) et la bande-son se limite à des ricanements malades, des rengaine...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             film-url  ... polarity\n",
              "0  http://www.allocine.fr/film/fichefilm-135259/critiques/spectateurs  ...        0\n",
              "1  http://www.allocine.fr/film/fichefilm-172430/critiques/spectateurs  ...        0\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubUTgv_EcU8q"
      },
      "source": [
        "# # splitting a df by rows\n",
        "# df_train = df.iloc[10001:-1]; len(df_train)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt3nmIxvxwzK"
      },
      "source": [
        "## Text Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80eX8senlzka",
        "outputId": "196b1406-5ba9-4549-ee25-a188e4aaea4f"
      },
      "source": [
        "tokenizer.vocab_size"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQizUHn-3n1_",
        "outputId": "4d15bc94-3a77-4b3c-8256-575a6fa88837"
      },
      "source": [
        "tokenizer.pad"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method PreTrainedTokenizerBase.pad of PreTrainedTokenizer(name_or_path='antoiloui/belgpt2', vocab_size=50257, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='left', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>'})>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ptsiVk5pG8u",
        "outputId": "dd304be3-140d-40ec-f82a-2082368ead64"
      },
      "source": [
        "print(tokenizer.eos_token)\n",
        "print(tokenizer.bos_token)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<|endoftext|>\n",
            "<|endoftext|>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx5BbfD4ZGxd"
      },
      "source": [
        "# class TransformersTokenizer(Transform):\n",
        "#     def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
        "#     def encodes(self, x): \n",
        "#         toks = self.tokenizer.tokenize(x)\n",
        "#         return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n",
        "#     def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHFfQ94Of9wo"
      },
      "source": [
        "class TransformersTokenizer(Transform):\n",
        "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
        "    def encodes(self, x): \n",
        "        toks = self.tokenizer.tokenize(x)\n",
        "        return TensorText(self.tokenizer.convert_tokens_to_ids(toks))\n",
        "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phKH_vpIEKqV",
        "outputId": "c3543d8b-f252-4675-d07c-0ebb2e8dd352"
      },
      "source": [
        "# testing the tokenizer \n",
        "tokenizer_fastai_fr = TransformersTokenizer(tokenizer)\n",
        "text = \"Peut-être que vous avez raison\"\n",
        "tokens_ids = tokenizer_fastai_fr.encodes(text)\n",
        "tokens = tokenizer_fastai_fr.tokenizer.convert_ids_to_tokens(tokens_ids)\n",
        "\n",
        "print('input text:',TitledStr(text))\n",
        "print('text tokens:',TitledStr(tokens))\n",
        "print('text tokens_ids:',TitledStr(tokens_ids))\n",
        "print('output text:',TitledStr(tokenizer_fastai_fr.decodes(tokens_ids)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input text: Peut-être que vous avez raison\n",
            "text tokens: ['Peut', '-', 'Ãªtre', 'Ġque', 'Ġvous', 'Ġavez', 'Ġraison']\n",
            "text tokens_ids: TensorText([46906,    15,  1519,   354,   472,  1578,  1835])\n",
            "output text: Peut-être que vous avez raison\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGshWi4o31cm"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6zi-Pxtibkk"
      },
      "source": [
        "# from transformers import AutoModelForSequenceClassification"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0ZBxEJcrVCH",
        "outputId": "e0b64ac7-1636-4542-9475-4edbab7eac80"
      },
      "source": [
        "bs"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-um-c8pP2tI"
      },
      "source": [
        "class HFTextBlock(TransformBlock):\n",
        "    \"A `TransformBlock` for texts\"\n",
        "    def __init__(self, tokenizer):\n",
        "        type_tfms = TransformersTokenizer(tokenizer)\n",
        "        pad_first = tokenizer.padding_side == 'left'\n",
        "        #pad_first = tokenizer.padding_side=='left')\n",
        "        return super().__init__(type_tfms=type_tfms,\n",
        "                                dl_type= SortedDL,\n",
        "                                dls_kwargs={'before_batch':Pad_Chunk(pad_idx = tokenizer.pad_token_id, pad_first=(tokenizer.padding_side=='left'))})\n",
        "        "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aER1jFau2MHu"
      },
      "source": [
        "# bs,sl = 8, 256"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ropWZMhvlYm9"
      },
      "source": [
        "dls_clas = DataBlock(\n",
        "        blocks=(HFTextBlock(tokenizer), CategoryBlock),\n",
        "        get_y=ColReader('polarity'), \n",
        "        get_x=ColReader('review'), \n",
        "        splitter=RandomSplitter(),\n",
        "\n",
        ").dataloaders(train_df, bs=4)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg6Yo5vDEN7S"
      },
      "source": [
        "# xb,yb=dls_clas.one_batch()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzFGUavO_H60"
      },
      "source": [
        "NOTE:  Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
        "\n",
        "Using pad_token, but it is not set yet.\n",
        "Could not do one pass in your dataloader, there is something wrong in it\n",
        "\n",
        "CPU times: user 10.4 s, sys: 35.5 ms, total: 10.4 s\n",
        "Wall time: 10.5 s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToGfw0Wn7xrs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "87681f6b-c2cd-4fcb-933f-796b1b21b6bc"
      },
      "source": [
        "dls_clas.show_batch(max=3)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Le souffle d’une peau blême laisse vibrer sur la platitude de l’écran les vrombissements intimes de l’érotisme. «The Docks of New York» (USA, 1928) de Josef von Sternberg délaissent les corps et leur chair dans les brumes opaques des ports marins. La poudre qui recouvre le corps mutique de Betty Compson l’a confond avec la pâleur éblouissante des angelots de Michel-Ange. Cette connivence ne va pas plus loin. L’innocence des chérubins est absente du corps de Mae au profit d’une sensibilité lubrique. Les jeux de séduction que se livre Compson et l’imposant Geroge Bancroft se résume à la correspondance de la peau féminine frêle et incandescente avec les yeux masculins limpides. Un objet du désir, le corps de la femme, se mue en écran (en adopte du moins les caractéristiques blafarde) pour mieux se laisser voir, admirer, palper et convoiter par le regard de l’homme. Sternberg, à l’orée de la</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'Crazy in Alabama’ a été pour moi un véritable coup de cœur… littéraire. Là où le roman de M.Childress brille par un style d’écriture alerte et vivace, le film pêche par un travail presque trop propre et soigné qui lui en fait perdre un peu d’âme et de saveur. A la manière de ‘Beignets de tomates vertes’, ce film se laisse agréablement regarder grâce à un rythme enlevé et une belle reconstitution des années 60, mais le petit grain de folie du roman qui m’a souvent déridé les maxillaires a disparu. Les quelques libertés prises par Banderas ne sont pas étrangères à ma rancœur : exit les situations cocasses, les petits détails croustillants et les dialogues qui font mouche, changement de couleur capillaire pour Lucille, changement de série… En fait, j’aurais surtout aimé que Banderas accentue le contraste émotionnel de ces 2 histoires en amplifiant le ton loufoque de la</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Les trois seuls films de Guédiguian que j'avais vu (A la vie à la mort, Marius et Jeannette et Le promeneur du champ de Mars) ne m'avaient pas vraiment emballé. Mais un début de bon bouche à oreille, malgré une bande-annonce pas très accrocheuse, m'a poussé à aller voir son dernier opus. Grand bien m'en prit car Lady Jane est une excellente surprise et j'ai été scotché au siège pendant 1h40. En s'attaquant au polar, le metteur en scène marseillais signe un film fort, dur, noir. Tous les codes du genre sont réunis, dans un scénario écrit au couteau, qui dès les premières minutes intrigue, étonne, et offre une scène choc, terrible à glacer le sang, qui nous emporte définitivement en nous liant au destin de cette femme que son passé rattrape tout d'un coup sans prévenir. L'histoire en devient alors passionnante et l'on suit les prérégrinations du trio d'amis</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Je me rends compte que ça aurait pu être sympa, au début, si ça continuait dans le thème ton... Un couple qui déménage avec un ami... Le couple qui est voué à se séparer parce que le mec est victime d'une maladie incurable et ne veut pas être un boulet pour sa copine... Sentir le désir, l'amour, qui reste malgré tout... avec peut-être un McGuffin dont finalement on finit par se foutre totalement pour se concentrer sur les personnages... un peu comme dans Monsters... Sauf que non... Ben oui! ça aurait été trop bien... Non... On veut faire... faire quoi d'ailleurs? Je ne sais même pas ce que le film tente de faire tant c'est foutraque et n'importe quoi. Si le premier quart d'heure est convenable (j'ai dit convenable, pas bon), ça devient vite tellement le bordel, avec une sorte de pseudo mystère bien chiant, puisqu'on capte après une dizaine</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VamGGio76Rl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2377bfa2-08da-418d-a2fa-d23f702a2a05"
      },
      "source": [
        "print(len(dls_clas.train), len(dls_clas.valid))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bK6YaZt7_qw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc48c36d-1c91-43c0-a77c-f7f531577830"
      },
      "source": [
        "dls_clas.c, dls_clas.vocab"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, [0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-WgNWdj8m7K"
      },
      "source": [
        "## Text Learner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BjTupXEBPd_"
      },
      "source": [
        "### CUDA out of memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yyibSsA96P6"
      },
      "source": [
        "# del learn\n",
        "# del model_name\n",
        "# torch.cuda.empty_cache()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t62Pu-PR8C8p"
      },
      "source": [
        "def default_splitter(model):\n",
        "    groups = L(model.base_model.children()) + L(m for m in list(model.children())[1:] if params(m))\n",
        "    return groups.map(params)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCTZHdsJN6IF"
      },
      "source": [
        "class DropOutput(Callback):\n",
        "    def after_pred(self): self.learn.pred = self.pred[0]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5_NOuNc-7fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b59651-497a-47a0-c7b3-197b88e78768"
      },
      "source": [
        "model = GPT2ForSequenceClassification.from_pretrained(model_name, pad_token_id=tokenizer.pad_token_id)\n",
        "learn = Learner(dls_clas, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=accuracy).to_fp16() "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at antoiloui/belgpt2 were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at antoiloui/belgpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7sDh63Y9p0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf09d1bf-1c35-44b2-b8cc-09a4209731b8"
      },
      "source": [
        "learn.model"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2ForSequenceClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjo5PVYN9uN_"
      },
      "source": [
        "# learn.lr_find()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8cO7X4O92V1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "49838e6c-8c14-467f-8318-802c560b6027"
      },
      "source": [
        "learn.fit_one_cycle(1, lr)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.151146</td>\n",
              "      <td>0.143197</td>\n",
              "      <td>0.942500</td>\n",
              "      <td>04:17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gylXNDhltmKM"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWHE3irjBHWi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87395aee-0b1e-4353-b45e-35089a1ce0cc"
      },
      "source": [
        "# amr predicting sentiment analysis\n",
        "learn.predict(\"Ce film est vraiment marrant\")\n",
        "#learn.predict(\"Ce film est un navet\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1', TensorText(1), TensorText([0.0433, 0.9567]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "pjgzNUmXvGz6",
        "outputId": "73bf892e-a8b6-464c-94cf-f201e181325e"
      },
      "source": [
        "# amr predicting sentiment analysis\n",
        "#learn.predict(\"Ce film est vraiment marrant\")\n",
        "learn.predict(\"Ce film est un navet\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('0', TensorText(0), TensorText([0.9451, 0.0549]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "cO4mlsqDg7_e",
        "outputId": "da71096f-25f0-4031-abc5-b6dd73b5d560"
      },
      "source": [
        "learn.show_results()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>category_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Allez… Je suis parvenu à y croire vingt minutes… Et encore, en faisant preuve de beaucoup d’ouverture d’esprit… Parce que bon, on ne va pas se la faire à l’envers : ce « Life », ce n’est quand même vraiment pas grand-chose en termes de créativité et d’émotion transmise. En gros, c’est un peu d’ « Alien », mélangé avec un peu de « Sunshine », et le tout colmaté avec toutes les ficelles des survivals habituels. Donc voilà quoi… Tant que la forme de vie extra-terrestre ne s’était pas révélée une forme de vie tueuse, j’arrivais encore à me laisser porter par l’élan mou de ce film. Bah ouais : moi je suis comme ça… Il suffit qu’on aborde un sujet comme la découverte d’une nouvelle forme de vie pour que je m’imagine toutes les possibilités créatives, philosophiques, visuelles que ça ouvre… Mais bon… Visiblement fallait pas déconner non</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Sa Rang qui se fait kidnappé\"? Hum, kidnapper, plutôt... Que dire, que dire de ce \"film\"?...? Hé bien, il existe des mauvais films américains, des mauvais films français, des mauvais films de toute nationalité, et là, c'est un mauvais film coréen. Très mauvais, même. En plus du jeu des acteurs, pas top du tout (pour être gentil), les doublages français ruinent toute leur crédibilité, au point où tout cela amène à croire que c'est limite une blague, tellement le doublage est raté. Parce que, que les voix soient nulles, c'est une chose, mais que les paroles le soient aussi, là, ça en devient navrant. Et ayant téléchargé ce film parce que Steven Seagal tenait le second rôle d'après la description du casting d'Allociné, je me suis aperçu que le site avait récidivé dans sa manière parfois très étrange de lister le casting. Car oui, parfois, on se retrouve avec des</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dans la famille Makhmalbaf (dont la fille Samira est indéniablement la plus talentueuse), Hana tente de creuser dans le métier qui a fait la réputation de la famille, du père à la fille. Ici, la réalisatrice tente d'observer la violence de son pays et le désastre des Talibans et des américains via de jeunes personnages. Si l'idée est bonne, et que certaines scènes arrivent à faire naître une tension particulière, le film se fait vite prendre au piège du discours enfantin. Car le monde des enfants - hormis leurs rêves, on y trouve l'incompréhension face aux adultes, le manque de communication et la dureté de vie dans un paysage fauché par la peur - invite forcément à une tendance symboliste. Et Makhmalbaf en abuse, justement, jouant de plus en plus progressivement sur l'altercation manichéenne entre l'héroïne - représentant à elle seule le peuple blotti dans la peur -, et les</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ce qui m'a marqué dans ce film se déroulant dans une Normandie obscure au milieu du XIème siècle, c'est la superbe photo de Russell Metty, un grand nom de la discipline assez méconnu, venu de la série B, mais qui avait déjà éclairé Charlton Heston dans \"La soif du Mal\" d'Orson Welles. Oui, ça pose le bonhomme. Et Heston, veillait au grain sur les différents aspects des films qu'il produisait (comme c'est le cas ici), et ce, à tous les niveaux de production. Le choix de Metty est donc une marque de respect (et il en fera de même en imposant Schaffner sur \"La planète des Singes\" quelques années plus tard). Au début, c'est un peu mou, la mise en place de l'histoire d'amour torturée entre le seigneur campé par Heston et la belle villageoise (Rosemary Forsyth), ainsi que les tensions naissantes entre le perso de Heston et son frère</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl7AsyNuvdgR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad617AZxBcbw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f7ca1333-29c1-4824-cb67-0dd06e4da223"
      },
      "source": [
        "from fastai.interpret import *\n",
        "#interp = Interpretation.from_learner(learn)\n",
        "interp = ClassificationInterpretation.from_learner(learn)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ADLgDfoBfdM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "34a34a7a-0c46-4ffa-8de2-b63033aa6595"
      },
      "source": [
        "interp.plot_confusion_matrix()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyYGd_7yBi6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72860b98-1b41-462c-eef7-b4247bb6f6cd"
      },
      "source": [
        "interp.plot_top_losses(9, figsize=(15,11))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "      <th>predicted</th>\n",
              "      <th>probability</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Un très bon film, très drôle, une heure et demie de détente sure. Tous les acteurs jouent formidablement pour nous offrir le renouement d'un nouveau groupe de rock. Bravo tout particulièrement à Eléonore Pourriat et Juie Depardieu. Film à voir et à délecter avec humour.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9996533393859863</td>\n",
              "      <td>7.967143535614014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Libero\"est le meilleur exemple de la renaissance du cinéma italien.Drame familial proche du thème de \"la chambre du fils\",il traduit très bien la complexité des rapports entre un enfant de 11 ans introverti et son père colérique et généreux.Une famille monoparentale qui ne peut tenir que par l'amour et les attentions les uns envers les autres.Tableau vivant très réaliste sur l'ouverture au monde extérieur lors de la pré-adolescence.Encore une fois,le film est d'une grande qualité.Ma note correspond juste à mon ressenti personnel.Histoire de gout en fait.Ceci étant dit,la pudeur et la justesse de cette histoire font affleurer l'émotion.Pas si courant de nos jours.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9962855577468872</td>\n",
              "      <td>5.595518112182617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>alors une fois vu les comédie musical a Londres ceux de France je les trouve nul la je suis allez voir en me disant encore une comédie musical nul ben bravo obispo et toute la troupe enfin une comédie musical a la hauteur de celle que savait vu a Londres et j'espère même venir vous voir en live si j'arrive a trouve des place du début jusque a la fin s'était dedans a si un defaux trop court sa passe trop vite</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9947900772094727</td>\n",
              "      <td>5.257176876068115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3D relativement captivante sur un rythme quelque peu répétitif. On se surprend cependant à être attiré, submergé par cette ambiance unique dans ce lieu magique et finalement intemporel. Plus une expérience, qu’un moment de cinéma inoubliable. Herzog s’éloignant un tant soit peu de son sujet lors d’un monologue final.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9879345893859863</td>\n",
              "      <td>4.417412281036377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>L’histoire se veut avant tout originale, belle, poétique et portée par une musique envoutante. Le Violon rouge (1999) nous emmène à travers différentes époques, pays et continents, aux côtés de cinq destins différents, on suit le parcourt incroyable de ce violon d’exception. De siècles en siècles, de mains en mains, cette fresque musicale est passionnante, tous vouent une fascination pour cet instrument, ouvrez grands vos oreilles et laissez vous bercer, c’est le meilleur conseil que l’on puisse vous donner! (le film a reçu l’Oscar de la Meilleure Musique Originale). Autre originalité du film, son casting, puisqu’il réunit des acteurs chinois, québécois, français (Jean-Luc Bideau de la série télévisée H) et américain (l’excellent Samuel L. Jackson). Seul regret, la mise en scène un peu longuet et répétitive par moment.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9783433079719543</td>\n",
              "      <td>3.832441568374634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Le barbier de Sibérie fait partie de ces films complétement atypiques, impossibles à ranger dans telle ou telle catégorie. Disons, pour essayer d'en faire une description, qu'il s'agit d'une fresque romantique burlesque et dramatique. La plupart des scènes sont totalement déjanté, et les personnages sont tous plus tordus les uns que les autres. Dans cet univers assez extrème (qui colle assez bien à la Russie), difficile de se sentir parfaitement à son aise, et on accroche par intermitence. Et forcement, le risque est grand pour les acteurs de surjouer. Mais au final, on est toutefois content d'avoir vu ce film si particulier et non sans charme dont on se souviendra un bon moment encore...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9782188534736633</td>\n",
              "      <td>3.826709270477295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Ca m'a bien fait rire, c'est pas prise de tête mais c'est pas crédible un seul instant, et on s'en fout parce qu'on se marre bien.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9759693741798401</td>\n",
              "      <td>3.7284257411956787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Elie Semoun adapte au cinéma l’un de ses personnages phare de ses « Petites Annonces », avec le dénommé Cyprien, le looser bigleux et geek qui recherche inlassablement « une blonde à forte poitrine ». Comme on s’y attendait, l’humour est au rendez-vous, toujours enfantin, jamais vulgaire. Les répliques fusent (et font preuves d’originalitées), au gré des diverses situations coquasses. Les générations 70 &amp; 80 s’y retrouveront, les plus jeunes auront quant à eux plus de mal à faire références aux nombreux clins d’œil (jeux vidéos, films, musiques, etc), mais cela ne les empêchera pas d’en rire! Elie Semoun qui interprète à la fois Cyprien et son alter ego, prouve une fois de plus ses talents de comique aux côtés de Léa Drucker, Laurent Stocker, Vincent Desagnat, Mouloud Achour &amp; Catherine Deneuve. Si le film accuse quelques longueurs ou gags redondants, il n’en reste pas moins drôle, gentillet et familiale!</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9678385257720947</td>\n",
              "      <td>3.4369869232177734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Première collaboration pour Jack Nicholson &amp; le réalisateur Monte Hellman. Une contribution qui leur sera bénéfique puisqu’ils se retrouveront l’année suivante pour The Shooting (1967). Avec L'Ouragan de la vengeance (Ride in the Whirlwind) (1965) (1966), à l’initiative des deux protagonistes, on retrouve Nicholson à la production, au scénario et en tête d’affiche, sous l’œil avisé de Hellman. A eux deux, ils vont nous raconter l’itinéraire désastreux de trois cow-boys, qui tombèrent nez à nez avec des hors la loi. Pris pour des complices, ils se retrouvent eux aussi pourchassés par la milice et doivent tout tenter pour s’enfuir s’ils veulent éviter la pendaison. S’engage alors dans la désert aride, une chasse à l’homme ne laissant très peu de chance à ses innocents de s’en sortir indemne. Avec un budget dérisoire, une histoire simple mais revendicatrice, le résultat est tout bonnement réussit. Seul la fin nous laissera (hélas) un arrière</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.967625081539154</td>\n",
              "      <td>3.4303715229034424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhPNqS4QndmJ"
      },
      "source": [
        "## NOTE\n",
        "https://github.com/huggingface/transformers/issues/2001\n",
        "\n",
        "So there are still things unclear, but from reading other issues this is my current understanding:\n",
        "\n",
        "GPT2 has no padding token, as it was trained on documents and not sentences.\n",
        "In order to use GPT2 with variable length inputs, we can apply padding with an arbitrary token and ensure that those tokens are not used by the model with an attention_mask.\n",
        "As for the labels, we should replace only on the labels variable the padded token ids with -1.\n",
        "So based on that, here is my current toy implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WHWsvoQnf5J"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}