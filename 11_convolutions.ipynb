{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "11_convolutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/fastai2-v4/blob/master/11_convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kDNXMrChk3xM",
        "outputId": "f6b4c675-30af-470b-fcc9-a69db48f7489"
      },
      "source": [
        "# imports\n",
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "34CKf7qpk3xO"
      },
      "source": [
        "# imports\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "\n",
        "matplotlib.rc('image', cmap='Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECVNUFNsk3xP"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qlHW7udlvjV"
      },
      "source": [
        "This lesson represents chapter 13 of the book [Deep Learning for Coders with Fastai & Pytorch](https://www.amazon.fr/Deep-Learning-Coders-Fastai-Pytorch/dp/1492045527) by Jeremy Howard and Sylvain Gugger,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb_cbI03k3xQ"
      },
      "source": [
        "In lesson 4 we learned how to create a neural network recognizing images. We were able to achieve a bit over 98% accuracy at distinguishing 3s from 7s—but we also saw that fastai's built-in classes were able to get close to 100%. Let's start trying to close the gap.\n",
        "\n",
        "In this chapter, we will begin by digging into what convolutions are and building a CNN from scratch. We will then study a range of techniques to improve training stability and learn all the tweaks the library usually applies for us to get great results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFFGccplk3xR"
      },
      "source": [
        "## The Magic of Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvJwRRmwk3xR"
      },
      "source": [
        "One of the most powerful tools that machine learning practitioners have at their disposal is ***feature engineering***. A ***feature*** is a transformation of the data which is designed to make it easier to model. For instance, the `add_datepart` function that we used for our tabular dataset preprocessing in lessons 6 and 7 added date features to the Bulldozers dataset. What kinds of features might we be able to create from images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH68ndMgk3xR"
      },
      "source": [
        "> jargon: **Feature engineering**: Creating new transformations of the input data in order to make it easier to model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMaFd4uqk3xR"
      },
      "source": [
        "**In the context of an image, a feature is a visually distinctive attribute, a pattern.** For example, the **number 7** is characterized by a horizontal edge near the top of the digit, and a top-right to bottom-left diagonal edge underneath that. On the other hand, the **number 3** is characterized by a diagonal edge in one direction at the top left and bottom right of the digit, the opposite diagonal at the bottom left and top right, horizontal edges at the middle, top, and bottom, and so forth. So what if we could extract information about where the edges occur in each image, and then use that information as our features, instead of raw pixels?\n",
        "\n",
        "It turns out that **finding the edges in an image is a very common task in computer vision**, and is surprisingly straightforward. To do it, we use something called a ***convolution***. A convolution requires nothing more than **multiplication, and addition ** — two operations that are responsible for the vast majority of work that we will see in every single deep learning model in this book!\n",
        "\n",
        "A convolution applies a ***kernel*** across an image. A kernel is a little matrix, such as the 3×3 matrix in the top right of the figure below: applying a kernel to one location.\n",
        "\n",
        "*NB1: we have different names for kernel, it is also called filter or feature detector.*\n",
        "\n",
        "*NB2: the input image fragment that is processed is called the input feature map.*\n",
        "\n",
        "*NB3: The ouput of a convolution is called a feature map; when the activation function ReLu is applied to the output, we call the output of the convolution an activation map.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IRcPJCQk3xS"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter9_conv_basic.png?raw=1\" id=\"basic_conv\" caption=\"Applying a kernel to one location\" alt=\"Applying a kernel to one location\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TScSOj3wk3xS"
      },
      "source": [
        "The 7×7 grid to the left is the ***image*** we're going to apply the kernel to. \n",
        "\n",
        ">The convolution operation multiplies each element of the kernel by each element of a 3×3 block of the image. The results of these multiplications are then added together. The diagram in above shows an example of applying a kernel to a single location in the image, the 3×3 block around cell 18.\n",
        "\n",
        "Let's do this with code. First, we create a little 3×3 matrix like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GraqyVvNk3xS"
      },
      "source": [
        "# creating a 3x3 matrix also called a kernel in CV\n",
        "top_edge = tensor([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]]).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA7zDO3Ek3xS"
      },
      "source": [
        "We're going to call this our **kernel** (because that's what fancy computer vision researchers call these). And we'll need an image, of course:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1lR5-OjPk3xS",
        "outputId": "87709869-6a66-4c12-df45-d01f7c868c15"
      },
      "source": [
        "# downloading the MNIST dataset sample of 3s and 7s\n",
        "path = untar_data(URLs.MNIST_SAMPLE, data='/content')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.14% [3219456/3214948 00:00<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9sc_WGbkSXW"
      },
      "source": [
        "NOTE: This MNIST_SAMPLE dataset contains only the hand-written numbers 3s and 7s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tRPvD7Qi3yYV"
      },
      "source": [
        "# ??untar_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aKepgVY8k3xT",
        "outputId": "9bf55f40-5260-4fba-ce95-2cb6864424e8"
      },
      "source": [
        "# setting up the path\n",
        "Path.BASE_PATH = path; path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Path('.')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_Q2hXKVNk3xT",
        "outputId": "ca07e2b5-1b63-4431-ed4e-7cb03f0d7f95"
      },
      "source": [
        "# getting a specific image - a 3\n",
        "im3 = Image.open(path/'train'/'3'/'12.png')\n",
        "show_image(im3);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COiliivbk3xT"
      },
      "source": [
        "Now we're going to take the top 3×3-pixel square of our image, and multiply each of those values by each item in our kernel. Then we'll add them up, like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OgKviNOAk3xT",
        "outputId": "e67887d3-edc4-4077-c4a3-cb40325e5716"
      },
      "source": [
        "# putting the image into a pytorch tensor\n",
        "im3_t = tensor(im3)\n",
        "print(im3_t.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "15WHLLWM8H9g",
        "outputId": "f010fc20-0609-4f05-a467-9634b07e3a11"
      },
      "source": [
        "# selecting a fragment in that image\n",
        "im3_t[0:3, 0:3] #rows and columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]], dtype=torch.uint8)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-h_IYDbnlfn3",
        "outputId": "b010cff0-14ba-4c50-b965-9c58d545bd8b"
      },
      "source": [
        "# applying the kernel to that image's fragment - and doing an element wise multiplication\n",
        "im3_t[0:3,0:3] * top_edge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0., -0., -0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LUnQ7fvdk3xT",
        "outputId": "51394739-b732-47c2-e357-e86e361b0fd7"
      },
      "source": [
        "# multiplying and summing it all up\n",
        "(im3_t[0:3,0:3] * top_edge).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU4m3w9Kk3xU"
      },
      "source": [
        "NOTE:  Not very interesting so far—all the pixels in the top-left corner are **white**. But let's pick a couple of more interesting spots:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j4ZRUbPK9Gkx",
        "outputId": "8c006221-943c-43fe-d0f9-aa4fae60dc6f"
      },
      "source": [
        "# getting a bigger fragment of a tensor from a dataframe\n",
        "df = pd.DataFrame(im3_t[:10,:20])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>99</td>\n",
              "      <td>91</td>\n",
              "      <td>142</td>\n",
              "      <td>155</td>\n",
              "      <td>246</td>\n",
              "      <td>182</td>\n",
              "      <td>155</td>\n",
              "      <td>155</td>\n",
              "      <td>155</td>\n",
              "      <td>155</td>\n",
              "      <td>131</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>252</td>\n",
              "      <td>210</td>\n",
              "      <td>122</td>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>220</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>235</td>\n",
              "      <td>189</td>\n",
              "      <td>189</td>\n",
              "      <td>189</td>\n",
              "      <td>189</td>\n",
              "      <td>150</td>\n",
              "      <td>189</td>\n",
              "      <td>205</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>74</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>224</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>153</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>90</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>247</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0   1   2    3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
              "0   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "1   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "2   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "3   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "4   0   0   0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "5   0   0   0   12   99   91  142  155  246  182  155  155  155  155  131   \n",
              "6   0   0   0  138  254  254  254  254  254  254  254  254  254  254  254   \n",
              "7   0   0   0  220  254  254  254  235  189  189  189  189  150  189  205   \n",
              "8   0   0   0   35   74   35   35   25    0    0    0    0    0    0   13   \n",
              "9   0   0   0    0    0    0    0    0    0    0    0    0    0    0   90   \n",
              "\n",
              "    15   16   17   18  19  \n",
              "0    0    0    0    0   0  \n",
              "1    0    0    0    0   0  \n",
              "2    0    0    0    0   0  \n",
              "3    0    0    0    0   0  \n",
              "4    0    0    0    0   0  \n",
              "5   52    0    0    0   0  \n",
              "6  252  210  122   33   0  \n",
              "7  254  254  254   75   0  \n",
              "8  224  254  254  153   0  \n",
              "9  254  254  247   53   0  "
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYPHyB5l9Wxq"
      },
      "source": [
        "NOTE: the dataframe `df` fragment has 10 rows and 20 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yuh5Crku9kxc",
        "outputId": "5726c615-5a0d-4578-b642-caaf291af511"
      },
      "source": [
        "# Coloring the fragment with grey shades\n",
        "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col0,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col1,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col2,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col3,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col4,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col5,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col6,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col7,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col8,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col9,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col10,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col11,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col12,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col13,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col14,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col15,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col16,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col17,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col18,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col19,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col0,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col1,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col2,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col3,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col4,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col5,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col6,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col7,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col8,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col9,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col10,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col11,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col12,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col13,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col14,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col15,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col16,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col17,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col18,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col19,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col0,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col1,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col2,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col3,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col4,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col5,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col6,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col7,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col8,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col9,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col10,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col11,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col12,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col13,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col14,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col15,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col16,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col17,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col18,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col19,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col0,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col1,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col2,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col3,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col4,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col5,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col6,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col7,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col8,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col9,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col10,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col11,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col12,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col13,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col14,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col15,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col16,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col17,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col18,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col19,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col0,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col1,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col2,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col3,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col4,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col5,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col6,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col7,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col8,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col9,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col10,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col11,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col12,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col13,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col14,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col15,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col16,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col17,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col18,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col19,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col0,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col1,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col2,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col16,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col17,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col18,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col19,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col0,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col1,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col2,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col19,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col0,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col1,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col2,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col19,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col0,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col1,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col2,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col8,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col9,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col10,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col11,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col12,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col13,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col19,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col0,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col1,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col2,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col3,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col4,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col5,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col6,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col7,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col8,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col9,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col10,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col11,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col12,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col13,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col19{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #ffffff;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col3,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col14{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #f9f9f9;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col4{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #b9b9b9;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col5{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #c1c1c1;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col6{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #858585;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col7,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col10,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col11,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col12,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col13{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #777777;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col8{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #090909;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col9{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #5b5b5b;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col14{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #919191;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col15{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #e1e1e1;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col3{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #727272;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col4,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col5,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col6,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col7,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col8,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col9,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col10,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col11,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col12,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col13,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col14,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col3,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col4,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col5,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col6,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col15,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col16,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col17,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col16,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col17,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col18,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col15,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col16{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #000000;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col15{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #020202;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col16{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #363636;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col17{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #9d9d9d;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col18{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #dfdfdf;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col7{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #161616;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col8,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col9,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col10,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col11,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col13{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #535353;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col12{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #7c7c7c;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col14{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #3d3d3d;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col18{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #999999;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col3{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #eaeaea;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col4{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #d0d0d0;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col5,#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col6{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #eeeeee;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col7{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #f3f3f3;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col15{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #232323;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col14{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #c2c2c2;\n",
              "            color:  #000000;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col17{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #080808;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col18{\n",
              "            font-size:  6pt;\n",
              "            background-color:  #c4c4c4;\n",
              "            color:  #000000;\n",
              "        }</style><table id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>        <th class=\"col_heading level0 col9\" >9</th>        <th class=\"col_heading level0 col10\" >10</th>        <th class=\"col_heading level0 col11\" >11</th>        <th class=\"col_heading level0 col12\" >12</th>        <th class=\"col_heading level0 col13\" >13</th>        <th class=\"col_heading level0 col14\" >14</th>        <th class=\"col_heading level0 col15\" >15</th>        <th class=\"col_heading level0 col16\" >16</th>        <th class=\"col_heading level0 col17\" >17</th>        <th class=\"col_heading level0 col18\" >18</th>        <th class=\"col_heading level0 col19\" >19</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col2\" class=\"data row0 col2\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col3\" class=\"data row0 col3\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col4\" class=\"data row0 col4\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col5\" class=\"data row0 col5\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col6\" class=\"data row0 col6\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col7\" class=\"data row0 col7\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col8\" class=\"data row0 col8\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col9\" class=\"data row0 col9\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col10\" class=\"data row0 col10\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col11\" class=\"data row0 col11\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col12\" class=\"data row0 col12\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col13\" class=\"data row0 col13\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col14\" class=\"data row0 col14\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col15\" class=\"data row0 col15\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col16\" class=\"data row0 col16\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col17\" class=\"data row0 col17\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col18\" class=\"data row0 col18\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row0_col19\" class=\"data row0 col19\" >0</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col1\" class=\"data row1 col1\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col3\" class=\"data row1 col3\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col4\" class=\"data row1 col4\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col5\" class=\"data row1 col5\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col6\" class=\"data row1 col6\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col7\" class=\"data row1 col7\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col8\" class=\"data row1 col8\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col9\" class=\"data row1 col9\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col10\" class=\"data row1 col10\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col11\" class=\"data row1 col11\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col12\" class=\"data row1 col12\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col13\" class=\"data row1 col13\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col14\" class=\"data row1 col14\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col15\" class=\"data row1 col15\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col16\" class=\"data row1 col16\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col17\" class=\"data row1 col17\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col18\" class=\"data row1 col18\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row1_col19\" class=\"data row1 col19\" >0</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col3\" class=\"data row2 col3\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col4\" class=\"data row2 col4\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col5\" class=\"data row2 col5\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col6\" class=\"data row2 col6\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col7\" class=\"data row2 col7\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col8\" class=\"data row2 col8\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col9\" class=\"data row2 col9\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col10\" class=\"data row2 col10\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col11\" class=\"data row2 col11\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col12\" class=\"data row2 col12\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col13\" class=\"data row2 col13\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col14\" class=\"data row2 col14\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col15\" class=\"data row2 col15\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col16\" class=\"data row2 col16\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col17\" class=\"data row2 col17\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col18\" class=\"data row2 col18\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row2_col19\" class=\"data row2 col19\" >0</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col0\" class=\"data row3 col0\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col1\" class=\"data row3 col1\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col2\" class=\"data row3 col2\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col3\" class=\"data row3 col3\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col4\" class=\"data row3 col4\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col5\" class=\"data row3 col5\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col6\" class=\"data row3 col6\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col7\" class=\"data row3 col7\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col8\" class=\"data row3 col8\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col9\" class=\"data row3 col9\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col10\" class=\"data row3 col10\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col11\" class=\"data row3 col11\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col12\" class=\"data row3 col12\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col13\" class=\"data row3 col13\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col14\" class=\"data row3 col14\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col15\" class=\"data row3 col15\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col16\" class=\"data row3 col16\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col17\" class=\"data row3 col17\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col18\" class=\"data row3 col18\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row3_col19\" class=\"data row3 col19\" >0</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col0\" class=\"data row4 col0\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col1\" class=\"data row4 col1\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col2\" class=\"data row4 col2\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col3\" class=\"data row4 col3\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col4\" class=\"data row4 col4\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col5\" class=\"data row4 col5\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col6\" class=\"data row4 col6\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col7\" class=\"data row4 col7\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col8\" class=\"data row4 col8\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col9\" class=\"data row4 col9\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col10\" class=\"data row4 col10\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col11\" class=\"data row4 col11\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col12\" class=\"data row4 col12\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col13\" class=\"data row4 col13\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col14\" class=\"data row4 col14\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col15\" class=\"data row4 col15\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col16\" class=\"data row4 col16\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col17\" class=\"data row4 col17\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col18\" class=\"data row4 col18\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row4_col19\" class=\"data row4 col19\" >0</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col0\" class=\"data row5 col0\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col1\" class=\"data row5 col1\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col2\" class=\"data row5 col2\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col3\" class=\"data row5 col3\" >12</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col4\" class=\"data row5 col4\" >99</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col5\" class=\"data row5 col5\" >91</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col6\" class=\"data row5 col6\" >142</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col7\" class=\"data row5 col7\" >155</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col8\" class=\"data row5 col8\" >246</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col9\" class=\"data row5 col9\" >182</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col10\" class=\"data row5 col10\" >155</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col11\" class=\"data row5 col11\" >155</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col12\" class=\"data row5 col12\" >155</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col13\" class=\"data row5 col13\" >155</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col14\" class=\"data row5 col14\" >131</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col15\" class=\"data row5 col15\" >52</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col16\" class=\"data row5 col16\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col17\" class=\"data row5 col17\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col18\" class=\"data row5 col18\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row5_col19\" class=\"data row5 col19\" >0</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col0\" class=\"data row6 col0\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col1\" class=\"data row6 col1\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col2\" class=\"data row6 col2\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col3\" class=\"data row6 col3\" >138</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col4\" class=\"data row6 col4\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col5\" class=\"data row6 col5\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col6\" class=\"data row6 col6\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col7\" class=\"data row6 col7\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col8\" class=\"data row6 col8\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col9\" class=\"data row6 col9\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col10\" class=\"data row6 col10\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col11\" class=\"data row6 col11\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col12\" class=\"data row6 col12\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col13\" class=\"data row6 col13\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col14\" class=\"data row6 col14\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col15\" class=\"data row6 col15\" >252</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col16\" class=\"data row6 col16\" >210</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col17\" class=\"data row6 col17\" >122</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col18\" class=\"data row6 col18\" >33</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row6_col19\" class=\"data row6 col19\" >0</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col0\" class=\"data row7 col0\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col1\" class=\"data row7 col1\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col2\" class=\"data row7 col2\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col3\" class=\"data row7 col3\" >220</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col4\" class=\"data row7 col4\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col5\" class=\"data row7 col5\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col6\" class=\"data row7 col6\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col7\" class=\"data row7 col7\" >235</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col8\" class=\"data row7 col8\" >189</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col9\" class=\"data row7 col9\" >189</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col10\" class=\"data row7 col10\" >189</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col11\" class=\"data row7 col11\" >189</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col12\" class=\"data row7 col12\" >150</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col13\" class=\"data row7 col13\" >189</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col14\" class=\"data row7 col14\" >205</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col15\" class=\"data row7 col15\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col16\" class=\"data row7 col16\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col17\" class=\"data row7 col17\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col18\" class=\"data row7 col18\" >75</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row7_col19\" class=\"data row7 col19\" >0</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col0\" class=\"data row8 col0\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col1\" class=\"data row8 col1\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col2\" class=\"data row8 col2\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col3\" class=\"data row8 col3\" >35</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col4\" class=\"data row8 col4\" >74</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col5\" class=\"data row8 col5\" >35</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col6\" class=\"data row8 col6\" >35</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col7\" class=\"data row8 col7\" >25</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col8\" class=\"data row8 col8\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col9\" class=\"data row8 col9\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col10\" class=\"data row8 col10\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col11\" class=\"data row8 col11\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col12\" class=\"data row8 col12\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col13\" class=\"data row8 col13\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col14\" class=\"data row8 col14\" >13</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col15\" class=\"data row8 col15\" >224</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col16\" class=\"data row8 col16\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col17\" class=\"data row8 col17\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col18\" class=\"data row8 col18\" >153</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row8_col19\" class=\"data row8 col19\" >0</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col0\" class=\"data row9 col0\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col1\" class=\"data row9 col1\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col2\" class=\"data row9 col2\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col3\" class=\"data row9 col3\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col4\" class=\"data row9 col4\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col5\" class=\"data row9 col5\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col6\" class=\"data row9 col6\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col7\" class=\"data row9 col7\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col8\" class=\"data row9 col8\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col9\" class=\"data row9 col9\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col10\" class=\"data row9 col10\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col11\" class=\"data row9 col11\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col12\" class=\"data row9 col12\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col13\" class=\"data row9 col13\" >0</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col14\" class=\"data row9 col14\" >90</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col15\" class=\"data row9 col15\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col16\" class=\"data row9 col16\" >254</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col17\" class=\"data row9 col17\" >247</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col18\" class=\"data row9 col18\" >53</td>\n",
              "                        <td id=\"T_fc6b687c_1e38_11ec_a5f7_0242ac1c0002row9_col19\" class=\"data row9 col19\" >0</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f2cf8dc9b90>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_19TPIek3xU"
      },
      "source": [
        "<img alt=\"Top section of a digit\" width=\"490\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00059.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDgT_Fi9k3xU"
      },
      "source": [
        "There's a top edge at cell 5,8. Let's repeat our calculation there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1zWXPMhN9-5K",
        "outputId": "fc68238b-4c81-4a8f-c186-e3fd3ddf296d"
      },
      "source": [
        "# identifying a specific fragment from the image\n",
        "im3_t[4:7, 6:9]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0],\n",
              "        [142, 155, 246],\n",
              "        [254, 254, 254]], dtype=torch.uint8)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XHYLIZxskdO1",
        "outputId": "78d7a6f7-e8bd-418c-e5cf-56649e2c58da"
      },
      "source": [
        "# applying the kernel to a specific location \n",
        "(im3_t[4:7,6:9] * top_edge)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ -0.,  -0.,  -0.],\n",
              "        [  0.,   0.,   0.],\n",
              "        [254., 254., 254.]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "liLo5IcXmrdz",
        "outputId": "23e8b5c3-5a3f-4ae0-9b9d-52bbe0bdde7c"
      },
      "source": [
        "# reminder - the top edge kernel\n",
        "top_edge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1., -1., -1.],\n",
              "        [ 0.,  0.,  0.],\n",
              "        [ 1.,  1.,  1.]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lvw1UR0vk3xU",
        "outputId": "e059dd9c-08a0-4719-eba3-3047869f30e9"
      },
      "source": [
        "# appling the kernel to another specific fragment of the image\n",
        "(im3_t[4:7,6:9] * top_edge).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(762.)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yZ1Gp4spHfr"
      },
      "source": [
        "NOTE: This computation returns a high number for the top edge because the 3x3 pixels square has low values at the top of the square `0` and high values at the bottom of the square `254`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGMJfgQck3xU"
      },
      "source": [
        "There's a right edge at cell 8,18. What does that give us?:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6srGzcNJk3xV",
        "outputId": "effe7221-16e0-443b-9bce-474305ac8e2b"
      },
      "source": [
        "# checking the right edge\n",
        "# identifying another specific location from the image\n",
        "(im3_t[7:10,17:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[254,  75,   0],\n",
              "        [254, 153,   0],\n",
              "        [247,  53,   0]], dtype=torch.uint8)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ThQ7IK_bnlrI",
        "outputId": "737cb960-d828-4b64-8b71-db41fefec7c5"
      },
      "source": [
        "# checking the right edge\n",
        "# identifying another specific location from the image and applying the kernel\n",
        "(im3_t[7:10,17:20] * top_edge)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-254.,  -75.,   -0.],\n",
              "        [   0.,    0.,    0.],\n",
              "        [ 247.,   53.,    0.]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6O6nulrinnLt",
        "outputId": "73e7b73d-744b-478f-c2c7-3789abccb3e7"
      },
      "source": [
        "# checking the right edge\n",
        "# identifying another specific fragment from the image and applying the kernel and summing it up\n",
        "(im3_t[7:10,17:20] * top_edge).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-29.)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCaaHYWzqa9a"
      },
      "source": [
        "NOTE: The computation returns a low value `-29`. The top row from the 3x3 pixels returns negative values `(-254, -75, 0)` and the bottom row, even though it returns positive values (`247, 53, 0)` , the total computation remains negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F9JQ61Xk3xV"
      },
      "source": [
        "As you can see, this little calculation is returning a high number where the 3×3-pixel square represents a top edge (i.e., where there are low values at the top of the square, and high values immediately underneath). That's because the `-1` values in our kernel have little impact in that case, but the `1` values have a lot.\n",
        "\n",
        "Let's look a tiny bit at the math. The filter will take any window of size 3×3 in our images, and if we name the pixel values like this:\n",
        "\n",
        "$$\\begin{matrix} a1 & a2 & a3 \\\\ a4 & a5 & a6 \\\\ a7 & a8 & a9 \\end{matrix}$$\n",
        "\n",
        "it will return $-a1-a2-a3+a7+a8+a9$`. \n",
        "\n",
        "If we are in a part of the image where $a1$, $a2$, and $a3$ add up to the same as $a7$, $a8$, and $a9$, then the terms will cancel each other out and we will get 0. \n",
        "\n",
        "However, if $a7$ is greater than $a1$, $a8$ is greater than $a2$, and $a9$ is greater than $a3$, we will get a bigger number as a result. So **this filter detects horizontal edges—more precisely**, (**from bright to dark**) edges where we go from bright parts of the image at the top to darker parts at the bottom.\n",
        "\n",
        "Changing our filter to have the row of `1`s at the top and the `-1`s at the bottom would detect **horizontal edges that go from dark to light**.\n",
        "\n",
        "Putting the `1`s and `-1`s in columns versus rows would give us filters that detect **vertical edges**. Each set of weights will produce a different kind of outcome.\n",
        "\n",
        "Let's create a function to do this for one location, and check it matches our result from before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gRAiYSDYk3xV"
      },
      "source": [
        "# function identifying an image location and a kernel and doing a matrix multiplication\n",
        "def apply_kernel(row, col, kernel):\n",
        "    return (im3_t[row-1:row+2,col-1:col+2] * kernel).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AsNaWqSfk3xW",
        "outputId": "78817d56-58c8-4fa6-8e38-dc549efb0e55"
      },
      "source": [
        "apply_kernel(5,7,top_edge) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(762.)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHyOA5GqA2Xz"
      },
      "source": [
        "NOTE: the kernel has been applied at the following location: `im3_t[4:7,6:9]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv6v__1o3B0M"
      },
      "source": [
        "NOTE: below is a decomposition of what the function `apply_kernel` does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HaoBYhxFuOdT",
        "outputId": "0dd022e3-f658-4948-a46d-1c607dd5f9a0"
      },
      "source": [
        "# amr - identifying the image fragment \n",
        "im3_t[4:7,6:9]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0],\n",
              "        [142, 155, 246],\n",
              "        [254, 254, 254]], dtype=torch.uint8)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6phSJrFIvHkZ",
        "outputId": "36722b08-e04b-4866-a9ad-df71dc082408"
      },
      "source": [
        "# amr - applying a kernel to the image fragment - multiplication element wise\n",
        "(im3_t[4:7,6:9] * top_edge)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ -0.,  -0.,  -0.],\n",
              "        [  0.,   0.,   0.],\n",
              "        [254., 254., 254.]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BzUG9OwcvZsG",
        "outputId": "dd64dae6-a492-4d3c-e6be-9f8e758353e8"
      },
      "source": [
        "# amr - applying a kernel - multiplication and addition, element wise\n",
        "(im3_t[4:7,6:9] * top_edge).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(762.)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgOmKD42k3xW"
      },
      "source": [
        "NOTE: But note that we can't apply it to the corner (e.g., **location 0,0**), since there isn't a complete 3×3 square there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRA1Tmlak3xW"
      },
      "source": [
        "### Mapping a Convolution Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jMRF2CZk3xW"
      },
      "source": [
        "We can map `apply_kernel()` across the coordinate grid. That is, we'll be taking our **3×3 kernel**, and applying it to each **3×3 section of our image**. For instance, the figure below (Applying a kernel across a grid) shows the positions a 3×3 kernel that can be applied to in the first row of a **5×5 image**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzvCW_y-k3xW"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter9_nopadconv.svg?raw=1\" id=\"nopad_conv\" caption=\"Applying a kernel across a grid\" alt=\"Applying a kernel across a grid\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1xZaYmXk3xW"
      },
      "source": [
        "To get a grid of coordinates we can use a *nested list comprehension*, like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n5TzzRgmk3xW",
        "outputId": "a34c0f99-b30b-44f0-8278-6fddd9566033"
      },
      "source": [
        "# getting a grid of coordinates with a nested list comprehension\n",
        "# i = row, j = column\n",
        "[[(i,j) for j in range(1,5)] for i in range(1,5)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[(1, 1), (1, 2), (1, 3), (1, 4)],\n",
              " [(2, 1), (2, 2), (2, 3), (2, 4)],\n",
              " [(3, 1), (3, 2), (3, 3), (3, 4)],\n",
              " [(4, 1), (4, 2), (4, 3), (4, 4)]]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsm57S87k3xW"
      },
      "source": [
        "> note: Nested List Comprehensions: Nested list comprehensions are used a lot in Python, so if you haven't seen them before, take a few minutes to make sure you understand what's happening here, and experiment with writing your own nested list comprehensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBGnjsZhk3xX"
      },
      "source": [
        "Here's the result of applying our kernel over a coordinate grid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mREVCRMwk3xX",
        "outputId": "51d286b4-546b-435b-950c-86c452a3c472"
      },
      "source": [
        "# picture has 28x28 pixels\n",
        "rng = range(1,27)\n",
        "# checking the top edge for the number 3\n",
        "top_edge3 = tensor([[apply_kernel(i,j,top_edge) for j in rng] for i in rng])\n",
        "\n",
        "show_image(top_edge3);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q062yy_VzhX9",
        "outputId": "9eebc750-4269-4803-d0af-3652c6c7214d"
      },
      "source": [
        "# kernel\n",
        "top_edge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1., -1., -1.],\n",
              "        [ 0.,  0.,  0.],\n",
              "        [ 1.,  1.,  1.]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyLLPmgZt2nL"
      },
      "source": [
        "NOTE: Think about it like this… Under what combinations of pixels will the value of the top sobel filter (finding edges) be at it’s maximum?\n",
        "- `+` This row should be as high as possible\n",
        "- `0` This row is multiplied by 0 so it doesn’t matter\n",
        "- `-` This row should be as low as possible\n",
        "\n",
        "If the top and the bottom row are the same they cancel each other out. The further they are apart the higher the output is going to be. Because we’re usually dealing with chars negative numbers will be treated as 0 so it’s a top filter because it has a high value for the top of edges and a low value otherwise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-_zCRIuk3xX"
      },
      "source": [
        "Looking good! **Our top edges are black, and bottom edges are white** (since they are the *opposite* of top edges). Now that our image contains negative numbers too, `matplotlib` in `show_img` has automatically changed our colors so that **white is the smallest number in the image, black the highest, and zeros appear as gray**.\n",
        "\n",
        "We can try the same thing for left edges:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CI63J7UDk3xX",
        "outputId": "87c9cda9-faa3-4e81-f2bc-941006b68d56"
      },
      "source": [
        "# checking the left edge\n",
        "left_edge = tensor([[-1,1,0],\n",
        "                    [-1,1,0],\n",
        "                    [-1,1,0]]).float()\n",
        "\n",
        "left_edge3 = tensor([[apply_kernel(i,j,left_edge) for j in rng] for i in rng])\n",
        "\n",
        "show_image(left_edge3);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhgbAmUAk3xX"
      },
      "source": [
        "As we mentioned before, a convolution is the operation of applying such a kernel over a grid in this way. In the paper [\"A Guide to Convolution Arithmetic for Deep Learning\"](https://arxiv.org/abs/1603.07285) there are many great diagrams showing how image kernels can be applied. Here's an example from the paper showing (below) a light blue **4×4 image**, with a dark blue **3×3 kernel** being applied, creating a **2×2** green **output activation map at the top**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLk5StM0k3xX"
      },
      "source": [
        "<img alt=\"Result of applying a 3×3 kernel to a 4×4 image\" width=\"782\" caption=\"Result of applying a 3×3 kernel to a 4×4 image (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"three_ex_four_conv\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00028.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc8GVyHqk3xY"
      },
      "source": [
        "Look at the shape of the result. If the original image has a height of `h` and a width of `w`, how many 3×3 windows can we find? As you can see from the example, there are `h-2` by `w-2` windows, so the image we get has a result as a height of `h-2` and a width of `w-2`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YApIzEPEk3xY"
      },
      "source": [
        "We won't implement this convolution function from scratch, but use PyTorch's implementation instead (it is way faster than anything we could do in Python)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvj9ny3mk3xY"
      },
      "source": [
        "### Convolutions in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJtXZ8N6k3xY"
      },
      "source": [
        "Convolution is such an important and widely used operation that PyTorch has it built in. It's called `F.conv2d` (recall that `F` is a fastai import from `torch.nn.functional`, as recommended by PyTorch). The PyTorch docs tell us that it includes these parameters:\n",
        "\n",
        "- input:: input tensor of shape `(minibatch, in_channels, iH, iW)`\n",
        "- weight:: filters of shape `(out_channels, in_channels, kH, kW)`\n",
        "\n",
        "Here `iH,iW` is the **height and width of the image** (i.e., `28,28`), and `kH,kW` is the **height and width of our kernel** (`3,3`). But apparently PyTorch is expecting rank-4 tensors for both these arguments, whereas currently we only have rank-2 tensors (i.e., matrices, or arrays with two axes).\n",
        "\n",
        "The reason for these extra axes is that PyTorch has a few tricks up its sleeve. The first trick is that **PyTorch can apply a convolution to multiple images at the same time**. That means we can call it on every item in a batch at once!\n",
        "\n",
        "The second trick is that **PyTorch can apply multiple kernels at the same time**. \n",
        "\n",
        "So let's create the diagonal-edge kernels too, and then stack all four of our edge kernels into a single tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6-5dOZ8Vk3xY",
        "outputId": "6ea26e9a-e414-4494-8b7a-65de36c6fede"
      },
      "source": [
        "diag1_edge = tensor([[ 0,-1, 1],\n",
        "                     [-1, 1, 0],\n",
        "                     [ 1, 0, 0]]).float()\n",
        "                     \n",
        "diag2_edge = tensor([[ 1,-1, 0],\n",
        "                     [ 0, 1,-1],\n",
        "                     [ 0, 0, 1]]).float()\n",
        "\n",
        "# left_edge = tensor([[-1,1,0],\n",
        "#                     [-1,1,0],\n",
        "#                     [-1,1,0]]).float()\n",
        "\n",
        "# top_edge = tensor([[-1,-1,-1],\n",
        "#                    [ 0, 0, 0],\n",
        "#                    [ 1, 1, 1]]).float()\n",
        "\n",
        "edge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])\n",
        "edge_kernels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 3])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hIshCh32-xB"
      },
      "source": [
        "NOTE: `edge_kernels.shape` has 4 kernels, with a kernel size of 3x3. The number of kernels defined the number of output channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG1i8btik3xZ"
      },
      "source": [
        "To test this, we'll need a `DataLoader` and a sample mini-batch. Let's use the data block API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MRPYxY5J-8Tc",
        "outputId": "3e5acef1-3b74-4265-8e81-072fc48644ba"
      },
      "source": [
        "path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Path('.')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aZ44FX0yk3xZ",
        "outputId": "15ee382c-ecec-44fb-d7c5-d1e9c2b5c589"
      },
      "source": [
        "# the fastai datablock api & dataloader\n",
        "mnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), \n",
        "                  get_items=get_image_files, \n",
        "                  splitter=GrandparentSplitter(),\n",
        "                  get_y=parent_label)\n",
        "\n",
        "dls = mnist.dataloaders(path)\n",
        "\n",
        "# accessing a sample mini-batch\n",
        "xb,yb = first(dls.valid)\n",
        "xb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 28, 28])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nJ2GuYg9nYl"
      },
      "source": [
        "[Fastai datablock api](https://docs.fast.ai/data.block.html) [Fastai datablock tranforms](https://docs.fast.ai/data.transforms.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9zP6roWM8fF"
      },
      "source": [
        "NOTE: the sample mini-batch has 64 records, one channel (black and white picture), and a picture size 28x28 pixels representing 28 rows and 28 columns in a tensor. A channel is a single basic color in an image—for regular full-color images there are three channels, red, green, and blue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "35XJxEE44mz9",
        "outputId": "2b357a16-b783-4b51-d44a-8397184d8563"
      },
      "source": [
        "# checking the number of classes and their vocabulary\n",
        "dls.c, dls.vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, ['3', '7'])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdlHnOp9k3xZ"
      },
      "source": [
        "By default, fastai puts data on the GPU when using data blocks. Let's move it to the CPU for our examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Oydb98fJk3xZ"
      },
      "source": [
        "# putting the sample batch into cpu\n",
        "xb,yb = to_cpu(xb),to_cpu(yb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8utkoTmk3xZ"
      },
      "source": [
        "One batch contains 64 images, each of 1 channel, with 28×28 pixels. `F.conv2d` can handle multichannel (i.e., color) images too. A *channel* is a single basic color in an image—for regular full-color images there are three channels, red, green, and blue. **PyTorch represents an image as a rank-3 tensor, with dimensions `[channels, rows, columns]`**.\n",
        "\n",
        "We'll see how to handle more than one channel later in this chapter. Kernels passed to `F.conv2d` need to be rank-4 tensors: `[channels_in, features_out, rows, columns]`. `edge_kernels` is currently missing one of these. We need to tell PyTorch that the number of input channels in the kernel is one, which we can do by inserting an axis of size one (this is known as a *unit axis*) in the first location, where the PyTorch docs show `in_channels` is expected. To insert a unit axis into a tensor, we use the `unsqueeze` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cViuBom-k3xZ",
        "outputId": "005c91bc-f696-48b4-9242-3c64a97e2d3f"
      },
      "source": [
        "# inserting a unit axis into a tensor\n",
        "edge_kernels.shape, edge_kernels.unsqueeze(1).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3]))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pil4YTPeOk5y"
      },
      "source": [
        "*NOTE*: The new dimension `unsqueeze(1)` has been added at position 1. This result in this tensor shape from  `(torch.Size([4, 3, 3])` 4 kernels, kernel size: 3x3, to  `([4, 1, 3, 3]))` 4 kernels, 1 unit axis, kernel size: 3x3.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz8ayXV5k3xa"
      },
      "source": [
        "This is now the correct shape for `edge_kernels`. Let's pass this all to `conv2d`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "56k3UU5Wk3xa"
      },
      "source": [
        "# adding the unit axis to edge_kernels\n",
        "edge_kernels = edge_kernels.unsqueeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k6q-v4kbNynP",
        "outputId": "c7d53944-90ca-42b9-9c53-2c4602f81405"
      },
      "source": [
        "# amr - batch input shape\n",
        "xb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 28, 28])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KltaotOTk3xa",
        "outputId": "26499e19-7708-466a-dd60-721a55fef1c9"
      },
      "source": [
        "# using Pytorch convolution layer\n",
        "batch_features = F.conv2d(xb, edge_kernels)\n",
        "batch_features.shape # output shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 4, 26, 26])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J2l_BADk3xa"
      },
      "source": [
        "The output shape shows we gave 64 images in the mini-batch, 4 kernels, and 26×26 edge maps (we started with 28×28 images, but lost one pixel from each side as discussed earlier). We can see we get the same results as when we did this manually:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG0hND77eckn"
      },
      "source": [
        "NOTE:  The input size is now 26x26 is because our kernels are size 3x3 so when we compute each location, **we lose 2 pixels in each direction because it won’t be able to stride past the 28x28 size**. See explanation below.\n",
        "(Showing it on 9 wide image and a kernel size = 3)\n",
        "```\n",
        "[1,2,3,4,5,6,7,8,9] \n",
        "[1,2,3] - 1\n",
        "[2,3,4] - 2\n",
        "[3,4,5] - 3\n",
        "[4,5,6] - 4\n",
        "[5,6,7] - 5\n",
        "[6,7,8] - 6\n",
        "[7,8,9] - 7\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "49rS3ZZQk3xa",
        "outputId": "f8c9eea5-c8cd-4a35-ef99-131a13afdd10"
      },
      "source": [
        "show_image(batch_features[0,0]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATxeJyd2k3xa"
      },
      "source": [
        "The most important trick that PyTorch has up its sleeve is that it can use the **GPU to do all this work in parallel—that is, applying multiple kernels, to multiple images, across multiple channels**. Doing lots of work in parallel is critical to getting GPUs to work efficiently; if we did each of these operations one at a time, we'd often run hundreds of times slower (and if we used our manual convolution loop from the previous section, we'd be millions of times slower!). Therefore, to become a strong deep learning practitioner, one skill to practice is giving your GPU plenty of work to do at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRmlL34Gk3xa"
      },
      "source": [
        "It would be nice to not lose those two pixels on each axis. The way we do that is to add ***padding***, which is simply additional pixels added around the outside of our image. Most commonly, pixels of zeros are added. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d9cu6mqk3xa"
      },
      "source": [
        "### Strides and Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4YvLmOgk3xb"
      },
      "source": [
        "With appropriate padding, we can ensure that the output activation map is the same size as the original image, which can make things a lot simpler when we construct our architectures. The picture below shows **a convolution with padding**, that is how adding padding allows us to apply the kernels in the image corners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-yusxfAk3xb"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter9_padconv.svg?raw=1\" id=\"pad_conv\" caption=\"A convolution with padding\" alt=\"A convolution with padding\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM_8hq4sk3xb"
      },
      "source": [
        "With a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6 activation map, as we can see in the figure below: \n",
        "\n",
        "a 4x4 kernel with a 5x5 image with 2 pixels padding resulting in a 6x6 output feature map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb57Hvh8k3xb"
      },
      "source": [
        "<img alt=\"A 4×4 kernel with 5×5 input and 2 pixels of padding\" width=\"783\" caption=\"A 4×4 kernel with 5×5 input and 2 pixels of padding (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"four_by_five_conv\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00029.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lckKxBlk3xb"
      },
      "source": [
        "If we add a kernel of size `ks` by `ks` (with `ks` an odd number), the necessary padding on each side to keep the same shape is `ks//2`. An even number for `ks` would require a different amount of padding on the top/bottom and left/right, but in practice we almost never use an even filter size.\n",
        "\n",
        "So far, when we have applied the kernel to the grid, we have moved it one pixel over at a time. But we can jump further; for instance, we could move over two pixels after each kernel application, as is shown in the figure below (a 3x3 kernel, a 5x5 input, and a stride-2 convolution with 1 pixel padding). This is known as a **stride-2** convolution. The most common kernel size in practice is 3×3, and the most common padding is 1. As you'll see, **stride-2 convolutions are useful for decreasing the size of our outputs**, and **stride-1 convolutions are useful for adding layers without changing the output size**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH6o6pLGk3xb"
      },
      "source": [
        "<img alt=\"A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel of padding\" width=\"774\" caption=\"A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel of padding (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"three_by_five_conv\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00030.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l2-7zdgk3xb"
      },
      "source": [
        "> In an image of size `h` by `w`, using a padding of 1 and a stride of 2 will give us a result of size `(h+1)//2` by `(w+1)//2`. The general formula for each dimension is `(n + 2*pad - ks)//stride + 1`, where `pad` is the padding, `ks`, the size of our kernel, and `stride` is the stride."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWIXtjuqk3xb"
      },
      "source": [
        "Let's now take a look at how the pixel values of the result of our convolutions are computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmdR96J4k3xb"
      },
      "source": [
        "### Understanding the Convolution Equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpuULJdkk3xb"
      },
      "source": [
        "To explain the math behind convolutions, fast.ai student Matt Kleinsmith came up with the very clever idea of showing [CNNs from different viewpoints](https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c). In fact, it's so clever, and so helpful, we're going to show it here too!\n",
        "\n",
        "Here's our 3×3 pixel image, with each pixel labeled with a letter:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5NFZClxk3xb"
      },
      "source": [
        "<img alt=\"The image\" width=\"75\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00032.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrOijOejk3xc"
      },
      "source": [
        "And here's our kernel, with each weight labeled with a Greek letter: (alpha, beta, gamma, delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaGABfYJk3xc"
      },
      "source": [
        "<img alt=\"The kernel\" width=\"55\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00033.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrWUSEtzk3xc"
      },
      "source": [
        "Since the filter fits in the image four times, we have four results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRPDad8Ck3xc"
      },
      "source": [
        "<img alt=\"The activations\" width=\"52\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00034.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWF7olPKk3xc"
      },
      "source": [
        "The figure below shows how we applied the kernel to each section of the image to yield each result. The output is called a feature map. The feature map reflects where in the image there is activation by a given filter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlesYpHSk3xc"
      },
      "source": [
        "<img alt=\"Applying the kernel\" width=\"366\" caption=\"Applying the kernel\" id=\"apply_kernel\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00035.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OUBJSfRk3xc"
      },
      "source": [
        "The equation view is show in the figure below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eqdMXq_k3xd"
      },
      "source": [
        "<img alt=\"The equation\" width=\"436\" caption=\"The equation\" id=\"eq_view\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00036.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8Nmhwnpk3xd"
      },
      "source": [
        "Notice that the bias term, ***b***, is the same for each section of the image. **You can consider the bias as part of the filter, just like the weights (α, β, γ, δ) are part of the filter.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSY4Du0Tk3xd"
      },
      "source": [
        "Here's an interesting insight — a convolution can be represented as a special kind of matrix multiplication, as illustrated in the figure below: **Convolution as Matrix Multiplications**. The weight matrix is just like the ones from traditional neural networks. However, this weight matrix has two special properties:\n",
        "\n",
        "1. The zeros shown in gray are untrainable. This means that they’ll stay zero throughout the optimization process.\n",
        "1. Some of the weights are equal, and while they are trainable (i.e., changeable), they must remain equal. These are called ***shared weights***.\n",
        "\n",
        "The zeros correspond to the pixels that the filter can't touch. Each row of the weight matrix corresponds to one application of the filter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3fwmX5Wk3xd"
      },
      "source": [
        "<img alt=\"Convolution as matrix multiplication\" width=\"683\" caption=\"Convolution as matrix multiplication\" id=\"conv_matmul\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00038.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jetHZwgtk3xd"
      },
      "source": [
        "Now that we understand what a convolution is, let's use them to build a neural net."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBoMefIyk3xd"
      },
      "source": [
        "## Our First Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf6EKx7uk3xd"
      },
      "source": [
        "There is no reason to believe that some particular edge filters are the most useful kernels for image recognition. Furthermore, we've seen that in later layers convolutional kernels become complex transformations of features from lower levels, but we don't have a good idea of how to manually construct these.\n",
        "\n",
        "Instead, it would be best to learn the values of the kernels. We already know how to do this—SGD! In effect, the model will learn the features that are useful for classification.\n",
        "\n",
        "When we use convolutions instead of (or in addition to) regular linear layers we create a *convolutional neural network* (CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkI0Oce8k3xe"
      },
      "source": [
        "### Creating the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp5cklsxk3xe"
      },
      "source": [
        "Let's go back to the  basic neural network we had in lesson 4. It was defined like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5AFeJYdVk3xe"
      },
      "source": [
        "# a basic neural network\n",
        "simple_net = nn.Sequential(\n",
        "    nn.Linear(28*28,30), \n",
        "    nn.ReLU(),\n",
        "    nn.Linear(30,1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-qjgZ3Dk3xe"
      },
      "source": [
        "We can view a model's definition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O29445ufk3xe",
        "outputId": "a75e88e3-9cf1-40f9-d88f-da43cbc377c1"
      },
      "source": [
        "simple_net"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=30, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=30, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUWQATirk3xe"
      },
      "source": [
        "We now want to create a similar architecture to this linear model, but using convolutional layers instead of linear. `nn.Conv2d` is the module equivalent of `F.conv2d`. It's more convenient than `F.conv2d` when creating an architecture, because it creates the weight matrix for us automatically when we instantiate it.\n",
        "\n",
        "Here's a possible architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R6NkPRJJk3xe"
      },
      "source": [
        "# a simple architecture using convolutional layers\n",
        "broken_cnn = sequential(\n",
        "    nn.Conv2d(1,30, kernel_size=3, padding=1), # 1 c (channel), 30 k (kernels), ks=3, pad=1\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(30,1, kernel_size=3, padding=1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S81KrjCtk3xe"
      },
      "source": [
        "> One thing to note here is that we didn't need to specify 28×28 as the input size. That's because a linear layer needs a weight in the weight matrix for every pixel, so it needs to know how many pixels there are, **but a convolution is applied over each pixel automatically**. The weights only depend on the number of input and output channels and the kernel size, as we saw in the previous section.\n",
        "\n",
        "Think about what the output shape is going to be, then let's try it and see:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B1qjTnkMk3xe",
        "outputId": "f76df3fc-fa8b-4b74-a32f-c21caad1abef"
      },
      "source": [
        "# output shape\n",
        "broken_cnn(xb).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 28, 28])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecYPleTYk3xf"
      },
      "source": [
        "> This is not something we can use to do classification, since we need a single output activation per image, not a 28×28 map of activations. One way to deal with this is to use enough **stride-2 convolutions such that the final layer is size 1**. That is, after one stride-2 convolution the size will be 14×14, after two it will be 7×7, then 4×4, 2×2, and finally size 1.\n",
        "\n",
        "Let's try that now. First, we'll define a function with the basic parameters we'll use in each convolution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kY1g8PCbk3xf"
      },
      "source": [
        "# function to define the basic parameters for each convolution\n",
        "def conv(ni, nf, ks=3, act=True):\n",
        "    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n",
        "    if act: res = nn.Sequential(res, nn.ReLU())\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xk9XONz_jpd7",
        "outputId": "9198c572-5e5b-4b9f-c40d-e19915df5161"
      },
      "source": [
        "conv(1,3) # input:1 channel, output: 3 channels feature map"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(1, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "  (1): ReLU()\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnnBpyd5k3xf"
      },
      "source": [
        "> important: Refactoring: Refactoring parts of your neural networks like this makes it much less likely you'll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X8q76PeWhWOq",
        "outputId": "f1274d63-9c7f-4ef6-f614-f7dea501195b"
      },
      "source": [
        "# amr test\n",
        "first_cnn = nn.Sequential(*[\n",
        "    conv(ni, nf) for ni, nf in zip([1,3,7,9], [3,7,9,12])])\n",
        "first_cnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): Conv2d(1, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (1): Sequential(\n",
              "    (0): Conv2d(3, 7, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (2): Sequential(\n",
              "    (0): Conv2d(7, 9, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (3): Sequential(\n",
              "    (0): Conv2d(9, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJmsGks1k3xf"
      },
      "source": [
        "**When we use a stride-2 convolution, we often increase the number of features at the same time**. This is because we're decreasing the number of activations in the activation map by a factor of 4; we don't want to decrease the capacity of a layer by too much at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eQIDB0ak3xf"
      },
      "source": [
        "> jargon: **channels and features**: These two terms are largely used interchangeably, and refer to the size of the second axis of a weight matrix, which is, the number of activations per grid cell after a convolution. **_Features_** is never used to refer to the input data, but **_channels_** can refer to either the input data (generally channels are colors) or activations inside the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9x5qP-pk3xf"
      },
      "source": [
        "Here is how we can build a simple CNN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xJBMKEJxk3xf"
      },
      "source": [
        "simple_cnn = sequential(\n",
        "    conv(1 ,4),            #14x14 (activation map)\n",
        "    conv(4 ,8),            #7x7\n",
        "    conv(8 ,16),           #4x4\n",
        "    conv(16,32),           #2x2\n",
        "    conv(32,2, act=False), #1x1\n",
        "    Flatten(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HlKP_hKk3xf"
      },
      "source": [
        "> j: I like to add comments like the ones here after each convolution to show how large the activation map will be after each layer. These comments assume that the input size is 28*28"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JGfmzWqQ1X_a",
        "outputId": "55776054-e738-48c7-a8e9-8a2c7ab9a902"
      },
      "source": [
        "# the formula (n + 2*pad - ks)//stride + 1\n",
        "# example with an image size 28x28 pixels, ks=3, pad=1, stride=1\n",
        "(28 + 2*1 - 3)//2+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7DKqSdaH2BcA",
        "outputId": "ff9df21b-cee5-40a4-97be-f371c71214bf"
      },
      "source": [
        "# the formula (n + 2*pad - ks)//stride + 1\n",
        "(14 + 2*1 - 3)//2+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r1nMJsFtVxsh"
      },
      "source": [
        "# amr - creating a function to get the activation map dimension\n",
        "pad = 1\n",
        "ks = 3\n",
        "stride = 2\n",
        "\n",
        "def get_act_dim(height,width):\n",
        " '''get activation map dimensions'''\n",
        " def _calc(n): return (n + 2*pad - ks)//stride + 1  # the formula\n",
        " return _calc(height), _calc(width) # returns the activation map dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bsf3K9p-WGFa",
        "outputId": "0cf6df91-6b4b-4d75-99c4-9d8818506c66"
      },
      "source": [
        "# amr - using the function\n",
        "layer1 = get_act_dim(28, 28); print(layer1)\n",
        "layer2 = get_act_dim(*layer1); print(layer2)\n",
        "layer3 = get_act_dim(*layer2); print(layer3)\n",
        "layer4 = get_act_dim(*layer3); print(layer4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14, 14)\n",
            "(7, 7)\n",
            "(4, 4)\n",
            "(2, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaimjow-D0xe"
      },
      "source": [
        "NOTE: when using stride-2 the activation map is smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prOAAUYEk3xf"
      },
      "source": [
        "Now the network outputs two activations, which map to the two possible levels in our labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-AQbUnInk3xg",
        "outputId": "96868d69-f2ec-4832-a1a3-42850380cf6d"
      },
      "source": [
        "simple_cnn(xb).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 2])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auG-gwkjk3xg"
      },
      "source": [
        "We can now create our `Learner`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tKeoKIHSk3xg"
      },
      "source": [
        "learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSBafNhAk3xg"
      },
      "source": [
        "To see exactly what's going on in the model, we can use `summary`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rDrK9d2sk3xg",
        "outputId": "de16d1e3-0a7b-4687-dfee-5e7134f2c01a"
      },
      "source": [
        "learn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "Sequential (Input shape: 64)\n",
              "============================================================================\n",
              "Layer (type)         Output Shape         Param #    Trainable \n",
              "============================================================================\n",
              "                     64 x 4 x 14 x 14    \n",
              "Conv2d                                    40         True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     64 x 8 x 7 x 7      \n",
              "Conv2d                                    296        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     64 x 16 x 4 x 4     \n",
              "Conv2d                                    1168       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     64 x 32 x 2 x 2     \n",
              "Conv2d                                    4640       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     64 x 2 x 1 x 1      \n",
              "Conv2d                                    578        True      \n",
              "____________________________________________________________________________\n",
              "                     []                  \n",
              "Flatten                                                        \n",
              "____________________________________________________________________________\n",
              "\n",
              "Total params: 6,722\n",
              "Total trainable params: 6,722\n",
              "Total non-trainable params: 0\n",
              "\n",
              "Optimizer used: <function Adam at 0x7f2cfe3b3ef0>\n",
              "Loss function: <function cross_entropy at 0x7f2d2b2724d0>\n",
              "\n",
              "Callbacks:\n",
              "  - TrainEvalCallback\n",
              "  - Recorder\n",
              "  - ProgressCallback"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOTr2mCfqMW8"
      },
      "source": [
        "NOTE: `learn.summary` does not tell you were the number of parameters comes from. This line in the `simple_cnn` model does: conv(4 ,8), by default the “kernel” size is 3x3, so you can just multiply all of these numbers to get your number of non-bias parameters `4*8*3*3=288`. Again, the number of bias parameters is equal to the number of output channels, so `288+8=296`.\n",
        "\n",
        "NB: the first one is conv(4,1) so the computation is `4*1*3*3=36` plus the number of bias parameters which is equal to the number of output channels, so `36+4=40`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URUyidOQk3xg"
      },
      "source": [
        "Note that the output of the final `Conv2d` layer is `64x2x1x1`. We need to remove those extra `1x1` axes; that's what `Flatten` does. It's basically the same as PyTorch's `squeeze` method, but as a module.\n",
        "\n",
        "Let's see if this trains! Since this is a deeper network than we've built from scratch before, we'll use a lower learning rate and more epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaAKJZG-k3xg"
      },
      "source": [
        "# 2 epochs and a learning rate of 0.01\n",
        "learn.fit_one_cycle(2, 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "48AcAgRMex3e",
        "outputId": "a0a780b6-5d75-4301-c947-463beb2a23fc"
      },
      "source": [
        "#amr\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "kernels = model.extractor[0].weight.detach().clone()\n",
        "kernels = kernels - kernels.min()\n",
        "kernels = kernels / kernels.max()\n",
        "img = make_grid(kernels)\n",
        "plt.imshow(img.permute(1, 2, 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-c142fbaa4b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkernels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mkernels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mkernels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernels\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07EohZZZk3xg"
      },
      "source": [
        "Success! It's getting closer to the `resnet18` result we had, although it's not quite there yet, and it's taking more epochs, and we're needing to use a lower learning rate. We still have a few more tricks to learn, but we're getting closer and closer to being able to create a modern CNN from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H4NXO_B7h_wh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc4zDUsOk3xg"
      },
      "source": [
        "### Understanding Convolution Arithmetic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF-DyO0Mk3xh"
      },
      "source": [
        "We can see from the summary that we have an input of size `64x1x28x28`. The axes are `batch,channel,height,width`. This is often represented as `NCHW` (where `N` refers to batch size). Tensorflow, on the other hand, uses `NHWC` axis order. The first layer is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HyP8TL7Yk3xh"
      },
      "source": [
        "m = learn.model[0]\n",
        "m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFwCzXUwk3xh"
      },
      "source": [
        "So we have 1 input channel, 4 output channels, and a 3×3 kernel. Let's check the weights of the first convolution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wxT_ccGfk3xh"
      },
      "source": [
        "m[0].weight.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuyCWQ6Wk3xh"
      },
      "source": [
        "The summary shows we have 40 parameters, and `4*1*3*3` is 36. What are the other four parameters? Let's see what the bias contains:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "duLXY3U-k3xi"
      },
      "source": [
        "m[0].bias.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To7apDYfk3xi"
      },
      "source": [
        "We can now use this information to clarify our statement in the previous section: **\"When we use a stride-2 convolution, we often increase the number of features because we're decreasing the number of activations in the activation map by a factor of 4**; we don't want to decrease the capacity of a layer by too much at a time.\"\n",
        "\n",
        "There is one bias for each channel. (Sometimes channels are called *features* or *filters* when they are not input channels.) The output shape is `64x4x14x14`, and this will therefore become the input shape to the next layer. The next layer, according to `summary`, has 296 parameters. Let's ignore the batch axis to keep things simple. So for each of `14*14=196` locations we are multiplying `296-8=288` weights (ignoring the bias for simplicity), so that's `196*288=56_448` multiplications at this layer. The next layer will have `7*7*(1168-16)=56_448` multiplications.\n",
        "\n",
        "What happened here is that our stride-2 convolution halved the *grid size* from `14x14` to `7x7`, and we doubled the *number of filters* from 8 to 16, resulting in no overall change in the amount of computation. If we left the number of channels the same in each stride-2 layer, the amount of computation being done in the net would get less and less as it gets deeper. But we know that the deeper layers have to compute semantically rich features (such as eyes or fur), so we wouldn't expect that doing *less* computation would make sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZgjJxU4k3xi"
      },
      "source": [
        "Another way to think of this is based on receptive fields."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjfJcHzck3xi"
      },
      "source": [
        "### Receptive Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0jI7SrWk3xi"
      },
      "source": [
        "**The *receptive field* is the area of an image that is involved in the calculation of a layer**. On the [book's website](https://book.fast.ai/), you'll find an Excel spreadsheet called *conv-example.xlsx* that shows the calculation of two stride-2 convolutional layers using an MNIST digit. Each layer has a single kernel. The figure below shows what we see if we click on one of the cells in the *conv2* section, which shows the output of the second convolutional layer, and click *trace precedents*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i09JOGPUk3xj"
      },
      "source": [
        "<img alt=\"Immediate precedents of conv2 layer\" width=\"308\" caption=\"Immediate precedents of Conv2 layer\" id=\"preced1\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00068.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs-pMTgxk3xj"
      },
      "source": [
        "Here, the cell with the green border is the cell we clicked on, and the blue highlighted cells are its *precedents*—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Let's now click *trace precedents* again, to see what cells are used to calculate these inputs. <<preced2>> shows what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRJixULsk3xj"
      },
      "source": [
        "<img alt=\"Secondary precedents of conv2 layer\" width=\"601\" caption=\"Secondary precedents of Conv2 layer\" id=\"preced2\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00069.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8saFK3XLk3xj"
      },
      "source": [
        "In this example, we have just two convolutional layers, each of stride 2, so this is now tracing right back to the input image. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. **This 7×7 area is the *receptive field*** in the input of the green activation in Conv2. We can also see that a second filter kernel is needed now, since we have two layers.\n",
        "\n",
        "As you see from this example, the deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer. A large receptive field means that a large amount of the input image is used to calculate each activation in that layer is. We now know that in the deeper layers of the network we have semantically rich features, corresponding to larger receptive fields. Therefore, we'd expect that we'd need more weights for each of our features to handle this increasing complexity. This is another way of saying the same thing we mentioned in the previous section: when we introduce a stride-2 conv in our network, we should also increase the number of channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G88p6N7Mk3xj"
      },
      "source": [
        "When writing this particular chapter, we had a lot of questions we needed answers for, to be able to explain CNNs to you as best we could. Believe it or not, we found most of the answers on Twitter. We're going to take a quick break to talk to you about that now, before we move on to color images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXgU7Fulk3xj"
      },
      "source": [
        "### A Note About Twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-oIkoEQk3xj"
      },
      "source": [
        "We are not, to say the least, big users of social networks in general. But our goal in writing this book is to help you become the best deep learning practitioner you can, and we would be remiss not to mention how important Twitter has been in our own deep learning journeys.\n",
        "\n",
        "You see, there's another part of Twitter, far away from Donald Trump and the Kardashians, which is the part of Twitter where deep learning researchers and practitioners talk shop every day. As we were writing this section, Jeremy wanted to double-check that what we were saying about stride-2 convolutions was accurate, so he asked on Twitter:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrhOE7Fek3xj"
      },
      "source": [
        "<img alt=\"twitter 1\" width=\"500\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00064.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emR_4n2mk3xj"
      },
      "source": [
        "A few minutes later, this answer popped up:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVjCLyzlk3xj"
      },
      "source": [
        "<img alt=\"twitter 2\" width=\"500\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00065.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKEuSfRWk3xk"
      },
      "source": [
        "Christian Szegedy is the first author of [Inception](https://arxiv.org/pdf/1409.4842.pdf), the 2014 ImageNet winner and source of many key insights used in modern neural networks. Two hours later, this appeared:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiutvvAPk3xk"
      },
      "source": [
        "<img alt=\"twitter 3\" width=\"500\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00066.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7VmaRvqk3xk"
      },
      "source": [
        "Do you recognize that name? You saw it in <<chapter_production>>, when we were talking about the Turing Award winners who established the foundations of deep learning today!\n",
        "\n",
        "Jeremy also asked on Twitter for help checking our description of label smoothing in <<chapter_sizing_and_tta>> was accurate, and got a response again from directly from Christian Szegedy (label smoothing was originally introduced in the Inception paper):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbRKCvs-k3xk"
      },
      "source": [
        "<img alt=\"twitter 4\" width=\"500\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00067.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suZ_BZ8Bk3xk"
      },
      "source": [
        "Many of the top people in deep learning today are Twitter regulars, and are very open about interacting with the wider community. One good way to get started is to look at a list of Jeremy's [recent Twitter likes](https://twitter.com/jeremyphoward/likes), or [Sylvain's](https://twitter.com/GuggerSylvain/likes). That way, you can see a list of Twitter users that we think have interesting and useful things to say.\n",
        "\n",
        "Twitter is the main way we both stay up to date with interesting papers, software releases, and other deep learning news. For making connections with the deep learning community, we recommend getting involved both in the [fast.ai forums](https://forums.fast.ai) and on Twitter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVBeAJtnk3xk"
      },
      "source": [
        "That said, let's get back to the meat of this chapter. Up until now, we have only shown you examples of pictures in black and white, with one value per pixel. In practice, most colored images have three values per pixel to define their color. We'll look at working with color images next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCUixHvCaX7d"
      },
      "source": [
        "NOTE: my twitter profile is: @royam0820"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy2AzyRdk3xk"
      },
      "source": [
        "## Color Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq3OtaC-k3xk"
      },
      "source": [
        "**A colour picture is a rank-3 tensor**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S9v808-Mk3xl"
      },
      "source": [
        "im = image2tensor(Image.open(image_bear()))\n",
        "im.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ9Ysuj-70PB"
      },
      "source": [
        "NOTE: `torch.Size([3, 1000, 846])` 3 channels (RGB), width=1000, height=846. The first axis contains the channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u8VgePEuk3xl"
      },
      "source": [
        "show_image(im);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrX-AzbMk3xl"
      },
      "source": [
        "The first axis contains the channels, red, green, and blue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pyOeOLdDk3xl"
      },
      "source": [
        "_,axs = subplots(1,3)\n",
        "for bear,ax,color in zip(im,axs,('Reds','Greens','Blues')):\n",
        "    show_image(255-bear, ax=ax, cmap=color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QSWCg2Ok3xl"
      },
      "source": [
        "We saw what the convolution operation was for one filter on one channel of the image (our examples were done on a square). A convolutional layer will take an image with a certain number of channels (three for the first layer for regular RGB color images) and output an image with a different number of channels. Like our hidden size that represented the numbers of neurons in a linear layer, we can decide to have as many filters as we want, and each of them will be able to specialize, some to detect horizontal edges, others to detect vertical edges and so forth, to give something like we studied in <<chapter_production>>.\n",
        "\n",
        "In one sliding window, we have a certain number of channels and we need as many filters (we don't use the same kernel for all the channels). So our kernel doesn't have a size of 3 by 3, but `ch_in` (for channels in) is 3 by 3. On each channel, we multiply the elements of our window by the elements of the coresponding filter, then sum the results (as we saw before) and sum over all the filters. In the example given in <<rgbconv>>, the result of our conv layer on that window is red + green + blue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofPzQagSk3xl"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter9_rgbconv.svg?raw=1\" id=\"rgbconv\" caption=\"Convolution over an RGB image\" alt=\"Convolution over an RGB image\" width=\"550\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQLmj-6bk3xm"
      },
      "source": [
        "So, in order to apply a convolution to a color picture we require a kernel tensor with a size that matches the first axis. At each location, the corresponding parts of the kernel and the image patch are multiplied together.\n",
        "\n",
        "These are then all added together, to produce a single number, for each grid location, for each output feature, as shown in <<rgbconv2>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3OQf7kUk3xm"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter9_rgb_conv_stack.svg?raw=1\" id=\"rgbconv2\" caption=\"Adding the RGB filters\" alt=\"Adding the RGB filters\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7egMwxQkk3xm"
      },
      "source": [
        "Then we have `ch_out` filters like this, so in the end, the result of our convolutional layer will be a batch of images with `ch_out` channels and a height and width given by the formula outlined earlier. This give us `ch_out` tensors of size `ch_in x ks x ks` that we represent in one big tensor of four dimensions. In PyTorch, the order of the dimensions for those weights is `ch_out x ch_in x ks x ks`.\n",
        "\n",
        "Additionally, we may want to have a bias for each filter. In the preceding example, the final result for our convolutional layer would be $y_{R} + y_{G} + y_{B} + b$ in that case. Like in a linear layer, there are as many bias as we have kernels, so the biases is a vector of size `ch_out`.\n",
        "\n",
        "> There are no special mechanisms required when setting up a CNN for training with color images. Just make sure your first layer has three inputs.\n",
        "\n",
        "There are lots of ways of processing color images. For instance, you can change them to black and white, change from RGB to HSV (hue, saturation, and value) color space, and so forth. In general, it turns out experimentally that changing the encoding of colors won't make any difference to your model results, as long as you don't lose information in the transformation. So, transforming to black and white is a bad idea, since it removes the color information entirely (and this can be critical; for instance, a pet breed may have a distinctive color); but converting to HSV generally won't make any difference.\n",
        "\n",
        "Now you know what those pictures in chapter 1 (lesson 1) of \"what a neural net learns\" from the [Zeiler and Fergus paper](https://arxiv.org/abs/1311.2901) mean! This is their picture of some of the layer 1 weights which we showed:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l8NDhsfk3xm"
      },
      "source": [
        "<img alt=\"Layer 1 kernels found by Zeiler and Fergus\" width=\"120\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00031.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOpK4HLtk3xm"
      },
      "source": [
        "This is taking the three slices of the convolutional kernel, for each output feature, and displaying them as images. We can see that even though the creators of the neural net never explicitly created kernels to find edges, for instance, the neural net automatically discovered these features using SGD.\n",
        "\n",
        "Now let's see how we can train these CNNs, and show you all the techniques fastai uses under the hood for efficient training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy9GA9Fpk3xm"
      },
      "source": [
        "## Improving Training Stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrC84U5Kk3xm"
      },
      "source": [
        "Since we are so good at recognizing 3s from 7s, let's move on to something harder—recognizing all 10 digits. That means we'll need to use `MNIST` instead of `MNIST_SAMPLE`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "19jVvZVWk3xm"
      },
      "source": [
        "path = untar_data(URLs.MNIST, data=\"/content\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EHocZTalk3xm"
      },
      "source": [
        "#hide\n",
        "Path.BASE_PATH = path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XeZTPTxuk3xn"
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aVzrCHIk3xn"
      },
      "source": [
        "The data is in two folders named *training* and *testing*, so we have to tell `GrandparentSplitter` about that (it defaults to `train` and `valid`). We did do that in the `get_dls` function, which we create to make it easy to change our batch size later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vMQF1dpMk3xn"
      },
      "source": [
        "def get_dls(bs=64):\n",
        "    return DataBlock(\n",
        "        blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), \n",
        "        get_items=get_image_files, \n",
        "        splitter=GrandparentSplitter('training','testing'),\n",
        "        get_y=parent_label,\n",
        "        batch_tfms=Normalize()\n",
        "    ).dataloaders(path, bs=bs)\n",
        "\n",
        "dls = get_dls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kVCTfJTk3xn"
      },
      "source": [
        "Remember, it's always a good idea to look at your data before you use it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bbT7laPAk3xn"
      },
      "source": [
        "dls.show_batch(max_n=9, figsize=(4,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F7GWZBGk3xn"
      },
      "source": [
        "Now that we have our data ready, we can train a simple model on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fU5qXcDk3xn"
      },
      "source": [
        "### A Simple Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIPRRzJFk3xn"
      },
      "source": [
        "Earlier in this chapter, we built a model based on a `conv` function like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sxx3pYHpk3xo"
      },
      "source": [
        "def conv(ni, nf, ks=3, act=True):\n",
        "    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n",
        "    if act: res = nn.Sequential(res, nn.ReLU())\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TWmkBnRk3xo"
      },
      "source": [
        "Let's start with a basic CNN as a baseline. We'll use the same one as earlier, but with one tweak: we'll use more activations. Since we have more numbers to differentiate, it's likely we will need to learn more filters.\n",
        "\n",
        "As we discussed, we generally want to double the number of filters each time we have a stride-2 layer. One way to increase the number of filters throughout our network is to double the number of activations in the first layer–then every layer after that will end up twice as big as in the previous version as well.\n",
        "\n",
        "But there is a subtle problem with this. Consider the kernel that is being applied to each pixel. By default, we use a 3×3-pixel kernel. That means that there are a total of 3×3 = 9 pixels that the kernel is being applied to at each location. Previously, our first layer had four output filters. That meant that there were four values being computed from nine pixels at each location. Think about what happens if we double this output to eight filters. Then when we apply our kernel we will be using nine pixels to calculate eight numbers. That means it isn't really learning much at all: the output size is almost the same as the input size. Neural networks will only create useful features if they're forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs.\n",
        "\n",
        "To fix this, we can use a larger kernel in the first layer. If we use a kernel of 5×5 pixels then there are 25 pixels being used at each kernel application. Creating eight filters from this will mean the neural net will have to find some useful features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CT4mW7xWk3xo"
      },
      "source": [
        "def simple_cnn():\n",
        "    return sequential(\n",
        "        conv(1 ,8, ks=5),        #14x14\n",
        "        conv(8 ,16),             #7x7\n",
        "        conv(16,32),             #4x4\n",
        "        conv(32,64),             #2x2\n",
        "        conv(64,10, act=False),  #1x1\n",
        "        Flatten(),\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuXZY0B6JN_0"
      },
      "source": [
        "# amr Checking above computation\n",
        "pad = 2\n",
        "ks = 5\n",
        "stride = 2\n",
        "\n",
        "def get_act_dim(height,width):\n",
        " '''get activation map dimensions'''\n",
        " def _calc(n): return (n + 2*pad - ks)//stride + 1  # the formula\n",
        " return _calc(height), _calc(width) # returns the activation map dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q00PCb6GJ-9X"
      },
      "source": [
        "layer1 = get_act_dim(28,28); print(layer1)\n",
        "layer2 = get_act_dim(*layer1); print(layer2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3EgcAJ6k3xo"
      },
      "source": [
        "As you'll see in a moment, we can look inside our models while they're training in order to try to find ways to make them train better. To do this we use the `ActivationStats` callback, which records the mean, standard deviation, and histogram of activations of every trainable layer (as we've seen, callbacks are used to add behavior to the training loop; we'll explore how they work in <<chapter_accel_sgd>>):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHU7AC_6k3xo"
      },
      "source": [
        "from fastai.callback.hook import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lugPjyeFk3xo"
      },
      "source": [
        "We want to train quickly, so that means training at a high learning rate. Let's see how we go at 0.06:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsH03pm_k3xo"
      },
      "source": [
        "# fit function with a learning rate of 0.06 and ActivationStats callback\n",
        "def fit(epochs=1):\n",
        "    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n",
        "                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n",
        "    learn.fit(epochs, 0.06)\n",
        "    return learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCzJQAbBMEI1"
      },
      "source": [
        "NOTE:  The learning rate is a positive scalar determining the size of the step.  It is often noted η and indicate at which pace the weights get updated. It can be fixed or adaptively changed. The current most popular method is called Adam, which is a method that adapts the learning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez2sNq9xk3xo"
      },
      "source": [
        "# running the training for one epoch\n",
        "learn = fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgMmJ5ook3xp"
      },
      "source": [
        "This didn't train at all well! Let's find out why.\n",
        "\n",
        "One handy feature of the callbacks passed to `Learner` is that they are made available automatically, with the same name as the callback class, except in `snake_case`. So, our `ActivationStats` callback can be accessed through `activation_stats`. I'm sure you remember `learn.recorder`... can you guess how that is implemented? That's right, it's a callback called `Recorder`!\n",
        "\n",
        "Ref.: [Hook Callbacks](https://fastai1.fast.ai/callbacks.hooks.html)\n",
        "\n",
        "`ActivationStats` includes some handy utilities for plotting the activations during training. `plot_layer_stats(idx)` plots the mean and standard deviation of the activations of layer number *`idx`*, along with the percentage of activations near zero. Here's the first layer's plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHvU-fBFk3xp"
      },
      "source": [
        "learn.activation_stats.plot_layer_stats(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HfqDm5XUGlT"
      },
      "source": [
        "NOTE: at layer index 0, the mean is around -0.3, the standard deviation is \"all other the place\" and 60% of the activations are closed to 0. The network is not learning much."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al0Dc0KzRMjb"
      },
      "source": [
        "# ??learn.activation_stats.plot_layer_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_HvEZoFjvwa"
      },
      "source": [
        "NOTE: `activation_stats` is a callback that record the mean and std of activations.  The callback class `ActivationStats` saves the layer activations in `self.stats` for all modules passed to it. By default it will save activations for all modules. The saved stats is a FloatTensor of shape (2,num_modules,num_batches). The first axis is (mean,stdev)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6rNkjtpk3xp"
      },
      "source": [
        "**Generally our model should have a consistent, or at least smooth, mean and standard deviation of layer activations during training**. **Activations near zero are particularly problematic, because it means we have computation in the model that's doing nothing at all** (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to the next layer... which will then create more zeros. Here's the penultimate layer of our network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn_j9Xd9k3xp"
      },
      "source": [
        "# checking the second last layer index (avant dernier)\n",
        "learn.activation_stats.plot_layer_stats(-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_xOxn09k3xp"
      },
      "source": [
        "**As expected, the problems get worse towards the end of the network, as the instability and zero activations compound over layers.** Let's look at what we can do to make training more stable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a8BHa84k3xp"
      },
      "source": [
        "### Increase Batch Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNcBN79Tk3xp"
      },
      "source": [
        "> One way to make training more stable is to increase the batch size. Larger batches have gradients that are more accurate, since they're calculated from more data. On the downside, though, a larger batch size means fewer batches per epoch, which means less opportunities for your model to update weights. Let's see if a batch size of 512 helps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEB7_P_Vk3xp"
      },
      "source": [
        "dls = get_dls(512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11SBconAk3xp"
      },
      "source": [
        "learn = fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8WBkR-1k3xq"
      },
      "source": [
        "Let's see what the penultimate layer looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14u-ryC-k3xq"
      },
      "source": [
        "learn.activation_stats.plot_layer_stats(-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnhEFMIJk3xq"
      },
      "source": [
        "Again, we've got most of our activations near zero. Let's see what else we can do to improve training stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXHPY1mtk3xq"
      },
      "source": [
        "### 1cycle Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXLCkuPyk3xq"
      },
      "source": [
        "**Our initial weights are not well suited to the task we're trying to solve. Therefore, it is dangerous to begin training with a high learning rate: we may very well make the training diverge instantly, as we've seen**. We probably don't want to end training with a high learning rate either, so that we don't skip over a minimum. But we want to train at a high learning rate for the rest of the training period, because we'll be able to train more quickly that way. Therefore, **we should change the learning rate during training, from low, to high, and then back to low again**.\n",
        "\n",
        "Leslie Smith (yes, the same guy that invented the learning rate finder!) developed this idea in his article [\"Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\"](https://arxiv.org/abs/1708.07120). He designed a schedule for learning rate separated into two phases: one where the learning rate grows from the minimum value to the maximum value (***warmup***), and one where it decreases back to the minimum value (***annealing***). Smith called this combination of approaches ***1cycle training***.\n",
        "\n",
        "1cycle training allows us to use a much higher maximum learning rate than other types of training, which gives two benefits:\n",
        "\n",
        "> - By training with higher learning rates, we train faster—a phenomenon Smith named ***super-convergence***.\n",
        "- By training with higher learning rates, **we overfit less** because we skip over the sharp local minima to end up in a smoother (and therefore more generalizable) part of the loss.\n",
        "\n",
        "The second point is an interesting and subtle one; it is based on the observation that **a model that generalizes well is one whose loss would not change very much if you changed the input by a small amount**. If a model trains at a large learning rate for quite a while, and can find a good loss when doing so, it must have found an area that also generalizes well, because it is jumping around a lot from batch to batch (that is basically the definition of a high learning rate). The problem is that, as we have discussed, just jumping to a high learning rate is more likely to result in diverging losses, rather than seeing your losses improve. So we don't jump straight to a high learning rate. Instead, **we start at a low learning rate, where our losses do not diverge, and we allow the optimizer to gradually find smoother and smoother areas of our parameters by gradually going to higher and higher learning rates**.\n",
        "\n",
        "Then, once we have found a nice smooth area for our parameters, we want to find the very best part of that area, which means we have to bring our learning rates down again. **This is why 1cycle training has a gradual learning rate warmup, and a gradual learning rate cooldown**. Many researchers have found that in practice this approach leads to more accurate models and trains more quickly. That is why it is the approach that is used by default for `fine_tune` in fastai.\n",
        "\n",
        "In chapter 16 we'll learn all about ***momentum*** in SGD. Briefly, **momentum is a technique where the optimizer takes a step not only in the direction of the gradients, but also that continues in the direction of previous steps**. Leslie Smith introduced the idea of *cyclical momentums* in [\"A Disciplined Approach to Neural Network Hyper-Parameters: Part 1\"](https://arxiv.org/pdf/1803.09820.pdf). It suggests that **the momentum varies in the opposite direction of the learning rate: when we are at high learning rates, we use less momentum, and we use more again in the annealing phase**.\n",
        "\n",
        "We can use 1cycle training in fastai by calling `fit_one_cycle`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpD-Wfyhk3xq"
      },
      "source": [
        "# using Leslie Smith one cycle training (fit_one_cyle)\n",
        "def fit(epochs=1, lr=0.06):\n",
        "    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n",
        "                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n",
        "    learn.fit_one_cycle(epochs, lr)\n",
        "    return learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGY_4k4Lk3xr"
      },
      "source": [
        "learn = fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueDDu_exk3xr"
      },
      "source": [
        "We're finally making some progress! It's giving us a reasonable accuracy now.\n",
        "\n",
        "We can view the learning rate and momentum throughout training by calling `plot_sched` on `learn.recorder`. `learn.recorder` (as the name suggests) records everything that happens during training, including losses, metrics, and hyperparameters such as learning rate and momentum:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIvjrL6Ok3xr"
      },
      "source": [
        "learn.recorder.plot_sched()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfaruF7rk3xr"
      },
      "source": [
        "Smith's original 1cycle paper used a linear warmup and linear annealing. As you can see, we adapted the approach in fastai by combining it with another popular approach: cosine annealing. `fit_one_cycle` provides the following parameters you can adjust:\n",
        "\n",
        "- `lr_max`:: The highest learning rate that will be used (this can also be a list of learning rates for each layer group, or a Python `slice` object containing the first and last layer group learning rates)\n",
        "- `div`:: How much to divide `lr_max` by to get the starting learning rate\n",
        "- `div_final`::  How much to divide `lr_max` by to get the ending learning rate\n",
        "- `pct_start`:: What percentage of the batches to use for the warmup\n",
        "- `moms`:: A tuple `(mom1,mom2,mom3)` where *`mom1`* is the initial momentum, *`mom2`* is the minimum momentum, and *`mom3`* is the final momentum\n",
        "\n",
        "Let's take a look at our layer stats again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mlA0Gjpk3xr"
      },
      "source": [
        "learn.activation_stats.plot_layer_stats(-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES7SuKKDk3xs"
      },
      "source": [
        "The percentage of near-zero weights is getting much better, although it's still quite high.\n",
        "\n",
        "We can see even more about what's going on in our training using `color_dim`, passing it a layer index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzFOXBdD8gAE"
      },
      "source": [
        "#import matplotlib\n",
        "matplotlib.rcParams['image.cmap'] = 'viridis'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfcCqQSQk3xs"
      },
      "source": [
        "learn.activation_stats.color_dim(-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf6LhPVVk3xs"
      },
      "source": [
        "`color_dim` was developed by fast.ai in conjunction with a student, Stefano Giomo. Stefano, who refers to the idea as the *colorful dimension*, provides an [in-depth explanation](https://forums.fast.ai/t/the-colorful-dimension/42908) of the history and details behind the method. The basic idea is to create a histogram of the activations of a layer, which we would hope would follow a smooth pattern such as the normal distribution (colorful_dist).\n",
        "\n",
        "Vertical axis represents a group (bin) of activation values. Each column in the horizontal axis is a batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jL2jVazk3xs"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/colorful_dist.jpeg?raw=1\" id=\"colorful_dist\" caption=\"Histogram in 'colorful dimension'\" alt=\"Histogram in 'colorful dimension'\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIvp7A6yk3xs"
      },
      "source": [
        "To create `color_dim`, we take the histogram shown on the left here, and convert it into just the colored representation shown at the bottom. Then we flip it on its side, as shown on the right. We found that the distribution is clearer if we take the log of the histogram values. Then, Stefano describes:\n",
        "\n",
        "> : The final plot for each layer is made by stacking the histogram of the activations from each batch along the horizontal axis. So each vertical slice in the visualisation represents the histogram of activations for a single batch. The color intensity corresponds to the height of the histogram, in other words the number of activations in each histogram bin.\n",
        "\n",
        "<<colorful_summ>> shows how this all fits together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5r7NtMfk3xt"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/colorful_summ.png?raw=1\" id=\"colorful_summ\" caption=\"Summary of the colorful dimension (courtesy of Stefano Giomo)\" alt=\"Summary of the colorful dimension\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_YRXKQXk3xt"
      },
      "source": [
        "This illustrates why log(f) is more colorful than *f* when *f* follows a normal distribution because taking a log changes the Gaussian in a quadratic, which isn't as narrow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ8JEAlfk3xt"
      },
      "source": [
        "So with that in mind, let's take another look at the result for the penultimate layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDtx-KwWk3xt"
      },
      "source": [
        "learn.activation_stats.color_dim(-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqSPlWjjk3xu"
      },
      "source": [
        "**This shows a classic picture of \"bad training.\"** We start with nearly all activations at zero—that's what we see at the far left, with all the dark blue. The bright yellow at the bottom represents the near-zero activations. Then, over the first few batches we see the number of nonzero activations exponentially increasing. But it goes too far, and collapses! We see the dark blue return, and the bottom becomes bright yellow again. It almost looks like training restarts from scratch. Then we see the activations increase again, and collapse again. After repeating this a few times, eventually we see a spread of activations throughout the range.\n",
        "\n",
        "It's much better if training can be smooth from the start. The cycles of exponential increase and then collapse tend to result in a lot of near-zero activations, resulting in slow training and poor final results. One way to solve this problem is to use batch normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFjL-D-Pk3xu"
      },
      "source": [
        "### Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVTu87nHk3xu"
      },
      "source": [
        "To fix the slow training and poor final results we ended up with in the previous section, we need to fix the initial large percentage of near-zero activations, and then try to maintain a good distribution of activations throughout training.\n",
        "\n",
        "Sergey Ioffe and Christian Szegedy presented a solution to this problem in the 2015 paper [\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"](https://arxiv.org/abs/1502.03167). In the abstract, they describe just the problem that we've seen:\n",
        "\n",
        "> : Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization... We refer to this phenomenon as **internal covariate shift (data distribution changing)**, and address the problem by normalizing layer inputs.\n",
        "\n",
        "Their solution, they say is:\n",
        "\n",
        "> : Making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization.\n",
        "\n",
        "The paper caused great excitement as soon as it was released, because it included the chart below, which clearly demonstrated that batch normalization could train a model that was even more accurate than the current state of the art (the *Inception* architecture) and around 5x faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky6h7a4dk3xu"
      },
      "source": [
        "<img alt=\"Impact of batch normalization\" width=\"553\" caption=\"Impact of batch normalization (courtesy of Sergey Ioffe and Christian Szegedy)\" id=\"batchnorm\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00046.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1ZtG0bRk3xu"
      },
      "source": [
        "> **Batch normalization (often just called *batchnorm*) works by taking an average of the mean and standard deviations of the activations of a layer and using those to normalize the activations**. However, this can cause problems because the network might want some activations to be really high in order to make accurate predictions. So they also added two learnable parameters (meaning they will be updated in the SGD step), usually called `gamma` and `beta`. After normalizing the activations to get some new activation vector `y`, a batchnorm layer returns `gamma*y + beta`.\n",
        "\n",
        "That's why our activations can have any mean or variance, independent from the mean and standard deviation of the results of the previous layer. Those statistics are learned separately, making training easier on our model. The behavior is different during training and validation: during training, we use the mean and standard deviation of the batch to normalize the data, while during validation we instead use a running mean of the statistics calculated during training.\n",
        "\n",
        "Let's add a batchnorm layer to `conv`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Q45WiSk3xu"
      },
      "source": [
        "def conv(ni, nf, ks=3, act=True):\n",
        "    layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n",
        "    if act: layers.append(nn.ReLU())\n",
        "    layers.append(nn.BatchNorm2d(nf))\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThJ13sGFk3xu"
      },
      "source": [
        "and fit our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML_L6mbrk3xu"
      },
      "source": [
        "learn = fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEDjc75Ok3xv"
      },
      "source": [
        "That's a great result! Let's take a look at `color_dim`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKJUFI7yk3xv"
      },
      "source": [
        "learn.activation_stats.color_dim(-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f9eaXL2k3xw"
      },
      "source": [
        "This is just what we hope to see: a smooth development of activations, with no \"crashes.\" Batchnorm has really delivered on its promise here! In fact, batchnorm has been so successful that we see it (or something very similar) in nearly all modern neural networks.\n",
        "\n",
        "An interesting observation about models containing batch normalization layers is that they tend to generalize better than models that don't contain them. Although we haven't as yet seen a rigorous analysis of what's going on here, most researchers believe that the reason for this is that batch normalization adds some extra randomness to the training process. Each mini-batch will have a somewhat different mean and standard deviation than other mini-batches. Therefore, the activations will be normalized by different values each time. In order for the model to make accurate predictions, it will have to learn to become robust to these variations. In general, adding additional randomization to the training process often helps.\n",
        "\n",
        "> in summary batch normalization\n",
        "- speeds up the training\n",
        "- decreases the importance of the initial weights\n",
        "- regularizes the model a little bit\n",
        "\n",
        "Since things are going so well, let's train for a few more epochs and see how it goes. In fact, let's *increase* the learning rate, since the abstract of the batchnorm paper claimed we should be able to \"train at much higher learning rates\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmpESfaZk3xw"
      },
      "source": [
        "learn = fit(5, lr=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CYAkkvtk3xw"
      },
      "source": [
        "At this point, I think it's fair to say we know how to recognize digits! It's time to move on to something harder..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAwAELM2kFIa"
      },
      "source": [
        "# Visualizing CNN Feature Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ53NprNkNCL"
      },
      "source": [
        "# amr\n",
        "# Grab a model - seeing the structure of our model\n",
        "m = learn.model\n",
        "\n",
        "# Grab the parameters\n",
        "#w, b = m[0].parameters()\n",
        "w = m[0].parameters\n",
        "\n",
        "# reshaping the weight parameter\n",
        "#w[0].view(28,28)\n",
        "w.view\n",
        "\n",
        "# plot the weight parameter - let's have a look!\n",
        "show_image(w[0].view(28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axYl4YMKncjw"
      },
      "source": [
        "m[0].parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpT9EGEjk3xw"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SNWUlRIk3xw"
      },
      "source": [
        "We've seen that convolutions are just a type of matrix multiplication, with two constraints on the weight matrix: some elements are always zero, and some elements are tied (forced to always have the same value). In the intro chapter,  we saw the eight requirements from the 1986 book *Parallel Distributed Processing*; one of them was \"A pattern of connectivity among units.\" That's exactly what these constraints do: they enforce a certain pattern of connectivity.\n",
        "\n",
        "These constraints allow us to use far fewer parameters in our model, without sacrificing the ability to represent complex visual features. That means we can train deeper models faster, with less overfitting. Although the universal approximation theorem shows that it should be *possible* to represent anything in a fully connected network in one hidden layer, we've seen now that in *practice* we can train much better models by being thoughtful about network architecture.\n",
        "\n",
        "Convolutions are by far the most common pattern of connectivity we see in neural nets (along with regular linear layers, which we refer to as *fully connected*), but it's likely that many more will be discovered.\n",
        "\n",
        "We've also seen how to interpret the activations of layers in the network to see whether training is going well or not, and how batchnorm helps regularize the training and makes it smoother. In the next chapter, we will use both of those layers to build the most popular architecture in computer vision: a residual network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbRqBheEk3xw"
      },
      "source": [
        "## Questionnaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yxjm1zwk3xw"
      },
      "source": [
        "1. What is a convolution?\n",
        "> A convolution does element-wise multiplication of each filter with it’s input window, then adds them all up.\n",
        "1. What is a \"feature\"?\n",
        "> A data transformation that makes it easier to model.\n",
        "1. Write out the convolutional kernel matrix for a top edge detector.\n",
        "\n",
        "```\n",
        "top_edge = tensor([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]]).float() \n",
        "```\n",
        "1. Write out the mathematical operation applied by a 3×3 kernel to a single pixel in an image.\n",
        "> an element wise multiplication.  we take each 3x3 area, and element wise multiply them with a kernel, and add each of those up together to create one output; this is called a convolution. \n",
        "1. What is the value of a convolutional kernel apply to a 3×3 matrix of zeros?\n",
        "> a zeros matrix.\n",
        "1. What is \"padding\"?\n",
        "> Padding adds an appropriate number of rows and columns on each side of the input feature. Padding essentially makes the feature maps produced by the filters the same size as the original image. In short, padding allows us to apply a kernel to the corners of a an input (image) matrix. It 'pads' the image. \n",
        "1. What is \"stride\"?\n",
        "> Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on. \n",
        "1. Create a nested list comprehension to complete any task that you choose.\n",
        "> ```\n",
        "[[[(i,j) for i in range(4)] for j in range(3)] for _ in range(2)]\n",
        "```\n",
        "1. What are the shapes of the `input` and `weight` parameters to PyTorch's 2D convolution?\n",
        "> **input shape**: (minibatch, in_channels, iH, iW) \n",
        "> **weight shape**: (out_channels, in_channels, kH, kW)\n",
        "1. What is a \"channel\"?\n",
        "> A single basic color in an image. For RGB images there are three channels: Red, Green, and Blue.\n",
        "1. What is the relationship between a convolution and a matrix multiplication?\n",
        "> They are the same thing.\n",
        "1. What is a \"convolutional neural network\"?\n",
        "> A series of matrix multiplications\n",
        "1. What is the benefit of refactoring parts of your neural network definition?\n",
        "> Reduces the likelihood of getting inconsistent shapes, etc in architectures.  Makes things clearer to the reader\n",
        "1. What is `Flatten`? Where does it need to be included in the MNIST CNN? Why?\n",
        "> It's the equivalent of the squeeze function. But you can call Flatten as a module in nn.Sequential.  It's included at the end of the MNIST CNN to convert a rank 4 tensor into a rank 2 tensor.\n",
        "1. What does \"NCHW\" mean?\n",
        "> N,C,H,W = batch,channel,height,width. For `64,1,28,28`:\n",
        "batch = `64`, channel = `1`, height = `28`, width = `28`\n",
        "1. Why does the third layer of the MNIST CNN have `7*7*(1168-16)` multiplications?\n",
        "1. What is a \"receptive field\"?\n",
        "> The area of an image that is involved in the calculation of a layer. Basically, the output feature (of any layer) to a given input region (patch)\n",
        "1. What is the size of the receptive field of an activation after two stride 2 convolutions? Why?\n",
        "> ```\n",
        "pad = 1\n",
        "ks = 3\n",
        "stride = 2\n",
        "def get_act_dim(height,width):\n",
        "    '''get activation map dimensions'''\n",
        "    def _calc(n): return (n + 2*pad - ks)//stride + 1\n",
        "    return _calc(height), _calc(width)\n",
        "layer1 = get_act_dim(28, 28); print(layer1)\n",
        "layer2 = get_act_dim(*layer1); print(layer2)\n",
        "layer3 = get_act_dim(*layer2); print(layer3)\n",
        "layer4 = get_act_dim(*layer3); print(layer4)\n",
        "(14, 14)\n",
        "(7, 7)\n",
        "(4, 4)\n",
        "(2, 2)\n",
        "Size is: (4, 4)\n",
        "```\n",
        "\n",
        "\n",
        "1. Run *conv-example.xlsx* yourself and experiment with *trace precedents*.\n",
        "1. Have a look at Jeremy or Sylvain's list of recent Twitter \"like\"s, and see if you find any interesting resources or ideas there.\n",
        "1. How is a color image represented as a tensor?\n",
        "> ch_out x ch_in x ks x ks. When dealing with RGB images ch_in for the first conv will be 3.\n",
        "1. How does a convolution work with a color input?\n",
        "> The RGB channels are treated separately. Each channel is multiplied by the kernel. These are added together for each grid location for each output feature.\n",
        "1. What method can we use to see that data in `DataLoaders`?\n",
        "> show_batch()\n",
        "1. Why do we double the number of filters after each stride-2 conv?\n",
        "> A stride 2 conv with the default padding (1) and ks (3) will reduce the activation map dimension by half. Formula: (`n + 2*pad - ks)//stride + 1`. As the activation map dimension reduces by half we double the number of filters. This results in no overall change in computation as the network gets deeper and deeper.\n",
        "1. Why do we use a larger kernel in the first conv with MNIST (with `simple_cnn`)?\n",
        "> Initially, the first conv in MNIST has ks of 3 and number of output filters of 8.  This means the kernel has shape 3x3. So we are using 9 pixels to obtain 8 numbers. The output size and the input size is roughly the same. The network won't learn very much.  As a result, we change the ks to 5. This results in a 5x5 kernel shape. So we're using 25 numbers to obtain 8 output numbers. This works better.\n",
        "1. What information does `ActivationStats` save for each layer?\n",
        "> ActivationStats saves information about the activations (mean, std, near zero %). They are indexed by layer.\n",
        "1. How can we access a learner's callback after training?\n",
        "> Use the learn object. Eg. `learn.activation_stats`\n",
        "1. What are the three statistics plotted by `plot_layer_stats`? What does the x-axis represent?\n",
        "> mean, std, near zero %\n",
        "1. Why are activations near zero problematic?\n",
        "> Multiplying a small number by a small number gives a very small number. As the number of mupltication tends toward infinity, the activations will tend towards zero. These don't really help the network.\n",
        "1. What are the upsides and downsides of training with a larger batch size?\n",
        "> Upside: Larger batches have gradients that are more accurate, since they train on more data. Downside: Fewer batches per epoch. Less time for the model to update weights.\n",
        "1. Why should we avoid using a high learning rate at the start of training?\n",
        "> We may diverge immediately and completely miss the minimum\n",
        "1. What is 1cycle training?\n",
        "> Consists of: **Warmup**: lr grows from min value to max value and **Annealing**: lr decreases from max value to min value\n",
        "1. What are the benefits of training with a high learning rate?\n",
        "> Faster training (super-convergence). We overfit less because we skip over the sharp local minima to end up in a smoother (and therefore more generalizable) part of the loss.\n",
        "1. Why do we want to use a low learning rate at the end of training?\n",
        "> to better reach the global minima.\n",
        "1. What is \"cyclical momentum\"?\n",
        "> The optimizer takes a step in the direction of the gradients and also continues in the direction of previous steps.\n",
        "1. What callback tracks hyperparameter values during training (along with other information)?\n",
        "> learn.recorder\n",
        "1. What does one column of pixels in the `color_dim` plot represent?\n",
        "> The histogram of activations for a single batch\n",
        "1. What does \"bad training\" look like in `color_dim`? Why?\n",
        "> Bad training would show a non-smooth curve on the color_dim graph. This is indicative of the activations increasing then collapsing to become near zero.\n",
        "1. What trainable parameters does a batch normalization layer contain?\n",
        "> gamma weights, beta weights\n",
        "1. What statistics are used to normalize in batch normalization during training? How about during validation?\n",
        "> During training we use the mean and std dev of the activations in a batch to normalize the data.  During validation we use the running mean of the statistics calculated during training the normalize the data.\n",
        "1. Why do models with batch normalization layers generalize better?\n",
        "> Some researchers think that it adds some extra randomness to the training process. The mini-batch will have a different mean and std dev of activations compared to other mini-batches. As a result, the activations will be normalised by different values throughout training. The model becomes robust to these extra bits of randomness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzLQobOck3xx"
      },
      "source": [
        "### Further Research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR_sb-f3k3xx"
      },
      "source": [
        "1. What features other than edge detectors have been used in computer vision (especially before deep learning became popular)?\n",
        "1. There are other normalization layers available in PyTorch. Try them out and see what works best. Learn about why other normalization layers have been developed, and how they differ from batch normalization.\n",
        "1. Try moving the activation function after the batch normalization layer in `conv`. Does it make a difference? See what you can find out about what order is recommended, and why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQHfdStAk3xx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAvIsdNv0qMP"
      },
      "source": [
        "# Ressources\n",
        "\n",
        "[Convolution Animation](https://www.youtube.com/watch?v=f0t-OCG79-U)\n",
        "\n",
        "[Image Kernels](https://setosa.io/ev/image-kernels/)\n",
        "\n",
        "[CNN glossary](https://medium.com/@prasadpal107/dictionary-for-cnn-753a1a39db45)\n",
        "\n",
        "[Convolutions Explained](https://nareshr8.github.io/julia-blogs/2021-08-30-convolutions.html)\n",
        "\n",
        "[CS231n: Convolutional Neural Networks for Visual Recognition - Stanford - Spring 2021 - Live visualization](http://cs231n.stanford.edu/)\n",
        "\n",
        "[A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)\n",
        "\n",
        "[Animated Convolutions](https://github.com/vdumoulin/conv_arithmetic)\n",
        "\n",
        "[Convolutions in fastai](https://ravimashru.dev/blog/2021-08-31-convolutions-in-fastai/)\n",
        "\n",
        "[Fastai ConvLayer Documentation](https://docs.fast.ai/layers.html#ConvLayer)\n",
        "\n",
        "[Batch Normalization](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cpPFO8Eb-Yq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}